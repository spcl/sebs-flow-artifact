
@article{luo_beeflow_2023,
	title = {{BeeFlow}: Behavior tree-based Serverless workflow modeling and scheduling for resource-constrained edge clusters},
	volume = {143},
	issn = {1383-7621},
	url = {https://www.sciencedirect.com/science/article/pii/S1383762123001479},
	doi = {10.1016/j.sysarc.2023.102968},
	shorttitle = {{BeeFlow}},
	abstract = {Serverless computing has gained popularity in edge computing due to its flexible features, including the pay-per-use pricing model, auto-scaling capabilities, and multi-tenancy support. Complex Serverless-based applications typically rely on Serverless workflows (also known as Serverless function orchestration) to express task execution logic, and numerous application- and system-level optimization techniques have been developed for Serverless workflow scheduling. However, there has been limited exploration of optimizing Serverless workflow scheduling in edge computing systems, particularly in high-density, resource-constrained environments such as system-on-chip clusters and single-board-computer clusters. In this work, we discover that existing Serverless workflow scheduling techniques typically assume models with limited expressiveness and cause significant resource contention. To address these issues, we propose modeling Serverless workflows using behavior trees, a novel and fundamentally different approach from existing directed-acyclic-graph- and state machine-based models. Behavior tree-based modeling allows for easy analysis without compromising workflow expressiveness. We further present observations derived from the inherent tree structure of behavior trees for contention-free function collections and awareness of exact and empirical concurrent function invocations. Based on these observations, we introduce {BeeFlow}, a behavior tree-based Serverless workflow system tailored for resource-constrained edge clusters. Experimental results demonstrate that {BeeFlow} achieves up to 3.2× speedup in a high-density, resource-constrained edge testbed and 2.5× speedup in a high-profile cloud testbed, compared with the state-of-the-art. {BeeFlow} also demonstrates superior robustness in scenarios with heavy system workloads.},
	pages = {102968},
	journaltitle = {Journal of Systems Architecture},
	shortjournal = {Journal of Systems Architecture},
	author = {Luo, Ke and Ouyang, Tao and Zhou, Zhi and Chen, Xu},
	urldate = {2024-05-15},
	date = {2023-10-01},
	keywords = {Serverless computing, Behavior tree, Edge computing, Serverless workflow, Workflow modeling, Workflow scheduling},
	file = {ScienceDirect Snapshot:/home/larissa/Zotero/storage/5BPKNEKZ/S1383762123001479.html:text/html;Submitted Version:/home/larissa/Zotero/storage/FA7UYGXE/Luo et al. - 2023 - BeeFlow Behavior tree-based Serverless workflow m.pdf:application/pdf},
}

@article{wen_joint_2024,
	title = {Joint Optimization of Parallelism and Resource Configuration for Serverless Function Steps},
	volume = {35},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1045-9219, 1558-2183, 2161-9883},
	url = {https://ieeexplore.ieee.org/document/10433749/},
	doi = {10.1109/TPDS.2024.3365134},
	abstract = {Function-as-a-Service ({FaaS}) offers a ﬁne-grained resource provision model, enabling developers to build highly elastic cloud applications. User requests are handled by a series of serverless functions step by step, which forms a multi-step workﬂow. The developers are required to set proper conﬁgurations for functions to meet service level objectives ({SLOs}) and save costs. However, developing the conﬁguration strategy is challenging. This is mainly because the execution of serverless functions often suffers from cold starts and performance ﬂuctuation, which requires a dynamic conﬁguration strategy to guarantee the {SLOs}. In this article, we present {StepConf}, a framework that automates the conﬁguration as the workﬂow runs. {StepConf} optimizes memory size for each function step in the workﬂow and takes inter and intra-function parallelism into consideration, which has been overlooked by existing work. {StepConf} intelligently predicts the potential conﬁgurations for subsequent function steps, and proactively prewarms function instances in a conﬁguration-aware manner to reduce the cold start overheads. We evaluate {StepConf} on {AWS} and Knative. Compared to existing work, {StepConf} improves performance by up to 5.6× under the same cost budget and achieves up to a 40\% cost reduction while maintaining the same level of performance.},
	pages = {560--576},
	number = {4},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	shortjournal = {{IEEE} Trans. Parallel Distrib. Syst.},
	author = {Wen, Zhaojie and Chen, Qiong and Niu, Yipei and Song, Zhen and Deng, Quanfeng and Liu, Fangming},
	urldate = {2024-05-15},
	date = {2024-04},
	langid = {english},
	file = {Wen et al. - 2024 - Joint Optimization of Parallelism and Resource Con.pdf:/home/larissa/Zotero/storage/TFWNKIMY/Wen et al. - 2024 - Joint Optimization of Parallelism and Resource Con.pdf:application/pdf},
}

@inproceedings{li_dataflower_2023,
	location = {Vancouver {BC} Canada},
	title = {{DataFlower}: Exploiting the Data-flow Paradigm for Serverless Workflow Orchestration},
	isbn = {9798400703942},
	url = {https://dl.acm.org/doi/10.1145/3623278.3624755},
	doi = {10.1145/3623278.3624755},
	shorttitle = {{DataFlower}},
	abstract = {Serverless computing that runs functions with auto-scaling is a popular task execution pattern in the cloud-native era. By connecting serverless functions into workflows, tenants can achieve complex functionality. Prior research adopts the control-flow paradigm to orchestrate a serverless workflow. However, the control-flow paradigm inherently results in long response latency, due to the heavy data persistence overhead, sequential resource usage, and late function triggering. Our investigation shows that the data-flow paradigm has the potential to resolve the above problems, with careful design and optimization. We propose {DataFlower}, a scheme that achieves the data-flow paradigm for serverless workflows. In {DataFlower}, a container is abstracted to be a function logic unit and a data logic unit. The function logic unit runs the functions, and the data logic unit handles the data transmission asynchronously. Moreover, a host-container collaborative communication mechanism is used to support efficient data transfer. Our experimental results show that compared to state-of-the-art serverless designs, {DataFlower} reduces the 99\%-ile latency of the benchmarks by up to 35.4\%, and improves the peak throughput by up to 3.8X.},
	eventtitle = {{ASPLOS} '23: 28th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
	pages = {57--72},
	booktitle = {Proceedings of the 28th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
	publisher = {{ACM}},
	author = {Li, Zijun and Xu, Chuhao and Chen, Quan and Zhao, Jieru and Chen, Chen and Guo, Minyi},
	urldate = {2024-05-15},
	date = {2023-03-25},
	langid = {english},
	file = {Li et al. - 2023 - DataFlower Exploiting the Data-flow Paradigm for .pdf:/home/larissa/Zotero/storage/IU622MSA/Li et al. - 2023 - DataFlower Exploiting the Data-flow Paradigm for .pdf:application/pdf},
}

@article{cheng_slo-aware_2024,
	title = {{SLO}-Aware Function Placement for Serverless Workflows With Layer-Wise Memory Sharing},
	volume = {35},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1045-9219, 1558-2183, 2161-9883},
	url = {https://ieeexplore.ieee.org/document/10506617/},
	doi = {10.1109/TPDS.2024.3391858},
	abstract = {Function-as-a-Service ({FaaS}) is a promising cloud computing model known for its scalability and elasticity. In various application domains, {FaaS} workﬂows have been widely adopted to manage user requests and complete computational tasks efﬁciently. Motivated by the fact that function containers collaboratively use the image layer’s memory, co-placing functions would leverage memory sharing to reduce cluster memory footprint, this article studies layer-wise memory sharing for serverless functions. We ﬁnd that overwhelming memory sharing by placing containers in the same cluster machine may lead to performance deterioration and Service Level Objective ({SLO}) violations due to the increased {CPU} pressure. We investigate how to maximally reduce cluster memory footprint via layer-wise memory sharing for serverless workﬂows while guaranteeing their {SLO}. First, we study the container memory sharing problem under serverless workﬂows with a static Directed Acyclic Graph ({DAG}) structure. We prove it is {NP}-Hard and propose a 2-approximation algorithm, namely {MDP}. Then we consider workﬂows with dynamic {DAG} structure scenarios, where the memory sharing problem is also {NP}-Hard. We design a Greedybased algorithm called {GSP} to address this issue. We implement a carefully designed prototype on the {OpenWhisk} platform, and our evaluation results demonstrate that both {MDP} and {GSP} achieve a balanced and satisfying state, effectively reducing up to 63\% of cache memory usage while guaranteeing serverless workﬂow {SLO}.},
	pages = {1074--1091},
	number = {6},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	shortjournal = {{IEEE} Trans. Parallel Distrib. Syst.},
	author = {Cheng, Dazhao and Yan, Kai and Cai, Xinquan and Gong, Yili and Hu, Chuang},
	urldate = {2024-05-15},
	date = {2024-06},
	langid = {english},
	file = {Cheng et al. - 2024 - SLO-Aware Function Placement for Serverless Workfl.pdf:/home/larissa/Zotero/storage/FAX7NVEB/Cheng et al. - 2024 - SLO-Aware Function Placement for Serverless Workfl.pdf:application/pdf},
}

@inproceedings{carver_search_2019,
	location = {Denver, {CO}, {USA}},
	title = {In Search of a Fast and Efficient Serverless {DAG} Engine},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72816-005-4},
	url = {https://ieeexplore.ieee.org/document/8955207/},
	doi = {10.1109/PDSW49588.2019.00005},
	abstract = {Python-written data analytics applications can be modeled as and compiled into a directed acyclic graph ({DAG}) based workﬂow, where the nodes are ﬁne-grained tasks and the edges are task dependencies.Such analytics workﬂow jobs are increasingly characterized by short, ﬁne-grained tasks with large fan-outs. These characteristics make them well-suited for a new cloud computing model called serverless computing or Function-as-a-Service ({FaaS}), which has become prevalent in recent years. The auto-scaling property of serverless computing platforms accommodates short tasks and bursty workloads, while the pay-per-use billing model of serverless computing providers keeps the cost of short tasks low.},
	eventtitle = {2019 {IEEE}/{ACM} Fourth International Parallel Data Systems Workshop ({PDSW})},
	pages = {1--10},
	booktitle = {2019 {IEEE}/{ACM} Fourth International Parallel Data Systems Workshop ({PDSW})},
	publisher = {{IEEE}},
	author = {Carver, Benjamin and Zhang, Jingyuan and Wang, Ao and Cheng, Yue},
	urldate = {2024-05-15},
	date = {2019-11},
	langid = {english},
	file = {Carver et al. - 2019 - In Search of a Fast and Efficient Serverless DAG E.pdf:/home/larissa/Zotero/storage/C2LZMX8B/Carver et al. - 2019 - In Search of a Fast and Efficient Serverless DAG E.pdf:application/pdf},
}

@article{mahgoub_wisefuse_2022,
	title = {{WISEFUSE}: Workload Characterization and {DAG} Transformation for Serverless Workflows},
	volume = {6},
	issn = {2476-1249},
	url = {https://dl.acm.org/doi/10.1145/3530892},
	doi = {10.1145/3530892},
	shorttitle = {{WISEFUSE}},
	abstract = {We characterize production workloads of serverless {DAGs} at a major cloud provider. Our analysis highlights two major factors that limit performance: (a) lack of efficient communication methods between the serverless functions in the {DAG}, and (b) stragglers when a {DAG} stage invokes a set of parallel functions that must complete before starting the next {DAG} stage. To address these limitations, we propose {WISEFUSE}, an automated approach to generate an optimized execution plan for serverless {DAGs} for a user-specified latency objective or budget. We introduce three optimizations: (1) Fusion combines in-series functions together in a single {VM} to reduce the communication overhead between cascaded functions. (2) Bundling executes a group of parallel invocations of a function in one {VM} to improve resource sharing among the parallel workers to reduce skew. (3) Resource Allocation assigns the right {VM} size to each function or function bundle in the {DAG} to reduce the E2E latency and cost. We implement {WISEFUSE} to evaluate it experimentally using three popular serverless applications with different {DAG} structures, memory footprints, and intermediate data sizes. Compared to competing approaches and other alternatives, {WISEFUSE} shows significant improvements in E2E latency and cost. Specifically, for a machine learning pipeline, {WISEFUSE} achieves P95 latency that is 67\% lower than Photons, 39\% lower than Faastlane, and 90\% lower than {SONIC} without increasing the cost.},
	pages = {1--28},
	number = {2},
	journaltitle = {Proceedings of the {ACM} on Measurement and Analysis of Computing Systems},
	shortjournal = {Proc. {ACM} Meas. Anal. Comput. Syst.},
	author = {Mahgoub, Ashraf and Yi, Edgardo Barsallo and Shankar, Karthick and Minocha, Eshaan and Elnikety, Sameh and Bagchi, Saurabh and Chaterji, Somali},
	urldate = {2024-05-15},
	date = {2022-05-26},
	langid = {english},
	file = {Mahgoub et al. - 2022 - WISEFUSE Workload Characterization and DAG Transf.pdf:/home/larissa/Zotero/storage/C762W68F/Mahgoub et al. - 2022 - WISEFUSE Workload Characterization and DAG Transf.pdf:application/pdf},
}

@inproceedings{bhasi_kraken_2021,
	location = {Seattle {WA} {USA}},
	title = {Kraken: Adaptive Container Provisioning for Deploying Dynamic {DAGs} in Serverless Platforms},
	isbn = {978-1-4503-8638-8},
	url = {https://dl.acm.org/doi/10.1145/3472883.3486992},
	doi = {10.1145/3472883.3486992},
	shorttitle = {Kraken},
	abstract = {The growing popularity of microservices has led to the proliferation of online cloud service-based applications, which are typically modelled as Directed Acyclic Graphs ({DAGs}) comprising of tens to hundreds of microservices. The vast majority of these applications are user-facing, and hence, have stringent {SLO} requirements. Serverless functions, having short resource provisioning times and instant scalability, are suitable candidates for developing such latency-critical applications. However, existing serverless providers are unaware of the workflow characteristics of application {DAGs}, leading to container over-provisioning in many cases. This is further exacerbated in the case of dynamic {DAGs}, where the function chain for an application is not known a priori. Motivated by these observations, we propose Kraken, a workflow-aware resource management framework that minimizes the number of containers provisioned for an application {DAG} while ensuring {SLO}-compliance. We design and implement Kraken on {OpenFaaS} and evaluate it on a multi-node Kubernetes-managed cluster. Our extensive experimental evaluation using {DeathStarbench} workload suite and real-world traces demonstrates that Kraken spawns up to 76\% fewer containers, thereby improving container utilization and saving cluster-wide energy by up to 4× and 48\%, respectively, when compared to state-of-the art schedulers employed in serverless platforms. {CCS} Concepts • Computer systems organization → Cloud Computing; Resource-Management; Scheduling.},
	eventtitle = {{SoCC} '21: {ACM} Symposium on Cloud Computing},
	pages = {153--167},
	booktitle = {Proceedings of the {ACM} Symposium on Cloud Computing},
	publisher = {{ACM}},
	author = {Bhasi, Vivek M. and Gunasekaran, Jashwant Raj and Thinakaran, Prashanth and Mishra, Cyan Subhra and Kandemir, Mahmut Taylan and Das, Chita},
	urldate = {2024-05-15},
	date = {2021-11},
	langid = {english},
	file = {Bhasi et al. - 2021 - Kraken Adaptive Container Provisioning for Deploy.pdf:/home/larissa/Zotero/storage/KDMU423N/Bhasi et al. - 2021 - Kraken Adaptive Container Provisioning for Deploy.pdf:application/pdf},
}

@inproceedings{li_faasflow_2022,
	location = {Lausanne Switzerland},
	title = {{FaaSFlow}: enable efficient workflow execution for function-as-a-service},
	isbn = {978-1-4503-9205-1},
	url = {https://dl.acm.org/doi/10.1145/3503222.3507717},
	doi = {10.1145/3503222.3507717},
	shorttitle = {{FaaSFlow}},
	abstract = {Serverless computing (Function-as-a-Service) provides fine-grain resource sharing by running functions (or Lambdas) in containers. Data-dependent functions are required to be invoked following a pre-defined logic, which is known as serverless workflows. However, our investigation shows that the traditional master-worker based workflow execution architecture performs poorly in serverless context. One significant overhead results from the master-side workflow schedule pattern, with which the functions are triggered in the master node and assigned to worker nodes for execution. Besides, the data movement between workers also reduces the throughput. To this end, we present a worker-side workflow schedule pattern for serverless workflow execution. Following the design, we implement {FaaSFlow} to enable efficient workflow execution in the serverless context. Besides, we propose an adaptive storage library {FaaStore} that enables fast data transfer between functions on the same node without through the database. Experiment results show that {FaaSFlow} effectively mitigates the workflow scheduling overhead by 74.6\% on average and data transmission overhead by 95\% at most. When the network bandwidth fluctuates, {FaaSFlow}-{FaaStore} reduces the throughput degradation by 23.0\%, and is able to multiply the utilization of network bandwidth by 1.5𝑋 -4𝑋 .},
	eventtitle = {{ASPLOS} '22: 27th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {782--796},
	booktitle = {Proceedings of the 27th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {{ACM}},
	author = {Li, Zijun and Liu, Yushi and Guo, Linsong and Chen, Quan and Cheng, Jiagan and Zheng, Wenli and Guo, Minyi},
	urldate = {2024-05-15},
	date = {2022-02-28},
	langid = {english},
	file = {Li et al. - 2022 - FaaSFlow enable efficient workflow execution for .pdf:/home/larissa/Zotero/storage/E6PLQGXL/Li et al. - 2022 - FaaSFlow enable efficient workflow execution for .pdf:application/pdf},
}

@misc{exton_raptor_2024,
	title = {Raptor: Distributed Scheduling for Serverless Functions},
	url = {http://arxiv.org/abs/2403.16457},
	shorttitle = {Raptor},
	abstract = {Serverless platforms that poorly schedule function requests inspire developers to implement workarounds to issues like high cold start latencies, poor fault tolerance, and limited support for parallel processing. These “solutions” litter environments with idle containers and add unnecessary pressure to the already underperforming scheduling services. An effective serverless scheduling policy should encourage developers to write small and reusable snippets of code, and give operators the freedom to administer cluster workloads however necessary in order to meet their operational demands. To this end, we have designed a distributed scheduling service that integrates with existing serverless frameworks. Our service addresses three key issues that affect modern serverless platforms; high cold start latencies, poor fault tolerance, and limited native support for parallel processing patterns like fork-join and map-reduce. We have built a prototype that integrates with the existing {OpenWhisk} services, and is fully backwards compatible with the existing implementation. The updated architecture improves performance and adds new scheduling and security features. Our empirical results demonstrate that our scheduler reduces cold start execution latencies by up to 80\%, steady state latencies by up to 10\%, and does so with negligible time and memory overhead.},
	number = {{arXiv}:2403.16457},
	publisher = {{arXiv}},
	author = {Exton, Kevin and Read, Maria},
	urldate = {2024-05-15},
	date = {2024-03-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2403.16457 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Exton and Read - 2024 - Raptor Distributed Scheduling for Serverless Func.pdf:/home/larissa/Zotero/storage/BYPDDTVE/Exton and Read - 2024 - Raptor Distributed Scheduling for Serverless Func.pdf:application/pdf},
}

@inproceedings{carver_wukong_2020,
	location = {Virtual Event {USA}},
	title = {Wukong: a scalable and locality-enhanced framework for serverless parallel computing},
	isbn = {978-1-4503-8137-6},
	url = {https://dl.acm.org/doi/10.1145/3419111.3421286},
	doi = {10.1145/3419111.3421286},
	shorttitle = {Wukong},
	abstract = {Executing complex, burst-parallel, directed acyclic graph ({DAG}) jobs poses a major challenge for serverless execution frameworks, which will need to rapidly scale and schedule tasks at high throughput, while minimizing data movement across tasks. We demonstrate that, for serverless parallel computations, decentralized scheduling enables scheduling to be distributed across Lambda executors that can schedule tasks in parallel, and brings multiple benefits, including enhanced data locality, reduced network I/Os, automatic resource elasticity, and improved cost effectiveness. We describe the implementation and deployment of our new serverless parallel framework, called Wukong, on {AWS} Lambda. We show that Wukong achieves near-ideal scalability, executes parallel computation jobs up to 68.17× faster, reduces network I/O by multiple orders of magnitude, and achieves 92.96\% tenantside cost savings compared to numpywren.},
	eventtitle = {{SoCC} '20: {ACM} Symposium on Cloud Computing},
	pages = {1--15},
	booktitle = {Proceedings of the 11th {ACM} Symposium on Cloud Computing},
	publisher = {{ACM}},
	author = {Carver, Benjamin and Zhang, Jingyuan and Wang, Ao and Anwar, Ali and Wu, Panruo and Cheng, Yue},
	urldate = {2024-05-15},
	date = {2020-10-12},
	langid = {english},
	file = {Carver et al. - 2020 - Wukong a scalable and locality-enhanced framework.pdf:/home/larissa/Zotero/storage/JNM2GJUX/Carver et al. - 2020 - Wukong a scalable and locality-enhanced framework.pdf:application/pdf},
}

@article{werner_reference_2024,
	title = {A reference architecture for serverless big data processing},
	volume = {155},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X24000360},
	doi = {10.1016/j.future.2024.01.029},
	abstract = {Despite significant advances in data management systems in recent decades, the processing of big data at scale remains very challenging. While cloud computing has been well-accepted as a solution to address scalability needs, cloud configuration and operation complexity persist and often present themselves as entry barriers, especially for novice data analysts. Serverless computing and Function-as-a-Service ({FaaS}) platforms have been suggested to reduce such entry barriers by shifting configuration and operational responsibilities from the application developer to the {FaaS} platform provider. Naturally, “serverless data processing ({SDP})”, that is, using {FaaS} for (big) data processing, has received increasing interest in recent years. However, {FaaS} platforms were never intended to support large data processing tasks primarily. {SDP}, therefore, manifests itself through workarounds and adaptations on the application level, addressing various quirks and limitations of the {FaaS} platforms in use for data processing needs. This, in turn, creates tensions between the platforms and the applications using them, again encouraging the constant (re-)design of both. Consequently, we present lessons learned from a series of application and platform re-designs that address these tensions, leading to the development of an {SDP} reference architecture and a platform instantiation and implementation thereof called {CREW}. Mitigating the tensions through the process of application platform co-design proves to reduce both entry barriers and costs significantly. In some experiments, {CREW} outperforms traditional, non-{SDP} big data processing frameworks by factors.},
	pages = {179--192},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Werner, Sebastian and Tai, Stefan},
	urldate = {2024-05-15},
	date = {2024-06-01},
	keywords = {Software engineering, Cloud computing, Function as a Service, Application platform co-design, Serverless data processing, Serverless reference architecture},
	file = {ScienceDirect Snapshot:/home/larissa/Zotero/storage/CQ6NJI5Q/S0167739X24000360.html:text/html},
}

@inproceedings{abdi_palette_2023,
	location = {Rome Italy},
	title = {Palette Load Balancing: Locality Hints for Serverless Functions},
	isbn = {978-1-4503-9487-1},
	url = {https://dl.acm.org/doi/10.1145/3552326.3567496},
	doi = {10.1145/3552326.3567496},
	shorttitle = {Palette Load Balancing},
	abstract = {Function-as-a-Service ({FaaS}) serverless computing enables a simple programming model with almost unbounded elasticity. Unfortunately, current {FaaS} platforms achieve this flexibility at the cost of lower performance for data-intensive applications compared to a serverful deployment. The ability to have computation close to data is a key missing feature. We introduce Palette load balancing, which offers {FaaS} applications a simple mechanism to express locality to the platform, through hints we term “colors”. Palette maintains the serverless nature of the service – users are still not allocating resources – while allowing the platform to place successive invocations related to each other on the same executing node. We compare a prototype of the Palette load balancer to a state-of-the-art locality-oblivious load balancer on representative examples of three applications. For a serverless web application with a local cache, Palette improves the hit ratio by 6x. For a serverless version of Dask, Palette improves run times by 46\% and 40\% on Task Bench and {TPC}-H, respectively. On a serverless version of {NumS}, Palette improves run times by 37\%. These improvements largely bridge the gap to serverful implementation of the same systems.},
	eventtitle = {{EuroSys} '23: Eighteenth European Conference on Computer Systems},
	pages = {365--380},
	booktitle = {Proceedings of the Eighteenth European Conference on Computer Systems},
	publisher = {{ACM}},
	author = {Abdi, Mania and Ginzburg, Samuel and Lin, Xiayue Charles and Faleiro, Jose and Chaudhry, Gohar Irfan and Goiri, Inigo and Bianchini, Ricardo and Berger, Daniel S and Fonseca, Rodrigo},
	urldate = {2024-05-15},
	date = {2023-05-08},
	langid = {english},
	file = {Abdi et al. - 2023 - Palette Load Balancing Locality Hints for Serverl.pdf:/home/larissa/Zotero/storage/9C3CMMJZ/Abdi et al. - 2023 - Palette Load Balancing Locality Hints for Serverl.pdf:application/pdf},
}

@incollection{maximilien_serverless_2017,
	location = {Cham},
	title = {Serverless Execution of Scientific Workflows},
	volume = {10601},
	isbn = {978-3-319-69034-6 978-3-319-69035-3},
	url = {https://link.springer.com/10.1007/978-3-319-69035-3_51},
	abstract = {In this paper, we present a serverless workﬂow execution system ({DEWE} v31) with Function-as-a-Service ({FaaS} aka serverless computing) as the target execution environment. {DEWE} v3 is designed to address problems of (1) execution of large-scale scientiﬁc workﬂows and (2) resource underutilization. At its core is our novel hybrid ({FaaS} and dedicated/local clusters) job dispatching approach taking into account resource consumption patterns of diﬀerent phases of workﬂow execution. In particular, the hybrid approach deals with the maximum execution duration limit, memory limit, and storage space limit. {DEWE} v3 significantly reduces the eﬀorts needed to execute large-scale scientiﬁc workﬂow applications on public clouds. We have evaluated {DEWE} v3 on both {AWS} Lambda and Google Cloud Functions and demonstrate that {FaaS} oﬀers an ideal solution for scientiﬁc workﬂows with complex precedence constraints. In our large-scale evaluations, the hybrid execution model surpasses the performance of the traditional cluster execution model with signiﬁcantly less execution cost.},
	pages = {706--721},
	booktitle = {Service-Oriented Computing},
	publisher = {Springer International Publishing},
	author = {Jiang, Qingye and Lee, Young Choon and Zomaya, Albert Y.},
	editor = {Maximilien, Michael and Vallecillo, Antonio and Wang, Jianmin and Oriol, Marc},
	urldate = {2024-05-15},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-3-319-69035-3_51},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Jiang et al. - 2017 - Serverless Execution of Scientific Workflows.pdf:/home/larissa/Zotero/storage/9TEER9AW/Jiang et al. - 2017 - Serverless Execution of Scientific Workflows.pdf:application/pdf},
}

@inproceedings{wen_measurement_2021,
	location = {Chicago, {IL}, {USA}},
	title = {A Measurement Study on Serverless Workflow Services},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-66541-681-8},
	url = {https://ieeexplore.ieee.org/document/9590280/},
	doi = {10.1109/ICWS53863.2021.00102},
	abstract = {Major cloud providers increasingly roll out their serverless workflow services to orchestrate serverless functions, making it possible to construct complex applications effectively. A comprehensive study is necessary to help developers understand the pros and cons, and make better choices among these serverless workflow services. However, the characteristics of these serverless workflow services have not been systematically analyzed. To fill the knowledge gap, we conduct a comprehensive measurement study on four mainstream serverless workflow services, focusing on both features and the performance. First, we review their official documentation and extract their features from six dimensions, including programming model, state management, etc. Then, we compare their performance (Le., the execution time of functions, execution time of workflows, orchestration overhead time of workflows) under various settings considering activity complexity and data-flow complexity of workflows, as well as function complexity of serverless functions. Our findings and implications could help developers and cloud providers improve their development efficiency and user experience.},
	eventtitle = {2021 {IEEE} International Conference on Web Services ({ICWS})},
	pages = {741--750},
	booktitle = {2021 {IEEE} International Conference on Web Services ({ICWS})},
	publisher = {{IEEE}},
	author = {Wen, Jinfeng and Liu, Yi},
	urldate = {2024-05-15},
	date = {2021-09},
	langid = {english},
	file = {Wen and Liu - 2021 - A Measurement Study on Serverless Workflow Service.pdf:/home/larissa/Zotero/storage/LD5H4KEY/Wen and Liu - 2021 - A Measurement Study on Serverless Workflow Service.pdf:application/pdf},
}

@inproceedings{jarachanthan_astra_2021,
	location = {Portland, {OR}, {USA}},
	title = {Astra: Autonomous Serverless Analytics with Cost-Efficiency and {QoS}-Awareness},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-66544-066-0},
	url = {https://ieeexplore.ieee.org/document/9460548/},
	doi = {10.1109/IPDPS49936.2021.00085},
	shorttitle = {Astra},
	abstract = {With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter with the difﬁculty in coordinating computation across different stages and provisioning resources in a large conﬁguration space. This paper presents our design and implementation of Astra, which conﬁgures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account ﬂexibly-speciﬁed user requirements. Astra relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-speciﬁc requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain optimal job execution. We deploy Astra in the {AWS} Lambda platform and conduct real-world experiments over three representative benchmarks with different scales. Results demonstrate that Astra can achieve the optimal execution decision for serverless analytics, by improving the performance of 21\% to 60\% under a given budget constraint, and resulting in a cost reduction of 20\% to 80\% without violating performance requirement, when compared with three baseline conﬁguration algorithms.},
	eventtitle = {2021 {IEEE} International Parallel and Distributed Processing Symposium ({IPDPS})},
	pages = {756--765},
	booktitle = {2021 {IEEE} International Parallel and Distributed Processing Symposium ({IPDPS})},
	publisher = {{IEEE}},
	author = {Jarachanthan, Jananie and Chen, Li and Xu, Fei and Li, Bo},
	urldate = {2024-05-15},
	date = {2021-05},
	langid = {english},
	file = {Jarachanthan et al. - 2021 - Astra Autonomous Serverless Analytics with Cost-E.pdf:/home/larissa/Zotero/storage/R6KIWHIV/Jarachanthan et al. - 2021 - Astra Autonomous Serverless Analytics with Cost-E.pdf:application/pdf},
}

@inproceedings{ristov_simless_2022,
	location = {San Francisco California},
	title = {{SimLess}: simulate serverless workflows and their twins and siblings in federated {FaaS}},
	isbn = {978-1-4503-9414-7},
	url = {https://dl.acm.org/doi/10.1145/3542929.3563478},
	doi = {10.1145/3542929.3563478},
	shorttitle = {{SimLess}},
	abstract = {Many researchers migrate scientific serverless workflows or function choreographies ({FCs}) on Function-as-a-Service ({FaaS}) to benefit from its high scalability and elasticity. Unfortunately, the heterogeneity of federated {FaaS} hampers decisions on appropriate parameter setup to run {FCs}. Consequently, scientists must choose between accurate but tedious and expensive experiments or simple but cheap and less accurate simulations. Unfortunately, related works support either simulation models for serverfull workflows running on virtual machines and containers or partial {FaaS} models for individual serverless functions focused on execution time and neglecting various kinds of federated overheads.},
	eventtitle = {{SoCC} '22: {ACM} Symposium on Cloud Computing},
	pages = {323--339},
	booktitle = {Proceedings of the 13th Symposium on Cloud Computing},
	publisher = {{ACM}},
	author = {Ristov, Sashko and Hautz, Mika and Hollaus, Christian and Prodan, Radu},
	urldate = {2024-05-15},
	date = {2022-11-07},
	langid = {english},
	file = {Ristov et al. - 2022 - SimLess simulate serverless workflows and their t.pdf:/home/larissa/Zotero/storage/YIDDSZPA/Ristov et al. - 2022 - SimLess simulate serverless workflows and their t.pdf:application/pdf},
}

@inproceedings{gunasekaran_fifer_2020,
	location = {Delft Netherlands},
	title = {Fifer: Tackling Resource Underutilization in the Serverless Era},
	isbn = {978-1-4503-8153-6},
	url = {https://dl.acm.org/doi/10.1145/3423211.3425683},
	doi = {10.1145/3423211.3425683},
	shorttitle = {Fifer},
	abstract = {Datacenters are witnessing a rapid surge in the adoption of serverless functions for microservices-based applications. A vast majority of these microservices typically span less than a second, have strict {SLO} requirements, and are chained together as per the requirements of an application. The aforementioned characteristics introduce a new set of challenges, especially in terms of container provisioning and management, as the state-of-the-art resource management frameworks, employed in serverless platforms, tend to look at microservice-based applications similar to conventional monolithic applications. Hence, these frameworks suffer from microservice agnostic scheduling and colossal container over-provisioning, especially during workload fluctuations, thereby resulting in poor resource utilization.},
	eventtitle = {Middleware '20: 21st International Middleware Conference},
	pages = {280--295},
	booktitle = {Proceedings of the 21st International Middleware Conference},
	publisher = {{ACM}},
	author = {Gunasekaran, Jashwant Raj and Thinakaran, Prashanth and Nachiappan, Nachiappan C. and Kandemir, Mahmut Taylan and Das, Chita R.},
	urldate = {2024-05-15},
	date = {2020-12-07},
	langid = {english},
	file = {Gunasekaran et al. - 2020 - Fifer Tackling Resource Underutilization in the S.pdf:/home/larissa/Zotero/storage/IIJPQF5X/Gunasekaran et al. - 2020 - Fifer Tackling Resource Underutilization in the S.pdf:application/pdf},
}

@article{kumari_workflow_2023,
	title = {Workflow aware analytical model to predict performance and cost of serverless execution},
	volume = {35},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7743},
	doi = {10.1002/cpe.7743},
	abstract = {Serverless computing has emerged as a powerful deployment model based on the Function-as-a-Service ({FaaS}) paradigm, where applications are orchestrated through a set of independent functions. The function orchestration within an application can be represented through a serverless workflow, which defines the overall execution plan of the application. To ensure the quality of service for serverless computing platforms, it is essential to develop performance and cost models that can predict the service quality that can be obtained from deploying and executing applications in the cloud platform. While several analytical models have been developed for various cloud deployment frameworks in recent years, there has been a lack of performance and cost analysis models for serverless computing platforms. The existing performance and cost monitoring tools available in serverless frameworks face several challenges, such as complexity, lack of transparency, and incomplete monitoring data. In this paper, we fill the gap by proposing an efficient workflow-based analytical model that can estimate the end-to-end response time and cost of the serverless execution plan. The proposed model can handle complex structures like loop, cycles, self-loop, and parallel substructures that exist in serverless workflows. Additionally, we propose a heuristic optimization algorithm to identify the optimal resource configuration to achieve the optimal response time under a given budget constraint. We evaluated the effectiveness of the proposed model by considering seven serverless applications in both {AWS} Lambda and Microsoft Azure platforms. We compared the accuracy of the proposed model with the real values of response time and cost obtained in {AWS} Lambda and Microsoft Azure serverless platforms. The proposed performance and cost model in the {AWS} Lambda platform has been observed to have an average accuracy of 99.2\% and 98.7\% respectively. In the Microsoft Azure platform, the average accuracy of the performance and cost model has been observed to be 98.6\% and 98.2\% respectively.},
	pages = {e7743},
	number = {22},
	journaltitle = {Concurrency and Computation: Practice and Experience},
	author = {Kumari, Anisha and Sahoo, Bibhudatta and Behera, Ranjan Kumar},
	urldate = {2024-05-15},
	date = {2023},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.7743},
	keywords = {performance modeling, cost modeling, serverless computing, function-as-a-service, serverless workflow, {AWS} lambda, container},
	file = {Full Text PDF:/home/larissa/Zotero/storage/XPZAHZCJ/Kumari et al. - 2023 - Workflow aware analytical model to predict perform.pdf:application/pdf;Snapshot:/home/larissa/Zotero/storage/TI4K5CHZ/cpe.html:text/html},
}

@article{jarachanthan_astrea_2022,
	title = {\textit{Astrea:} Auto-Serverless Analytics Towards Cost-Efficiency and {QoS}-Awareness},
	volume = {33},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1045-9219, 1558-2183, 2161-9883},
	url = {https://ieeexplore.ieee.org/document/9767624/},
	doi = {10.1109/TPDS.2022.3172069},
	shorttitle = {{\textless}i{\textgreater}Astrea},
	abstract = {With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter the difﬁculty in coordinating computation across different stages and provisioning resources in a large conﬁguration space. This paper presents our design and implementation of Astrea, which conﬁgures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account ﬂexibly-speciﬁed user requirements. Astrea relies on the modeling of performance and cost which characterizes the intricate interplay among multidimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-speciﬁc requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain the optimal job execution. We deploy Astrea in the {AWS} Lambda platform and conduct real-world experiments over representative benchmarks, including Big Data analytics and machine learning workloads, at different scales. Extensive results demonstrate that Astrea can achieve the optimal execution decision for serverless data analytics, in comparison with various provisioning and deployment baselines. For example, when compared with three provisioning baselines, Astrea manages to reduce the job completion time by 21\% to 69\% under a given budget constraint, while saving cost by 20\% to 84\% without violating performance requirements.},
	pages = {3833--3849},
	number = {12},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	shortjournal = {{IEEE} Trans. Parallel Distrib. Syst.},
	author = {Jarachanthan, Jananie and Chen, Li and Xu, Fei and Li, Bo},
	urldate = {2024-05-15},
	date = {2022-12-01},
	langid = {english},
	file = {Jarachanthan et al. - 2022 - Astrea Auto-Serverless Analytics Towards C.pdf:/home/larissa/Zotero/storage/9SNAIFV9/Jarachanthan et al. - 2022 - Astrea Auto-Serverless Analytics Towards C.pdf:application/pdf},
}

@inproceedings{triendl_peeking_2024,
	location = {London United Kingdom},
	title = {Peeking Behind the Serverless Implementations and Deployments of the Montage Workflow},
	isbn = {9798400704451},
	url = {https://dl.acm.org/doi/10.1145/3629527.3651420},
	doi = {10.1145/3629527.3651420},
	abstract = {The development of serverless scientific workflows is a complex and tedious procedure and opens several challenges in how to compose workflow processing steps as serverless functions and how much memory to assign to each serverless function, which affects not only the computing resources, but also the networking communication to the cloud storage. Merging multiple processing steps into a single serverless function (fusion) reduces the number of invocations, but restricts the developer to assign the maximum required memory of all fused processing steps, which may increase the overall costs.},
	eventtitle = {{ICPE} '24: 15th {ACM}/{SPEC} International Conference on Performance Engineering},
	pages = {196--203},
	booktitle = {Companion of the 15th {ACM}/{SPEC} International Conference on Performance Engineering},
	publisher = {{ACM}},
	author = {Triendl, Simon and Ristov, Sashko},
	urldate = {2024-05-15},
	date = {2024-05-07},
	langid = {english},
	file = {Triendl and Ristov - 2024 - Peeking Behind the Serverless Implementations and .pdf:/home/larissa/Zotero/storage/4W7NNMKC/Triendl and Ristov - 2024 - Peeking Behind the Serverless Implementations and .pdf:application/pdf},
}

@inproceedings{werner_evaluation_2020,
	location = {Delft Netherlands},
	title = {An Evaluation of Serverless Data Processing Frameworks},
	isbn = {978-1-4503-8204-5},
	url = {https://dl.acm.org/doi/10.1145/3429880.3430095},
	doi = {10.1145/3429880.3430095},
	eventtitle = {Middleware '20: 21st International Middleware Conference},
	pages = {19--24},
	booktitle = {Proceedings of the 2020 Sixth International Workshop on Serverless Computing},
	publisher = {{ACM}},
	author = {Werner, Sebastian and Girke, Richard and Kuhlenkamp, Jörn},
	urldate = {2024-05-15},
	date = {2020-12-07},
	langid = {english},
	file = {Werner et al. - 2020 - An Evaluation of Serverless Data Processing Framew.pdf:/home/larissa/Zotero/storage/CAGUTF5Z/Werner et al. - 2020 - An Evaluation of Serverless Data Processing Framew.pdf:application/pdf},
}

@article{van_eyk_spec-rg_2019,
	title = {The {SPEC}-{RG} Reference Architecture for {FaaS}: From Microservices and Containers to Serverless Platforms},
	volume = {23},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1089-7801, 1941-0131},
	url = {https://ieeexplore.ieee.org/document/8894540/},
	doi = {10.1109/MIC.2019.2952061},
	shorttitle = {The {SPEC}-{RG} Reference Architecture for {FaaS}},
	pages = {7--18},
	number = {6},
	journaltitle = {{IEEE} Internet Computing},
	shortjournal = {{IEEE} Internet Comput.},
	author = {Van Eyk, Erwin and Grohmann, Johannes and Eismann, Simon and Bauer, Andre and Versluis, Laurens and Toader, Lucian and Schmitt, Norbert and Herbst, Nikolas and Abad, Cristina L. and Iosup, Alexandru},
	urldate = {2024-05-15},
	date = {2019-11-01},
	langid = {english},
	file = {Van Eyk et al. - 2019 - The SPEC-RG Reference Architecture for FaaS From .pdf:/home/larissa/Zotero/storage/HKNZP7V8/Van Eyk et al. - 2019 - The SPEC-RG Reference Architecture for FaaS From .pdf:application/pdf},
}

@inproceedings{mathew_exploring_2021,
	location = {Leicester United Kingdom},
	title = {Exploring the cost and performance benefits of {AWS} step functions using a data processing pipeline},
	isbn = {978-1-4503-8564-0},
	url = {https://dl.acm.org/doi/10.1145/3468737.3494084},
	doi = {10.1145/3468737.3494084},
	abstract = {In traditional cloud computing, dedicated hardware is substituted by dynamically allocated, utility-oriented resources such as virtualized servers. While cloud services are following the pay-as-you-go pricing model, resources are billed based on instance allocation and not on the actual usage, leading the customers to be charged needlessly. In serverless computing, as exemplified by the Functionas-a-Service ({FaaS}) model where functions are the basic resources, functions are typically not allocated or charged until invoked or triggered. Functions are not applications, however, and to build compelling serverless applications they frequently need to be orchestrated with some kind of application logic. A major issue emerging by the use of orchestration is that it complicates further the already complex billing model used by {FaaS} providers, which in combination with the lack of granular billing and execution details offered by the providers makes the development and evaluation of serverless applications challenging.},
	eventtitle = {{UCC} '21: 2021 {IEEE}/{ACM} 14th International Conference on Utility and Cloud Computing},
	pages = {1--10},
	booktitle = {Proceedings of the 14th {IEEE}/{ACM} International Conference on Utility and Cloud Computing},
	publisher = {{ACM}},
	author = {Mathew, Anil and Andrikopoulos, Vasilios and Blaauw, Frank J.},
	urldate = {2024-05-15},
	date = {2021-12-06},
	langid = {english},
	file = {Mathew et al. - 2021 - Exploring the cost and performance benefits of AWS.pdf:/home/larissa/Zotero/storage/CSWBG5W7/Mathew et al. - 2021 - Exploring the cost and performance benefits of AWS.pdf:application/pdf},
}

@article{ristov_xafcl_2022,
	title = {{xAFCL}: Run Scalable Function Choreographies Across Multiple {FaaS} Systems},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1939-1374, 2372-0204},
	url = {https://ieeexplore.ieee.org/document/9616383/},
	doi = {10.1109/TSC.2021.3128137},
	shorttitle = {{xAFCL}},
	abstract = {Most well-known cloud providers offer advanced support for serverless applications that goes beyond single function invocation by enabling developers to build entire workﬂows, which are known as serverless function choreographies ({FCs}). Current support for {FCs} by many {FaaS} systems uncovered important problems including maximum number of parallel function executions, unexpected considerable delays, and provider lock-in. These limitations can result in longer execution times or even failure to execute individual functions or entire {FCs}. To overcome some of these limitations, we introduce a scalable middleware service {xAFCL} that can schedule and execute different functions of the same {FC} across multiple {FaaS} systems (currently supporting all top ﬁve providers). In order to support scheduling under {xAFCL}, we introduce a novel {FaaS} model which estimates the completion time of functions by considering {FaaS} system limitations, submission delays, and overheads for executing functions. Experimental results demonstrate that {xAFCL}’s {FaaS} model shows very low inaccuracy of up to 2:9\% for {AWS} and 20\% for {IBM} for real-life {BWA} data-bound {FC} that uses S3. Moreover, {xAFCL} outperforms an earliest start time ({EST}) scheduler by up to 43\% for makespan and 2:7Â for throughput.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Services Computing},
	shortjournal = {{IEEE} Trans. Serv. Comput.},
	author = {Ristov, Sasko and Pedratscher, Stefan and Fahringer, Thomas},
	urldate = {2024-05-15},
	date = {2022},
	langid = {english},
	file = {Ristov et al. - 2022 - xAFCL Run Scalable Function Choreographies Across.pdf:/home/larissa/Zotero/storage/ISEAQ4E2/Ristov et al. - 2022 - xAFCL Run Scalable Function Choreographies Across.pdf:application/pdf},
}

@inproceedings{suo_tackling_2021,
	location = {Portland, {OR}, {USA}},
	title = {Tackling Cold Start of Serverless Applications by Efficient and Adaptive Container Runtime Reusing},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72819-666-4},
	url = {https://ieeexplore.ieee.org/document/9556063/},
	doi = {10.1109/Cluster48925.2021.00018},
	abstract = {During the past few years, serverless computing has changed the paradigm of application development and deployment in the cloud and edge due to its unique advantages, including easy administration, automatic scaling, built-in fault tolerance, etc. Nevertheless, serverless computing is also facing challenges such as long latency due to the cold start. In this paper, we present an in-depth performance analysis of cold start in the serverless framework and propose {HotC}, a container-based runtime management framework that leverages the lightweight containers to mitigate the cold start and improve the network performance of serverless applications. {HotC} maintains a live container runtime pool, analyzes the user input or conﬁguration ﬁle, and provides available runtime for immediate reuse. To precisely predict the request and efﬁciently manage the hot containers, we design an adaptive live container control algorithm combining the exponential smoothing model and Markov chain method. Our evaluation results show that {HotC} introduces negligible overhead and can efﬁciently improve the performance of various applications with different network trafﬁc patterns in both cloud servers and edge devices.},
	eventtitle = {2021 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	pages = {433--443},
	booktitle = {2021 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	publisher = {{IEEE}},
	author = {Suo, Kun and Son, Junggab and Cheng, Dazhao and Chen, Wei and Baidya, Sabur},
	urldate = {2024-05-15},
	date = {2021-09},
	langid = {english},
	file = {Suo et al. - 2021 - Tackling Cold Start of Serverless Applications by .pdf:/home/larissa/Zotero/storage/8KZBPTR4/Suo et al. - 2021 - Tackling Cold Start of Serverless Applications by .pdf:application/pdf},
}

@inproceedings{bermbach_future_2021,
	location = {San Francisco, {CA}, {USA}},
	title = {On the Future of Cloud Engineering},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66544-970-0},
	url = {https://ieeexplore.ieee.org/document/9610599/},
	doi = {10.1109/IC2E52221.2021.00044},
	abstract = {Ever since the commercial offerings of the Cloud started appearing in 2006, the landscape of cloud computing has been undergoing remarkable changes with the emergence of many different types of service offerings, developer productivity enhancement tools, and new application classes as well as the manifestation of cloud functionality closer to the user at the edge. The notion of utility computing, however, has remained constant throughout its evolution, which means that cloud users always seek to save costs of leasing cloud resources while maximizing their use. On the other hand, cloud providers try to maximize their proﬁts while assuring service-level objectives of the cloud-hosted applications and keeping operational costs low. All these outcomes require systematic and sound cloud engineering principles. The aim of this paper is to highlight the importance of cloud engineering, survey the landscape of best practices in cloud engineering and its evolution, discuss many of the existing cloud engineering advances, and identify both the inherent technical challenges and research opportunities for the future of cloud computing in general and cloud engineering in particular.},
	eventtitle = {2021 {IEEE} International Conference on Cloud Engineering ({IC}2E)},
	pages = {264--275},
	booktitle = {2021 {IEEE} International Conference on Cloud Engineering ({IC}2E)},
	publisher = {{IEEE}},
	author = {Bermbach, David and Chandra, Abhishek and Krintz, Chandra and Gokhale, Aniruddha and Slominski, Aleksander and Thamsen, Lauritz and Cavalcante, Everton and Guo, Tian and Brandic, Ivona and Wolski, Rich},
	urldate = {2024-05-15},
	date = {2021-10},
	langid = {english},
	file = {Bermbach et al. - 2021 - On the Future of Cloud Engineering.pdf:/home/larissa/Zotero/storage/JM4WC7GE/Bermbach et al. - 2021 - On the Future of Cloud Engineering.pdf:application/pdf},
}

@inproceedings{luo_behavior_2023,
	location = {Hong Kong, Hong Kong},
	title = {Behavior Tree-based Workflow Modeling and Scheduling for Serverless Edge Computing},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350339864},
	url = {https://ieeexplore.ieee.org/document/10272475/},
	doi = {10.1109/ICDCS57875.2023.00100},
	abstract = {Despite the popularity of Serverless computing, there are insufﬁcient efforts dedicated to Serverless workﬂows (i.e., Serverless function orchestration), particularly for Serverless edge computing. In this paper, we ﬁrst identify the challenges of deploying the state-ofthe-art cloud-oriented Serverless workﬂow scheduling on resource-constrained edge devices, then propose to model Serverless workﬂows with behavior trees, and ﬁnally reveal our key observations and preliminary results for behavior tree-based Serverless workﬂow scheduling.},
	eventtitle = {2023 {IEEE} 43rd International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {955--956},
	booktitle = {2023 {IEEE} 43rd International Conference on Distributed Computing Systems ({ICDCS})},
	publisher = {{IEEE}},
	author = {Luo, Ke and Ouyang, Tao and Zhou, Zhi and Chen, Xu},
	urldate = {2024-05-15},
	date = {2023-07},
	langid = {english},
	file = {Luo et al. - 2023 - Behavior Tree-based Workflow Modeling and Scheduli.pdf:/home/larissa/Zotero/storage/BDACJFXM/Luo et al. - 2023 - Behavior Tree-based Workflow Modeling and Scheduli.pdf:application/pdf},
}

@inproceedings{farahani_towards_2023,
	location = {Coimbra Portugal},
	title = {Towards Sustainable Serverless Processing of Massive Graphs on the Computing Continuum},
	isbn = {9798400700729},
	url = {https://dl.acm.org/doi/10.1145/3578245.3585331},
	doi = {10.1145/3578245.3585331},
	abstract = {With the ever-increasing volume of data and the demand to analyze and comprehend it, graph processing has become an essential approach for solving complex problems in various domains, like social networks, bioinformatics, and finance. Despite the potential benefits of current graph processing platforms, they often encounter difficulties supporting diverse workloads, models, and languages. Moreover, existing platforms suffer from limited portability and interoperability, resulting in redundant efforts and inefficient resource and energy utilization due to vendor and even platform lock-in. To bridge the aforementioned gaps, the Graph-Massivizer project, funded by the Horizon Europe research and innovation program, conducts research and develops a high-performance, scalable, and sustainable platform for information processing and reasoning based on the massive graph ({MG}) representation of extreme data. In this paper, we briefly introduce the Graph-Massivizer platform. We explore how the emerging serverless computing paradigm can be leveraged to devise a scalable graph analytics tool over a codesigned computing continuum infrastructure. Finally, we sketch seven crucial research questions in our design and outline three ongoing and future research directions for addressing them.},
	eventtitle = {{ICPE} '23: {ACM}/{SPEC} International Conference on Performance Engineering},
	pages = {221--226},
	booktitle = {Companion of the 2023 {ACM}/{SPEC} International Conference on Performance Engineering},
	publisher = {{ACM}},
	author = {Farahani, Reza and Kimovski, Dragi and Ristov, Sashko and Iosup, Alexandru and Prodan, Radu},
	urldate = {2024-05-15},
	date = {2023-04-15},
	langid = {english},
	file = {Farahani et al. - 2023 - Towards Sustainable Serverless Processing of Massi.pdf:/home/larissa/Zotero/storage/XRALU6RE/Farahani et al. - 2023 - Towards Sustainable Serverless Processing of Massi.pdf:application/pdf},
}

@misc{shi_dflow_2023,
	title = {{DFlow}: Efficient Dataflow-based Invocation Workflow Execution for Function-as-a-Service},
	url = {http://arxiv.org/abs/2306.11043},
	shorttitle = {{DFlow}},
	abstract = {The Serverless Computing is becoming increasingly popular due to its ease of use and fine-grained billing. These features make it appealing for stateful application or serverless workflow. However, current serverless workflow systems utilize a controlflow-based invocation pattern to invoke functions. In this execution pattern, the function invocation depends on the state of the function. A function can only begin executing once all its precursor functions have completed. As a result, this pattern may potentially lead to longer end-to-end execution time.},
	number = {{arXiv}:2306.11043},
	publisher = {{arXiv}},
	author = {Shi, Xiaoxiang and Li, Chao and Li, Zijun and Liu, Zihan and Sheng, Dianmo and Chen, Quan and Leng, Jingwen and Guo, Minyi},
	urldate = {2024-05-15},
	date = {2023-07-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2306.11043 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Operating Systems},
	file = {Shi et al. - 2023 - DFlow Efficient Dataflow-based Invocation Workflo.pdf:/home/larissa/Zotero/storage/JIJF5ZGC/Shi et al. - 2023 - DFlow Efficient Dataflow-based Invocation Workflo.pdf:application/pdf},
}

@inproceedings{ristov_colder_2022,
	location = {Salerno Italy},
	title = {Colder Than the Warm Start and Warmer Than the Cold Start! Experience the Spawn Start in {FaaS} Providers},
	isbn = {978-1-4503-9280-8},
	url = {https://dl.acm.org/doi/10.1145/3524053.3542751},
	doi = {10.1145/3524053.3542751},
	abstract = {Many researchers reported considerable delay of up to a few seconds when invoking serverless functions for the first time. This phenomenon, which is known as a cold start, affects even more when users are running multiple serverless functions orchestrated in a workflow. However, in many cases users need to instantly spawn numerous serverless functions, usually as a part of parallel loops. In this paper, we introduce the spawn start and analyze the behavior of three Function-as-a-Service ({FaaS}) providers {AWS} Lambda, Google Cloud Functions, and {IBM} Cloud Functions when running parallel loops, both as warm and cold starts. We conducted a series of experiments and observed three insights that are beneficial for the research community. Firstly, cold start on {IBM} Cloud Functions, which is up to 2 s delay compared to the warm start, is negligible compared to the spawn start because the latter generates additional 20 s delay. Secondly, Google Cloud Functions’ cold start is "warmer" than the warm start of the same serverless function. Finally, while Google Cloud Functions and {IBM} Cloud Functions run the same serverless function with low concurrency faster than {AWS} Lambda, the spawn start effect on Google Cloud Functions and {IBM} Cloud Functions makes {AWS} the preferred provider when spawning numerous serverless functions.},
	eventtitle = {{PODC} '22: {ACM} Symposium on Principles of Distributed Computing},
	pages = {35--39},
	booktitle = {Proceedings of the 2022 Workshop on Advanced tools, programming languages, and {PLatforms} for Implementing and Evaluating algorithms for Distributed systems},
	publisher = {{ACM}},
	author = {Ristov, Sashko and Hollaus, Christian and Hautz, Mika},
	urldate = {2024-05-15},
	date = {2022-07-25},
	langid = {english},
	file = {Ristov et al. - 2022 - Colder Than the Warm Start and Warmer Than the Col.pdf:/home/larissa/Zotero/storage/LTQTJ9YD/Ristov et al. - 2022 - Colder Than the Warm Start and Warmer Than the Col.pdf:application/pdf},
}

@article{scheuner_function-as--service_2020,
	title = {Function-as-a-Service performance evaluation: A multivocal literature review},
	volume = {170},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121220301527},
	doi = {10.1016/j.jss.2020.110708},
	shorttitle = {Function-as-a-Service performance evaluation},
	abstract = {Function-as-a-Service ({FaaS}) is one form of the serverless cloud computing paradigm and is defined through {FaaS} platforms (e.g., {AWS} Lambda) executing event-triggered code snippets (i.e., functions). Many studies that empirically evaluate the performance of such {FaaS} platforms have started to appear but we are currently lacking a comprehensive understanding of the overall domain. To address this gap, we conducted a multivocal literature review ({MLR}) covering 112 studies from academic (51) and grey (61) literature. We find that existing work mainly studies the {AWS} Lambda platform and focuses on micro-benchmarks using simple functions to measure {CPU} speed and {FaaS} platform overhead (i.e., container cold starts). Further, we discover a mismatch between academic and industrial sources on tested platform configurations, find that function triggers remain insufficiently studied, and identify {HTTP} {API} gateways and cloud storages as the most used external service integrations. Following existing guidelines on experimentation in cloud systems, we discover many flaws threatening the reproducibility of experiments presented in the surveyed studies. We conclude with a discussion of gaps in literature and highlight methodological suggestions that may serve to improve future {FaaS} performance evaluation studies.},
	pages = {110708},
	journaltitle = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Scheuner, Joel and Leitner, Philipp},
	urldate = {2024-05-15},
	date = {2020-12-01},
	keywords = {Performance, Cloud computing, Function-as-a-Service, Serverless, Benchmarking, Multivocal literature review},
	file = {ScienceDirect Snapshot:/home/larissa/Zotero/storage/NRBPC8WG/S0164121220301527.html:text/html;Submitted Version:/home/larissa/Zotero/storage/W5JK7LBU/Scheuner and Leitner - 2020 - Function-as-a-Service performance evaluation A mu.pdf:application/pdf},
}

@misc{ustiugov_expedited_2023,
	title = {Expedited Data Transfers for Serverless Clouds},
	url = {http://arxiv.org/abs/2309.14821},
	abstract = {Serverless computing has emerged as a popular cloud deployment paradigm. In serverless, the developers implement their application as a set of chained functions that form a workflow in which functions invoke each other. The cloud providers are responsible for automatically scaling the number of instances for each function on demand and forwarding the requests in a workflow to the appropriate function instance. Problematically, today’s serverless clouds lack efficient support for cross-function data transfers in a workflow, preventing the efficient execution of data-intensive serverless applications. In production clouds, functions transmit intermediate, i.e., ephemeral, data to other functions either as part of invocation {HTTP} requests (i.e., inline) or via third-party services, such as {AWS} S3 storage or {AWS} {ElastiCache} in-memory cache. The former approach is restricted to small transfer sizes, while the latter supports arbitrary transfers but suffers from performance and cost overheads.},
	number = {{arXiv}:2309.14821},
	publisher = {{arXiv}},
	author = {Ustiugov, Dmitrii and Jesalpura, Shyam and Alper, Mert Bora and Baczun, Michal and Feyzkhanov, Rustem and Bugnion, Edouard and Grot, Boris and Kogias, Marios},
	urldate = {2024-05-15},
	date = {2023-09-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2309.14821 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Operating Systems, 68, D.4.4},
	file = {Ustiugov et al. - 2023 - Expedited Data Transfers for Serverless Clouds.pdf:/home/larissa/Zotero/storage/3ZCZFHGX/Ustiugov et al. - 2023 - Expedited Data Transfers for Serverless Clouds.pdf:application/pdf},
}

@misc{von_laszewski_hybrid_2022,
	title = {Hybrid Reusable Computational Analytics Workflow Management with Cloudmesh},
	url = {http://arxiv.org/abs/2210.16941},
	abstract = {In this paper, we summarize our effort to create and utilize a simple framework to coordinate computational analytics tasks with the help of a workflow system. Our design is based on a minimalistic approach while at the same time allowing to access computational resources offered through the owner’s computer, {HPC} computing centers, cloud resources, and distributed systems in general. The access to this framework includes a simple {GUI} for monitoring and managing the workflow, a {REST} service, a command line interface, as well as a Python interface. The resulting framework was developed for several examples targeting benchmarks of {AI} applications on hybrid compute resources and as an educational tool for teaching scientists and students sophisticated concepts to execute computations on resources ranging from a single computer to many thousands of computers as part of on-premise and cloud infrastructure. We demonstrate the usefulness of the tool on a number of examples. The code is available as an open-source project in {GitHub} and is based on an easy-to-enhance tool called cloudmesh.},
	number = {{arXiv}:2210.16941},
	publisher = {{arXiv}},
	author = {von Laszewski, Gregor and Fleischer, J. P. and Fox, Geoffrey C.},
	urldate = {2024-05-15},
	date = {2022-10-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2210.16941 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {von Laszewski et al. - 2022 - Hybrid Reusable Computational Analytics Workflow M.pdf:/home/larissa/Zotero/storage/NTRB4L7M/von Laszewski et al. - 2022 - Hybrid Reusable Computational Analytics Workflow M.pdf:application/pdf},
}

@misc{liao_flock_2024,
	title = {Flock: A Low-Cost Streaming Query Engine on {FaaS} Platforms},
	url = {http://arxiv.org/abs/2312.16735},
	shorttitle = {Flock},
	abstract = {Existing serverless data analytics systems rely on external storage services like S3 for data shuffling and communication between cloud functions. While this approach provides the elasticity benefits of serverless computing, it incurs additional latency and cost overheads. We present Flock, a novel cloud-native streaming query engine that leverages the on-demand scalability of {FaaS} platforms for real-time data analytics. Flock utilizes function invocation payloads for efficient data exchange, eliminating the need for external storage. This not only reduces latency and cost but also simplifies the architecture by removing the requirement for a centralized coordinator. Flock employs a template-based approach to dynamically create cloud functions for each query stage and a function group mechanism for handling data aggregation and shuffling. It supports both {SQL} and {DataFrame} {APIs}, making it easy to use. Our evaluation shows that Flock provides significant performance gains and cost savings compared to existing serverless and serverful streaming systems. It outperforms Apache Flink by 10-20x in cost while achieving similar latency and throughput.},
	number = {{arXiv}:2312.16735},
	publisher = {{arXiv}},
	author = {Liao, Gang and Deshpande, Amol and Abadi, Daniel J.},
	urldate = {2024-05-15},
	date = {2024-04-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2312.16735 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Databases},
	file = {Liao et al. - 2024 - Flock A Low-Cost Streaming Query Engine on FaaS P.pdf:/home/larissa/Zotero/storage/QQNLK9AL/Liao et al. - 2024 - Flock A Low-Cost Streaming Query Engine on FaaS P.pdf:application/pdf},
}

@inproceedings{nguyen_storm-rts_2023,
	location = {Chicago, {IL}, {USA}},
	title = {Storm-{RTS}: Stream Processing with Stable Performance for Multi-Cloud and Cloud-edge},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350304817},
	url = {https://ieeexplore.ieee.org/document/10254965/},
	doi = {10.1109/CLOUD60044.2023.00015},
	shorttitle = {Storm-{RTS}},
	abstract = {Stream Processing Engines ({SPEs}) traditionally deploy applications on a set of shared workers (e.g., threads, processes, or containers) requiring complex performance management by {SPEs} and application developers. We explore a new approach that replaces workers with Rate-based Abstract Machines ({RBAMs}). This allows {SPEs} to translate stream operations into {FaaS} invocations, and exploit guaranteed invocation rates to manage performance. This approach enables {SPE} applications to achieve transparent and predictable performance.},
	eventtitle = {2023 {IEEE} 16th International Conference on Cloud Computing ({CLOUD})},
	pages = {45--57},
	booktitle = {2023 {IEEE} 16th International Conference on Cloud Computing ({CLOUD})},
	publisher = {{IEEE}},
	author = {Nguyen, Hai Duc and Chien, Andrew A.},
	urldate = {2024-05-15},
	date = {2023-07},
	langid = {english},
	file = {Nguyen and Chien - 2023 - Storm-RTS Stream Processing with Stable Performan.pdf:/home/larissa/Zotero/storage/CUYX285B/Nguyen and Chien - 2023 - Storm-RTS Stream Processing with Stable Performan.pdf:application/pdf},
}

@article{liang_survey_2024,
	title = {A Survey on Spatio-Temporal Big Data Analytics Ecosystem: Resource Management, Processing Platform, and Applications},
	volume = {10},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {2332-7790, 2372-2096},
	url = {https://ieeexplore.ieee.org/document/10356753/},
	doi = {10.1109/TBDATA.2023.3342619},
	shorttitle = {A Survey on Spatio-Temporal Big Data Analytics Ecosystem},
	abstract = {With the rapid evolution of the Internet, Internet of Things ({IoT}), and geographic information systems ({GIS}), spatiotemporal Big Data ({STBD}) is experiencing exponential growth, marking the onset of the {STBD} era. Recent studies have concentrated on developing algorithms and techniques for the collection, management, storage, processing, analysis, and visualization of {STBD}. Researchers have made signiﬁcant advancements by enhancing {STBD} handling techniques, creating novel systems, and integrating spatio-temporal support into existing systems. However, these studies often neglect resource management and system optimization, crucial factors for enhancing the efﬁciency of {STBD} processing and applications. Additionally, the transition of {STBD} to the innovative Cloud-Edge-End uniﬁed computing system needs to be noticed. In this survey, we comprehensively explore the entire ecosystem of {STBD} analytics systems. We delineate the {STBD} analytics ecosystem and categorize the technologies used to process {GIS} data into ﬁve modules: {STBD}, computation resources, processing platform, resource management, and applications. Speciﬁcally, we subdivide {STBD} and its applications into geoscience-oriented and human-social activity-oriented. Within the processing platform module, we further categorize it into the data management layer ({DBMS}-{GIS}), data processing layer ({BigData}-{GIS}), data analysis layer ({AI}-{GIS}), and cloud native layer (Cloud-{GIS}). The resource management module and each layer in the processing platform are classiﬁed into three categories: task-oriented, resource-oriented, and cloud-based. Finally, we propose research agendas for potential future developments.},
	pages = {174--193},
	number = {2},
	journaltitle = {{IEEE} Transactions on Big Data},
	shortjournal = {{IEEE} Trans. Big Data},
	author = {Liang, Huanghuang and Zhang, Zheng and Hu, Chuang and Gong, Yili and Cheng, Dazhao},
	urldate = {2024-05-15},
	date = {2024-04},
	langid = {english},
	file = {Liang et al. - 2024 - A Survey on Spatio-Temporal Big Data Analytics Eco.pdf:/home/larissa/Zotero/storage/36SHMNSU/Liang et al. - 2024 - A Survey on Spatio-Temporal Big Data Analytics Eco.pdf:application/pdf},
}

@article{arjona_triggerflow_2021,
	title = {Triggerflow: Trigger-based orchestration of serverless workflows},
	volume = {124},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001989},
	doi = {10.1016/j.future.2021.06.004},
	shorttitle = {Triggerflow},
	abstract = {As more applications are being moved to the Cloud thanks to serverless computing, it is increasingly necessary to support the native life cycle execution of those applications in the data center. But existing cloud orchestration systems either focus on short-running workflows (like {IBM} Composer or Amazon Step Functions Express Workflows) or impose considerable overheads for synchronizing massively parallel jobs (Azure Durable Functions, Amazon Step Functions). None of them are open systems enabling extensible interception and optimization of custom workflows. We present Triggerflow: an extensible Trigger-based Orchestration architecture for serverless workflows. We demonstrate that Triggerflow is a novel serverless building block capable of constructing different reactive orchestrators (State Machines, Directed Acyclic Graphs, Workflow as code, Federated Learning orchestrator). We also validate that it can support high-volume event processing workloads, auto-scale on demand with scale down to zero when not used, and transparently guarantee fault tolerance and efficient resource usage when orchestrating long running scientific workflows.},
	pages = {215--229},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Arjona, Aitor and López, Pedro García and Sampé, Josep and Slominski, Aleksander and Villard, Lionel},
	urldate = {2024-05-17},
	date = {2021-11-01},
	keywords = {Serverless, Orchestration, Event-based},
	file = {ScienceDirect Snapshot:/home/larissa/Zotero/storage/774SUB7K/S0167739X21001989.html:text/html;Submitted Version:/home/larissa/Zotero/storage/YWNIDGB8/Arjona et al. - 2021 - Triggerflow Trigger-based orchestration of server.pdf:application/pdf},
}

@article{li_soda_2022,
	title = {{SoDa}: A Serverless-Oriented Deadline-Aware Workflow Scheduling Engine for {IoT} Applications in Edge Clouds},
	volume = {2022},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1530-8677, 1530-8669},
	url = {https://www.hindawi.com/journals/wcmc/2022/7862911/},
	doi = {10.1155/2022/7862911},
	shorttitle = {{SoDa}},
	abstract = {As a coordination tool, workflow with a large number of interdependent tasks has increasingly become a new paradigm for orchestrating computationally intensive tasks in large-scale and complex Internet of Things ({IoT}) applications. Serverless computing has also recently been applied to real-world problems at the network edge as well, primarily aimed at event based {IoT} applications. However, the existing workflow scheduling algorithm based on the virtual machine resource model is inefficient in ensuring the {QoS} (Quality of Service) of users on the serverless platform. In this paper, we design an elastic workflow scheduling framework in edge clouds called {EWSF} based on the serverless architecture. In addition, we propose a serverless-oriented deadline-aware workflow scheduling algorithm called {SoDa}. Furthermore, we implemented the {EWSF} prototype based on Knative and Kubernetes and integrated {SoDa} as the scheduling engine. The performance of {SoDa} has been verified on the experimental platform in comparison with six counterparts. The experiment results show that {SoDa} adapts to various scheduling environments and achieves better performance in terms of overall makespan and execution success rate. In the case of tight cluster resources, {SoDa} improves the overall makespan and success rate by 10.4\% and 55\%, respectively, compared with the second-best algorithm.},
	pages = {1--20},
	journaltitle = {Wireless Communications and Mobile Computing},
	shortjournal = {Wireless Communications and Mobile Computing},
	author = {Li, Dazhi and Duan, Jiaang and Yao, Yan and Qian, Shiyou and Zhou, Jie and Xue, Guangtao and Cao, Jian},
	editor = {Ansari, Mohd Dilshad},
	urldate = {2024-05-17},
	date = {2022-10-07},
	langid = {english},
	file = {Li et al. - 2022 - SoDa A Serverless-Oriented Deadline-Aware Workflo.pdf:/home/larissa/Zotero/storage/XVUWREEN/Li et al. - 2022 - SoDa A Serverless-Oriented Deadline-Aware Workflo.pdf:application/pdf},
}

@misc{wen_empirical_2021,
	title = {An Empirical Study on Serverless Workflow Service},
	url = {http://arxiv.org/abs/2101.03513},
	doi = {10.48550/arXiv.2101.03513},
	abstract = {Along with the wide-adoption of Serverless Computing, more and more applications are developed and deployed on cloud platforms. Major cloud providers present their serverless workflow services to orchestrate serverless functions, making it possible to perform complex applications effectively. A comprehensive instruction is necessary to help developers understand the pros and cons, and make better choices among these serverless workflow services. However, the characteristics of these serverless workflow services have not been systematically analyzed. To fill the knowledge gap, we survey four mainstream serverless workflow services, investigating their characteristics and performance. Specifically, we review their official documents and compare them in terms of seven dimensions including programming model, state management, etc. Then, we compare the performance (i.e., execution time of functions, execution time of workflows, orchestration overhead of workflows) under various experimental settings considering activity complexity and data-flow complexity of workflows, as well as function complexity of serverless functions. Finally, we discuss and verify the service effectiveness for two actual workloads. Our findings could help application developers and serverless providers to improve the development efficiency and user experience.},
	number = {{arXiv}:2101.03513},
	publisher = {{arXiv}},
	author = {Wen, Jinfeng and Liu, Yi},
	urldate = {2024-05-17},
	date = {2021-01-12},
	eprinttype = {arxiv},
	eprint = {2101.03513 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/home/larissa/Zotero/storage/TFB9NJXY/Wen and Liu - 2021 - An Empirical Study on Serverless Workflow Service.pdf:application/pdf;arXiv.org Snapshot:/home/larissa/Zotero/storage/ZBMEWVM4/2101.html:text/html},
}

@article{risco_serverless_2021,
	title = {Serverless Workflows for Containerised Applications in the Cloud Continuum},
	volume = {19},
	issn = {1570-7873, 1572-9184},
	url = {https://link.springer.com/10.1007/s10723-021-09570-2},
	doi = {10.1007/s10723-021-09570-2},
	abstract = {Abstract
            This paper introduces an open-source platform to support serverless computing for scientific data-processing workflow-based applications across the Cloud continuum (i.e. simultaneously involving both on-premises and public Cloud platforms to process data captured at the edge). This is achieved via dynamic resource provisioning for {FaaS} platforms compatible with scale-to-zero approaches that minimise resource usage and cost for dynamic workloads with different elasticity requirements. The platform combines the usage of dynamically deployed auto-scaled Kubernetes clusters on on-premises Clouds and automated Cloud bursting into {AWS} Lambda to achieve higher levels of elasticity. A use case in public health for smart cities is used to assess the platform, in charge of detecting people not wearing face masks from captured videos. Faces are blurred for enhanced anonymity in the on-premises Cloud and detection via Deep Learning models is performed in {AWS} Lambda for this data-driven containerised workflow. The results indicate that hybrid workflows across the Cloud continuum can efficiently perform local data processing for enhanced regulations compliance and perform Cloud bursting for increased levels of elasticity.},
	pages = {30},
	number = {3},
	journaltitle = {Journal of Grid Computing},
	shortjournal = {J Grid Computing},
	author = {Risco, Sebastián and Moltó, Germán and Naranjo, Diana M. and Blanquer, Ignacio},
	urldate = {2024-05-17},
	date = {2021-09},
	langid = {english},
	file = {Risco et al. - 2021 - Serverless Workflows for Containerised Application.pdf:/home/larissa/Zotero/storage/G3RRKK3Z/Risco et al. - 2021 - Serverless Workflows for Containerised Application.pdf:application/pdf},
}

@inproceedings{john_sweep_2019,
	location = {Auckland New Zealand},
	title = {{SWEEP}: Accelerating Scientific Research Through Scalable Serverless Workflows},
	isbn = {978-1-4503-7044-8},
	url = {https://dl.acm.org/doi/10.1145/3368235.3368839},
	doi = {10.1145/3368235.3368839},
	shorttitle = {{SWEEP}},
	abstract = {Scientific and commercial applications are increasingly being executed in the cloud, but the difficulties associated with cluster management render on-demand resources inaccessible or inefficient to many users. Recently, the serverless execution model, in which the provisioning of resources is abstracted from the user, has gained prominence as an alternative to traditional cyberinfrastructure solutions. With its inherent elasticity, the serverless paradigm constitutes a promising computational model for scientific workflows, allowing domain specialists to develop and deploy workflows that are subject to varying workloads and intermittent usage without the overhead of infrastructure maintenance. We present the Serverless Workflow Enablement and Execution Platform ({SWEEP}), a cloud-agnostic workflow management system with a purely serverless execution model that allows users to define, run and monitor generic cloud-native workflows. We demonstrate the use of {SWEEP} on workflows from two disparate scientific domains and present an evaluation of performance and scaling.},
	eventtitle = {{UCC} '19: {IEEE}/{ACM} 12th International Conference on Utility and Cloud Computing},
	pages = {43--50},
	booktitle = {Proceedings of the 12th {IEEE}/{ACM} International Conference on Utility and Cloud Computing Companion},
	publisher = {{ACM}},
	author = {John, Aji and Ausmees, Kristiina and Muenzen, Kathleen and Kuhn, Catherine and Tan, Amanda},
	urldate = {2024-05-17},
	date = {2019-12-02},
	langid = {english},
	file = {John et al. - 2019 - SWEEP Accelerating Scientific Research Through Sc.pdf:/home/larissa/Zotero/storage/WLZMNT2M/John et al. - 2019 - SWEEP Accelerating Scientific Research Through Sc.pdf:application/pdf},
}

@article{mathew_pattern-based_2024,
	title = {Pattern-based serverless data processing pipelines for Function-as-a-Service orchestration systems},
	volume = {154},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004855},
	doi = {10.1016/j.future.2023.12.026},
	abstract = {The serverless cloud computing execution model presents an opportunity for implementing data processing pipelines in a cost efficient manner with minimum or no consideration about managing their operational aspects. In doing so, however, a major concern that emerges is the adoption of provider-specific languages required for orchestrating serverless functions and supporting services into the higher order logic of pipelines. As a result, the vendor lock-in problem inherent to cloud computing becomes further intensified. In this work we propose an approach to mitigate this issue through the adoption and adaptation of well-known patterns from the literature as the means for modeling the pipeline logic. By providing mappings from these patterns to vendor-specific orchestration language constructs we allow for efficient realization of provider-agnostic pipeline models as provider-specific executable workflows. An industrial case study provides evidence towards the suitability of our proposal for practical purposes.},
	pages = {87--100},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Mathew, Anil and Andrikopoulos, Vasilios and Blaauw, Frank J. and Karastoyanova, Dimka},
	urldate = {2024-05-17},
	date = {2024-05-01},
	keywords = {Serverless, Enterprise integration patterns, Function orchestration, Function-as-a-Service ({FaaS}), Workflow patterns},
	file = {ScienceDirect Snapshot:/home/larissa/Zotero/storage/FYL4TNUC/S0167739X23004855.html:text/html},
}

@article{bharti_reactivefnj_2023,
	title = {{ReactiveFnJ}: A choreographed model for Fork-Join Workflow in Serverless Computing},
	volume = {12},
	issn = {2192-113X},
	url = {https://doi.org/10.1186/s13677-023-00429-3},
	doi = {10.1186/s13677-023-00429-3},
	shorttitle = {{ReactiveFnJ}},
	abstract = {Function-as-a-Service ({FaaS}) is an event-based reactive programming model where functions run in ephemeral stateless containers for short duration. For building complex serverless applications, function composition is crucial to coordinate and synchronize the workflow of an application. Some serverless orchestration systems exist, but they are in their primitive state and do not provide inherent support for non-trivial workflows like, Fork-Join. To address this gap, we propose a fully serverless and scalable design model {ReactiveFnJ} for Fork-Join workflow. The intent of this work is to illustrate a design which is completely choreographed, reactive, asynchronous, and represents a dynamic composition model for serverless applications based on Fork-Join workflow. Our design uses two innovative patterns, namely, Relay Composition and Master-Worker Composition to solve execution time-out challenges. As a Proof-of-Concept ({PoC}), the prototypical implementation of Split-Sort-Merge use case, based on Fork-Join workflow is discussed and evaluated. The {ReactiveFnJ} handles embarrassingly parallel computations, and its design does not depend on any external orchestration services, messaging services, and queue services. {ReactiveFnJ} facilitates in designing fully automated pipelines for distributed data processing systems, satisfying the Serverless Trilemma in true essence. A file of any size can be processed using our effective and extensible design without facing execution time-out challenges. The proposed model is generic and can be applied to a wide range of serverless applications that are based on the Fork-Join workflow pattern. It fosters the choreographed serverless composition for complex workflows. The proposed design model is useful for software engineers and developers in industry and commercial organizations, total solution vendors and academic researchers.},
	pages = {63},
	number = {1},
	journaltitle = {Journal of Cloud Computing},
	shortjournal = {J Cloud Comp},
	author = {Bharti, Urmil and Goel, Anita and Gupta, S. C.},
	urldate = {2024-05-17},
	date = {2023-04-24},
	langid = {english},
	keywords = {Distributed computing, Serverless computing, {FaaS}, Orchestration, Choreography, Event-driven function composition, Fork and Join, Parallel computing},
	file = {Full Text PDF:/home/larissa/Zotero/storage/UNGAVGLE/Bharti et al. - 2023 - ReactiveFnJ A choreographed model for Fork-Join W.pdf:application/pdf},
}

@inproceedings{yang_faasctdo_2023,
	location = {Chicago, {IL}, {USA}},
	title = {{FaaSCTDO}: Collaborative Task-Data Orchestration for Serverless Workflows},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350304817},
	url = {https://ieeexplore.ieee.org/document/10254972/},
	doi = {10.1109/CLOUD60044.2023.00070},
	shorttitle = {{FaaSCTDO}},
	abstract = {The use of Function-as-a-Service ({FaaS}) platforms for executing complex serverless workﬂows has gained signiﬁcant popularity. However, the stateless nature of {FaaS} requires functions to rely on remote storage to store their state, leading to performance overhead and reduced eﬃciency. Previous approaches using excess memory resources as caches can cause resource contention and decreased task execution performance. Moreover, the impact of workﬂow orchestration approaches on latency has not been fully explored. To address these challenges, we propose {FaaSCTDO}, a framework for collaborative task and data orchestration in serverless workﬂows. {FaaSCTDO} provides developers with comprehensive lifecycle management capabilities, treating data as orchestratable objects. It groups tasks and data with high data correlation together and allocates them on the same server node, leveraging data locality to expedite function access to data.},
	eventtitle = {2023 {IEEE} 16th International Conference on Cloud Computing ({CLOUD})},
	pages = {526--528},
	booktitle = {2023 {IEEE} 16th International Conference on Cloud Computing ({CLOUD})},
	publisher = {{IEEE}},
	author = {Yang, Neng and Zhang, Haitao and Zhang, Yepeng},
	urldate = {2024-05-17},
	date = {2023-07},
	langid = {english},
	file = {Yang et al. - 2023 - FaaSCTDO Collaborative Task-Data Orchestration fo.pdf:/home/larissa/Zotero/storage/Y486P6V2/Yang et al. - 2023 - FaaSCTDO Collaborative Task-Data Orchestration fo.pdf:application/pdf},
}

@inproceedings{sicari_openwolf_2022,
	title = {{OpenWolf}: A Serverless Workflow Engine for Native Cloud-Edge Continuum},
	url = {https://ieeexplore.ieee.org/abstract/document/9927926},
	doi = {10.1109/DASC/PiCom/CBDCom/Cy55231.2022.9927926},
	shorttitle = {{OpenWolf}},
	abstract = {Nowadays, Serverless computing is emerging as one of the most used Cloud services. In particular, the Function as Service ({FaaS}) is bringing to Cloud consumers, developers, and devops many advantages in terms of service costs, speed of development, and ease of deployment. In fact, it stands to be a key technology for enabling the Cloud-Edge Continuum. Regardless of these features, it is still not possible to build {FaaS} native applications without a Cloud broker that coordinates the functions. Therefore, {FaaS} usage is limited to very simple and specific jobs. In this work, we brush up on the concept of Scientific Workflow using the {FaaS} paradigm, in order to realize full Native Serverless Workflows-based applications. We define a custom Workflow Manifest {DSL} used to describe function interactions, then we describe the implementation of an agent able to deploy architecture-independent functions and coordinate them according to the Manifest. Finally, federating the Cloud-Fog-Edge tiers in a single Continuum environment, we allow functions to take advantage of the Continuum tier’s characteristics where they are deployed. This project is called {OpenWolf}, it’s repository is published on {GitHub}, under {GNU} General Public License v3.0.},
	eventtitle = {2022 {IEEE} Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress ({DASC}/{PiCom}/{CBDCom}/{CyberSciTech})},
	pages = {1--8},
	booktitle = {2022 {IEEE} Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress ({DASC}/{PiCom}/{CBDCom}/{CyberSciTech})},
	author = {Sicari, Christian and Carnevale, Lorenzo and Galletta, Antonino and Villari, Massimo},
	urldate = {2024-05-17},
	date = {2022-09},
	keywords = {Costs, Engines, Big Data, Serverless computing, {FaaS}, Serverless, {DSL}, Workflow, Licenses, Brushes, Cloud-Edge Continuum, {FaaS} Composition},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/BW33VTFU/9927926.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/K433LH6H/Sicari et al. - 2022 - OpenWolf A Serverless Workflow Engine for Native .pdf:application/pdf},
}

@inproceedings{farahani_serverless_2024,
	location = {London United Kingdom},
	title = {Serverless Workflow Management on the Computing Continuum: A Mini-Survey},
	isbn = {9798400704451},
	url = {https://dl.acm.org/doi/10.1145/3629527.3652901},
	doi = {10.1145/3629527.3652901},
	shorttitle = {Serverless Workflow Management on the Computing Continuum},
	abstract = {The growing desire among application providers for a cost model based on pay-per-use, combined with the need for a seamlessly integrated platform to manage the complex workflows of their applications, has spurred the emergence of a promising computing paradigm known as serverless computing. Although serverless computing was initially considered for cloud environments, it has recently been extended to other layers of the computing continuum, i.e., edge and fog. This extension emphasizes that the proximity of computational resources to data sources can further reduce costs and improve performance and energy efficiency. However, orchestrating the computing continuum in complex application workflows, including a set of serverless functions, introduces new challenges. This paper investigates the opportunities and challenges introduced by serverless computing for workflow management systems ({WMS}) on the computing continuum. In addition, the paper provides a taxonomy of state-of-the-art {WMSs} and reviews their capabilities.},
	eventtitle = {{ICPE} '24: 15th {ACM}/{SPEC} International Conference on Performance Engineering},
	pages = {146--150},
	booktitle = {Companion of the 15th {ACM}/{SPEC} International Conference on Performance Engineering},
	publisher = {{ACM}},
	author = {Farahani, Reza and Loh, Frank and Roman, Dumitru and Prodan, Radu},
	urldate = {2024-05-17},
	date = {2024-05-07},
	langid = {english},
	file = {Farahani et al. - 2024 - Serverless Workflow Management on the Computing Co.pdf:/home/larissa/Zotero/storage/U4KCXUY4/Farahani et al. - 2024 - Serverless Workflow Management on the Computing Co.pdf:application/pdf},
}

@inproceedings{shahidi_cross-platform_2021,
	title = {Cross-Platform Performance Evaluation of Stateful Serverless Workflows},
	url = {https://ieeexplore.ieee.org/abstract/document/9668304},
	doi = {10.1109/IISWC53511.2021.00017},
	abstract = {Serverless computing, with its inherent event-driven design along with instantaneous scalability due to cloud-provider managed infrastructure, is starting to become a de-facto model for deploying latency critical user-interactive services. However, as much as they are suitable for event-driven services, their stateless nature is a major impediment for deploying long-running stateful applications. While commercial cloud providers offer a variety of solutions that club serverless functions along with intermediate storage to maintain application state, they are still far from optimized for deploying stateful applications at scale. More specifically, factors such as storage latency and scalability, network bandwidth, and deployment costs play a crucial role in determining whether current serverless applications are suitable for stateful workloads. In this paper, we evaluate the two widely-used stateful server-less offerings, Azure Durable functions and {AWS} Step functions, to quantify their effectiveness for implementing complex stateful workflows. We conduct a detailed measurement-driven characterization study with two real-world use cases, machine learning pipelines (inference and training) and video analytics, in order to characterize the different performance latency and cost tradeoffs. We observe from our experiments that {AWS} is suitable for workloads with higher degree of parallelism, while Azure durable entities offer a simplified framework that enables quicker application development. Overall, {AWS} is 89\% more expensive than Azure for machine learning training application while Azure is 2× faster than {AWS} for the machine learning inference application. Our results indicate that Azure durable is extremely inefficient in implementing parallel processing. Furthermore, we summarize the key findings from our characterization, which we believe to be insightful for any cloud tenant who has the problem of choosing an appropriate cloud vendor and offering, when deploving stateful workloads on serverless platforms,},
	eventtitle = {2021 {IEEE} International Symposium on Workload Characterization ({IISWC})},
	pages = {63--73},
	booktitle = {2021 {IEEE} International Symposium on Workload Characterization ({IISWC})},
	author = {Shahidi, Narges and Gunasekaran, Jashwant Raj and Kandemir, Mahmut Taylan},
	urldate = {2024-05-17},
	date = {2021-11},
	keywords = {Costs, Scalability, Performance evaluation, Pipelines, Serverless Computing, {FaaS}, {AWS}, Training, {AWS} Step Function, Azure, Azure Durable Function, Current measurement, Stateful, Visual analytics},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/5BKFI4LX/9668304.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/CWHCL7QP/Shahidi et al. - 2021 - Cross-Platform Performance Evaluation of Stateful .pdf:application/pdf},
}

@article{ristov_afcl_2021,
	title = {{AFCL}: An Abstract Function Choreography Language for serverless workflow specification},
	volume = {114},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X20302648},
	doi = {10.1016/j.future.2020.08.012},
	shorttitle = {{AFCL}},
	abstract = {Serverless workflow applications or function choreographies ({FCs}), which connect serverless functions by data- and control-flow, have gained considerable momentum recently to create more sophisticated applications as part of Function-as-a-Service ({FaaS}) platforms. Initial experimental analysis of the current support for {FCs} uncovered important weaknesses, including provider lock-in, and limited support for important data-flow and control-flow constructs. To overcome some of these weaknesses, we introduce the Abstract Function Choreography Language ({AFCL}) for describing {FCs} at a high-level of abstraction, which abstracts the function implementations from the developer. {AFCL} is a {YAML}-based language that supports a rich set of constructs to express advanced control-flow (e.g. {parallelFor} loops, parallel sections, dynamic loop iterations counts) and data-flow (e.g multiple input and output parameters of functions, {DAG}-based data-flow). We introduce data collections which can be distributed to loop iterations and parallel sections that may substantially reduce the delays for function invocations due to reduced data transfers between functions. We also support asynchronous functions to avoid delays due to blocking functions. {AFCL} supports properties (e.g. expected size of function input data) and constraints (e.g. minimize execution time) for the user to optionally provide hints about the behavior of functions and {FCs} and to control the optimization by the underlying execution environment. We implemented a prototype {AFCL} environment that supports {AFCL} as input language with multiple backends ({AWS} Lambda and {IBM} Cloud Functions) thus avoiding provider lock-in which is a common problem in serverless computing. We created two realistic {FCs} from two different domains and encoded them with {AWS} Step Functions, {IBM} Composer and {AFCL}. Experimental results demonstrate that our current implementation of the {AFCL} environment substantially outperforms {AWS} Step Functions and {IBM} Composer in terms of development effort, economic costs, and makespan.},
	pages = {368--382},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Ristov, Sasko and Pedratscher, Stefan and Fahringer, Thomas},
	urldate = {2024-05-17},
	date = {2021-01-01},
	keywords = {Performance, {FaaS}, {AWS} Step Functions, Cost, {IBM} Composer},
	file = {Full Text:/home/larissa/Zotero/storage/2LYUFXRK/Ristov et al. - 2021 - AFCL An Abstract Function Choreography Language f.pdf:application/pdf;ScienceDirect Snapshot:/home/larissa/Zotero/storage/4I28JQ8E/S0167739X20302648.html:text/html},
}

@article{burckhardt_durable_2021,
	title = {Durable functions: semantics for stateful serverless},
	volume = {5},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3485510},
	doi = {10.1145/3485510},
	shorttitle = {Durable functions},
	abstract = {{SEBASTIAN} {BURCKHARDT}, Microsoft Research, {USA} {CHRIS} {GILLUM}, Microsoft Azure, {USA} {DAVID} {JUSTO}, Microsoft Azure, {USA} {KONSTANTINOS} {KALLAS}, University of Pennsylvania, {USA} {CONNOR} {MCMAHON}, Microsoft Azure, {USA} {CHRISTOPHER} S. {MEIKLEJOHN}, Carnegie Mellon University, {USA} Serverless, or Functions-as-a-Service ({FaaS}), is an increasingly popular paradigm for application development, as it provides implicit elastic scaling and load based billing. However, the weak execution guarantees and intrinsic compute-storage separation of {FaaS} create serious challenges when developing applications that require persistent state, reliable progress, or synchronization. This has motivated a new generation of serverless frameworks that provide stateful abstractions. For instance, Azure’s Durable Functions ({DF}) programming model enhances {FaaS} with actors, workflows, and critical sections. As a programming model, {DF} is interesting because it combines task and actor parallelism, which makes it suitable for a wide range of serverless applications. We describe {DF} both informally, using examples, and formally, using an idealized high-level model based on the untyped lambda calculus. Next, we demystify how the {DF} runtime can (1) execute in a distributed unreliable serverless environment with compute-storage separation, yet still conform to the fault-free high-level model, and (2) persist execution progress without requiring checkpointing support by the language runtime. To this end we define two progressively more complex execution models, which contain the compute-storage separation and the record-replay, and prove that they are equivalent to the high-level model. {CCS} Concepts: · Computing methodologies → Distributed computing methodologies; Distributed programming languages; · Software and its engineering → Concurrent programming structures; Frameworks.},
	pages = {1--27},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Burckhardt, Sebastian and Gillum, Chris and Justo, David and Kallas, Konstantinos and {McMahon}, Connor and Meiklejohn, Christopher S.},
	urldate = {2024-05-17},
	date = {2021-10-20},
	langid = {english},
	file = {Burckhardt et al. - 2021 - Durable functions semantics for stateful serverle.pdf:/home/larissa/Zotero/storage/VQLRMYXS/Burckhardt et al. - 2021 - Durable functions semantics for stateful serverle.pdf:application/pdf},
}

@article{ristov_faascinating_2022,
	title = {{FaaScinating} Resilience for Serverless Function Choreographies in Federated Clouds},
	volume = {19},
	rights = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {1932-4537, 2373-7379},
	url = {https://ieeexplore.ieee.org/document/9741250/},
	doi = {10.1109/TNSM.2022.3162036},
	abstract = {Cloud applications often beneﬁt from deployment on serverless technology Function-as-a-Service ({FaaS}), which may instantly spawn numerous functions and charges users for the period when serverless functions are running. Maximum beneﬁt is achieved when functions are orchestrated in a workﬂow or function choreographies ({FCs}). However, many provider limitations speciﬁc for {FaaS}, such as maximum concurrency or duration often increase the failure rate, which can severely hamper the execution of entire {FCs}. Current support for resilience is often limited to function retries or try-catch, which are applicable within the same cloud region only. To overcome these limitations, we introduce {rAFCL}, a middleware platform that maintains reliability of complex {FCs} in federated clouds. In order to support resilient {FC} execution under {rAFCL}, our model creates an alternative strategy for each function based on the required availability speciﬁed by the user. Alternative strategies are not restricted to the same cloud region, but may contain alternative functions across ﬁve providers, invoked concurrently in a single alternative plan or executed subsequently in multiple alternative plans. With this approach, {rAFCL} offers ﬂexibility in terms of cost-performance trade-off. We evaluated {rAFCL} by running three real-life applications across three cloud providers. Experimental results demonstrated that {rAFCL} outperforms the resilience of {AWS} Step Functions, increasing the success rate of entire {FC} by 53.45\%, while invoking only 3.94\% more functions with zero wasted function invocations. {rAFCL} signiﬁcantly improves availability of entire {FCs} to almost 1 and survives even after massive failures of alternative functions.},
	pages = {2440--2452},
	number = {3},
	journaltitle = {{IEEE} Transactions on Network and Service Management},
	shortjournal = {{IEEE} Trans. Netw. Serv. Manage.},
	author = {Ristov, Sasko and Kimovski, Dragi and Fahringer, Thomas},
	urldate = {2024-05-17},
	date = {2022-09},
	langid = {english},
	file = {Ristov et al. - 2022 - FaaScinating Resilience for Serverless Function Ch.pdf:/home/larissa/Zotero/storage/X62CYD8B/Ristov et al. - 2022 - FaaScinating Resilience for Serverless Function Ch.pdf:application/pdf},
}

@article{risco_rescheduling_2024,
	title = {Rescheduling serverless workloads across the cloud-to-edge continuum},
	volume = {153},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004764},
	doi = {10.1016/j.future.2023.12.015},
	abstract = {Serverless computing was a breakthrough in Cloud computing due to its high elasticity capabilities and fine-grained pay-per-use model offered by the main public Cloud providers. Meanwhile, open-source serverless platforms supporting the {FaaS} (Function as a Service) model allow users to take advantage of many of their benefits while operating on the on-premises platforms of organizations. This opens the possibility to deploy and exploit them on the different layers of the cloud-to-edge continuum, either on {IoT} (Internet of Things) devices located at the Edge (i.e. next to data acquisition devices), in on-premises clusters closer to the data sources (i.e. Fog computing) or directly on the Cloud. This paper presents two strategies to mitigate the overload that disparate data ingestion rates may cause in low-powered devices at the Edge or Fog layers. To this end, it is proposed to delegate and reschedule serverless jobs between the different layers of the cloud-to-edge continuum using an open-source platform for event-driven file processing. To demonstrate the performance of these strategies, a use case for fire detection is proposed that includes processing in the Fog via minified Kubernetes clusters located near the Edge, in the private Cloud via on-premises elastic clusters and, finally, in the public Cloud by using the {AWS} (Amazon Web Services) Lambda {FaaS} service. The results indicate that these strategies can mitigate overloads in use cases involving processing across the cloud-to-edge continuum by coordinating several layers of computing resources.},
	pages = {457--466},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Risco, Sebastián and Alarcón, Caterina and Langarita, Sergio and Caballer, Miguel and Moltó, Germán},
	urldate = {2024-05-17},
	date = {2024-04-01},
	keywords = {Cloud computing, Containers, Serverless computing, {FaaS}, Cloud-to-edge continuum, Kubernetes},
	file = {Risco et al. - 2024 - Rescheduling serverless workloads across the cloud.pdf:/home/larissa/Zotero/storage/L59D586Q/Risco et al. - 2024 - Rescheduling serverless workloads across the cloud.pdf:application/pdf;ScienceDirect Snapshot:/home/larissa/Zotero/storage/LUWUKN4H/S0167739X23004764.html:text/html},
}

@incollection{sicari_toward_2024,
	location = {Cham},
	title = {Toward the Edge-Cloud Continuum Through the Serverless Workflows},
	isbn = {978-3-031-42194-5},
	url = {https://doi.org/10.1007/978-3-031-42194-5_1},
	abstract = {Cloud-edge continuum is a recently recreated term used to refer to solutions that aim at taking advantage of a distributed cloud-edge infrastructure to run applications where they best fit at a given moment, which is dependent on infrastructure system, application, and constraints, eventually adapting that according to a change on those factors. Until now, there not exists a universal approach to build continuum native applications, and therefore different frameworks have been proposed focusing on the context and scenario where they are applied. In this work, we try to provide a reference architecture that allows to build relocable continuum native applications by the use of the function-as-a-service ({FaaS}) model, adapting the concept of scientific workflow, to build instead {FaaS} continuum workflows. By doing that, we will introduce a dictionary of terms and architecture guidelines to follow to get a continuum infrastructure. We will validate this work by describing a compliant implementation of this infrastructure called {OpenWolf}, a recently born open-source project that lets to design {FaaS}-based workflows for heterogeneous Kubernetes clusters. Moreover, we will also validate this project by designing and testing a continuous learning workflow used to keep security and safety in smart city.},
	pages = {1--18},
	booktitle = {Device-Edge-Cloud Continuum: Paradigms, Architectures and Applications},
	publisher = {Springer Nature Switzerland},
	author = {Sicari, Christian and Catalfamo, Alessio and Carnevale, Lorenzo and Galletta, Antonino and Celesti, Antonio and Fazio, Maria and Villari, Massimo},
	editor = {Savaglio, Claudio and Fortino, Giancarlo and Zhou, {MengChu} and Ma, Jianhua},
	urldate = {2024-05-17},
	date = {2024},
	langid = {english},
	doi = {10.1007/978-3-031-42194-5_1},
	keywords = {Scientific workflows, Serverless workflow, Cloud edge continuum, Distributed architecture, Distributed serverless, Event driven workflows, {FaaS} workflow, Function as a service},
	file = {Full Text PDF:/home/larissa/Zotero/storage/ZQ87ISN9/Sicari et al. - 2024 - Toward the Edge-Cloud Continuum Through the Server.pdf:application/pdf},
}

@inproceedings{wang_enhancing_2022,
	location = {Cham},
	title = {Enhancing Performance Modeling of Serverless Functions via Static Analysis},
	isbn = {978-3-031-20984-0},
	doi = {10.1007/978-3-031-20984-0_5},
	abstract = {Serverless computing leverages the design of complex applications as the composition of small, individual functions to simplify development and operations. However, this flexibility complicates reasoning about the trade-off between performance and costs, requiring accurate models to support prediction and configuration decisions. Established performance model inference from execution traces is typically more expensive for serverless applications due to the significantly larger topologies and numbers of parameters resulting from the higher fragmentation into small functions. On the other hand, individual functions tend to embed simpler logic than larger services, which enables inferring some structural information by reasoning directly from their source code. In this paper, we use static control and data flow analysis to extract topological and parametric dependencies among interacting functions from their source code. To enhance the accuracy of model parameterization, we devise an instrumentation strategy to infer performance profiles driven by code analysis. We then build a compact layered queueing network ({LQN}) model of the serverless workflow based on the static analysis and code profiling data. We evaluated our method on serverless workflows with several common composition patterns deployed on Azure Functions, showing it can accurately predict the performance of the application under different resource provisioning strategies and workloads with a mean error under 7.3\%.},
	pages = {71--88},
	booktitle = {Service-Oriented Computing},
	publisher = {Springer Nature Switzerland},
	author = {Wang, Runan and Casale, Giuliano and Filieri, Antonio},
	editor = {Troya, Javier and Medjahed, Brahim and Piattini, Mario and Yao, Lina and Fernández, Pablo and Ruiz-Cortés, Antonio},
	date = {2022},
	langid = {english},
	keywords = {Static analysis, Serverless computing, Code profiling, Layered queueing networks, Performance modeling},
	file = {Full Text PDF:/home/larissa/Zotero/storage/GKKKKZCP/Wang et al. - 2022 - Enhancing Performance Modeling of Serverless Funct.pdf:application/pdf},
}

@article{lin_modeling_2021,
	title = {Modeling and Optimization of Performance and Cost of Serverless Applications},
	volume = {32},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/abstract/document/9214428},
	doi = {10.1109/TPDS.2020.3028841},
	abstract = {Function-as-a-Service ({FaaS}) and serverless applications have proliferated significantly in recent years because of their high scalability, ease of resource management, and pay-as-you-go pricing model. However, cloud users are facing practical problems when they migrate their applications to the serverless pattern, which are the lack of analytical performance and billing model and the trade-off between limited budget and the desired quality of service of serverless applications. In this article, we fill this gap by proposing and answering two research questions regarding the prediction and optimization of performance and cost of serverless applications. We propose a new construct to formally define a serverless application workflow, and then implement analytical models to predict the average end-to-end response time and the cost of the workflow. Consequently, we propose a heuristic algorithm named Probability Refined Critical Path Greedy algorithm ({PRCP}) with four greedy strategies to answer two fundamental optimization questions regarding the performance and the cost. We extensively evaluate the proposed models by conducting experimentation on {AWS} Lambda and Step Functions. Our analytical models can predict the performance and cost of serverless applications with more than 98 percent accuracy. The {PRCP} algorithms can achieve the optimal configurations of serverless applications with 97 percent accuracy on average.},
	pages = {615--632},
	number = {3},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	author = {Lin, Changyuan and Khazaei, Hamzeh},
	urldate = {2024-05-17},
	date = {2021-03},
	note = {Conference Name: {IEEE} Transactions on Parallel and Distributed Systems},
	keywords = {performance modeling, Time factors, Analytical models, Computational modeling, Optimization, Cloud computing, Cloud serverless computing, cost modeling, cost optimization, {FAA}, performance optimization},
	file = {IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/K6AUY8XX/Lin and Khazaei - 2021 - Modeling and Optimization of Performance and Cost .pdf:application/pdf},
}

@inproceedings{bharti_sequential_2021,
	location = {Singapore},
	title = {Sequential Workflow in Production Serverless {FaaS} Orchestration Platform},
	isbn = {9789811584435},
	doi = {10.1007/978-981-15-8443-5_58},
	abstract = {Today, serverless computing or Function-as-a-Service ({FaaS}) is an excellent way to exploit cloud computing benefits due to its simplicity, billing, and innate elasticity. {FaaS} providers own the responsibility of server provisioning, administration, management and patching thus making developers less burdened and more productive. Serverless applications based on complex workflows involve multiple functions which have data dependencies on each other. This brings the concept of function orchestration in picture and makes it important in implementing any serverless application. In this context Amazon Web Services ({AWS} Step Function), International Business Machines ({IBM}) Cloud Function Sequences and Azure Durable Functions are currently available in market. In this paper, different sequential composition workflows like Reflexive, Fusion, Chaining, and Client Scheduled are implemented in {AWS} serverless platform and compared their performance in terms of execution duration. Successively, sequential workflows in different language runtime environments in {AWS} Step function and {IBM} Cloud Function Sequences are also analyzed. Experiments are also performed to observe the cold start behavior in {AWS} Lambda and ({IBM}) Cloud functions.},
	pages = {681--693},
	booktitle = {Proceedings of International Conference on Intelligent Computing, Information and Control Systems},
	publisher = {Springer},
	author = {Bharti, Urmil and Bajaj, Deepali and Goel, Anita and Gupta, S. C.},
	editor = {Pandian, A. Pasumpon and Palanisamy, Ram and Ntalianis, Klimis},
	date = {2021},
	langid = {english},
	keywords = {{FaaS}, {AWS} lambda, Function orchestration, {AWS} step function, Function composition, {IBM} cloud functions, {IBM} sequences, Serverless functions, Serverless trilemma},
	file = {Full Text PDF:/home/larissa/Zotero/storage/AGLQ5D5Y/Bharti et al. - 2021 - Sequential Workflow in Production Serverless FaaS .pdf:application/pdf},
}

@inproceedings{kritikos_review_2018,
	title = {A Review of Serverless Frameworks},
	url = {https://ieeexplore.ieee.org/abstract/document/8605774},
	doi = {10.1109/UCC-Companion.2018.00051},
	abstract = {Serverless computing is a new computing paradigm that promises to revolutionize the way applications are built and provisioned. In this computing kind, small pieces of software called functions are deployed in the cloud with zero administration and minimal costs for the software developer. Further, this computing kind has various applications in areas like image processing and scientific computing. Due to the above advantages, the current uptake of serverless computing is being addressed by traditional big cloud providers like Amazon, who offer serverless platforms for serverless application deployment and provisioning. However, as in the case of cloud computing, such providers attempt to lock-in their customers with the supply of complementary services which provide added-value support to serverless applications. To this end, to resolve this issue, serverless frameworks have been recently developed. Such frameworks either abstract away from serverless platform specificities, or they enable the production of a mini serverless platform on top of existing clouds. However, these frameworks differ in various features that do have an impact on the serverless application lifecycle. To this end, to assist the developers in selecting the most suitable framework, this paper attempts to review these frameworks according to a certain set of criteria that directly map to the application lifecycle. Further, based on the review results, some remaining challenges are supplied, which when confronted will make serverless frameworks highly usable and suitable for the handling of both serverless as well as mixed application kinds.},
	eventtitle = {2018 {IEEE}/{ACM} International Conference on Utility and Cloud Computing Companion ({UCC} Companion)},
	pages = {161--168},
	booktitle = {2018 {IEEE}/{ACM} International Conference on Utility and Cloud Computing Companion ({UCC} Companion)},
	author = {Kritikos, Kyriakos and Skrzypek, Paweł},
	urldate = {2024-05-17},
	date = {2018-12},
	keywords = {Software, Monitoring, Testing, Runtime, Cloud computing, Production, Computer languages, serverless, function-as-a-service, abstraction, provisioning, framework, review},
	file = {IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/UIQPVNX3/Kritikos and Skrzypek - 2018 - A Review of Serverless Frameworks.pdf:application/pdf},
}

@inproceedings{li_rethinking_2023,
	location = {New York, {NY}, {USA}},
	title = {Rethinking Deployment for Serverless Functions: A Performance-First Perspective},
	isbn = {9798400701092},
	url = {https://dl.acm.org/doi/10.1145/3581784.3613211},
	doi = {10.1145/3581784.3613211},
	series = {{SC} '23},
	shorttitle = {Rethinking Deployment for Serverless Functions},
	abstract = {Serverless computing commonly adopts strong isolation mechanisms for deploying functions, which may bring significant performance overhead because each function needs to run in a completely new environment (i.e., the "one-to-one" model). To accelerate the function computation, prior work has proposed using sandbox sharing to reduce the overhead, i.e., the "many-to-one" model. Nonetheless, either process-based true parallelism or thread-based pseudo-parallelism still causes high latency, preventing its adaptation for latency-sensitive web services. To achieve optimal performance and resource efficiency for serverless workflow, we argue an "m-to-n" deployment model that manipulates multiple granularities of computing abstractions such as processes, threads, and sandboxes to amortize overhead. We propose wrap, a new deployment abstraction that balances the tradeoffs between interaction overhead, startup overhead and function execution. We further design Chiron, a wrap-based deployment manager that can automatically perform the orchestration of multiple computing abstractions based on performance prioritization. Our comprehensive evaluation indicates that Chiron outperforms state-of-the-art systems by 1.3×-21.8× on system throughput.},
	pages = {1--14},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	publisher = {Association for Computing Machinery},
	author = {Li, Yiming and Zhao, Laiping and Yang, Yanan and Qu, Wenyu},
	urldate = {2024-05-17},
	date = {2023-11-11},
	keywords = {deployment model, graph partition, serverless workflows},
	file = {Full Text PDF:/home/larissa/Zotero/storage/JQFU7RGG/Li et al. - 2023 - Rethinking Deployment for Serverless Functions A .pdf:application/pdf},
}

@article{pakdil_serverless_2022,
	title = {Serverless Geospatial Data Processing Workflow System Design},
	volume = {11},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2220-9964},
	url = {https://www.mdpi.com/2220-9964/11/1/20},
	doi = {10.3390/ijgi11010020},
	abstract = {Geospatial data and related technologies have become an increasingly important aspect of data analysis processes, with their prominent role in most of them. Serverless paradigm have become the most popular and frequently used technology within cloud computing. This paper reviews the serverless paradigm and examines how it could be leveraged for geospatial data processes by using open standards in the geospatial community. We propose a system design and architecture to handle complex geospatial data processing jobs with minimum human intervention and resource consumption using serverless technologies. In order to define and execute workflows in the system, we also propose new models for both workflow and task definitions models. Moreover, the proposed system has new Open Geospatial Consortium ({OGC}) Application Programming Interface ({API}) Processes specification-based web services to provide interoperability with other geospatial applications with the anticipation that it will be more commonly used in the future. We implemented the proposed system on one of the public cloud providers as a proof of concept and evaluated it with sample geospatial workflows and cloud architecture best practices.},
	pages = {20},
	number = {1},
	journaltitle = {{ISPRS} International Journal of Geo-Information},
	author = {Pakdil, Mete Ercan and Çelik, Rahmi Nurhan},
	urldate = {2024-05-17},
	date = {2022-01},
	langid = {english},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {serverless computing, geospatial workflows, workflow management system},
	file = {Full Text PDF:/home/larissa/Zotero/storage/8WGUY494/Pakdil and Çelik - 2022 - Serverless Geospatial Data Processing Workflow Sys.pdf:application/pdf},
}

@misc{wang_briskchain_2023,
	location = {Rochester, {NY}},
	title = {Briskchain: Decentralized Function Composition for High-Performance Serverless Computing},
	url = {https://papers.ssrn.com/abstract=4655012},
	doi = {10.2139/ssrn.4655012},
	shorttitle = {Briskchain},
	abstract = {Serverless computing allows developers to create workflows for complex tasks through the composition of serverless functions. Current serverless workflow engines rely on master-side pattern which do not permit direct interaction between consecutive serverless functions in the workflow. In this paper, a decentralized worker-side pattern is proposed that provides better performance by allowing consecutive functions to flow from the current node to the next without first having to interact with the master controller. It treats the serverless workflow as a whole unit and uses a locality strategy to optimize performance. This approach is implemented in a workflow engine called {BriskChain} to demonstrate its effectiveness. Experiments using both synthetic and real application workflows show that {BriskChain} requires significantly reduced runtime overhead compared with {OpenWhisk} and Apache Composer, two open-source serverless platforms.},
	number = {4655012},
	author = {Wang, Kan and Ma, Jing and Lai, Edmund Ming Kit},
	urldate = {2024-05-17},
	date = {2023-12-05},
	langid = {english},
	keywords = {Serverless workflow, Function as a service, Runtime overhead, Worker-side pattern},
	file = {Full Text PDF:/home/larissa/Zotero/storage/7TWDB726/Wang et al. - 2023 - Briskchain Decentralized Function Composition for.pdf:application/pdf},
}

@misc{ambroszkiewicz_functionals_2022,
	title = {Functionals in the Clouds: An abstract architecture of serverless Cloud-Native Apps},
	url = {http://arxiv.org/abs/2105.10362},
	doi = {10.48550/arXiv.2105.10362},
	shorttitle = {Functionals in the Clouds},
	abstract = {Cloud Native Application {CNApp} (as a distributed system) is a collection of independent components (micro-services) interacting via communication protocols. This gives rise to present an abstract architecture of {CNApp} as dynamically re-configurable acyclic directed multi graph where vertices are microservices, and edges are the protocols. Generic mechanisms for such reconfigurations evidently correspond to higher-level functions (functionals). This implies also internal abstract architecture of microservice as a collection of event-triggered serverless functions (including functions implementing the protocols) that are dynamically composed into event-dependent data-flow graphs. Again, generic mechanisms for such compositions correspond to calculus of functionals and relations.},
	number = {{arXiv}:2105.10362},
	publisher = {{arXiv}},
	author = {Ambroszkiewicz, Stanislaw and Bartyna, Waldemar and Bylka, Stanislaw},
	urldate = {2024-05-17},
	date = {2022-08-25},
	eprinttype = {arxiv},
	eprint = {2105.10362 [cs]},
	keywords = {68M14, Computer Science - Computation and Language, Computer Science - Logic in Computer Science, F.1.1, F.4.3},
	file = {arXiv Fulltext PDF:/home/larissa/Zotero/storage/ENXKX53V/Ambroszkiewicz et al. - 2022 - Functionals in the Clouds An abstract architectur.pdf:application/pdf;arXiv.org Snapshot:/home/larissa/Zotero/storage/4R4GC3ZE/2105.html:text/html},
}

@inproceedings{meladakis_transferring_2022,
	location = {New York, {NY}, {USA}},
	title = {Transferring transactional business processes to {FaaS}},
	isbn = {978-1-4503-9927-2},
	url = {https://dl.acm.org/doi/10.1145/3565382.3565882},
	doi = {10.1145/3565382.3565882},
	series = {{WoSC} '22},
	abstract = {Function-as-a-Service ({FaaS}) is a modern cloud service model that has gained significant attention from the research and industry communities in recent years for its many benefits such as dynamic scaling, cost efficiency, faster programming, flexibility to microservices and containers technology. However, the building and deployment of serverless applications come with many challenges that need to be tackled, like workflow design complexity and migration of other applications. When transactions between different parties are involved, the workflow becomes knotty and the communication between participants and all properties of transactions have to be properly resolved. Transactions have widely been discussed in Business processes, so same practices might be adopted by serverless workflows. In this work we provide guidelines and mapping mechanisms for transforming transactional Business Process Modeling Notation 2.0 ({BPMN}2) applications to a serverless platform. We shed light on the current inability of function orchestrators to express workflow definitions, and deal with various architectural dilemmas that stem from the dissimilar nature of stateful {BPMN} vs. stateless serverless applications. We overcome the unbalanced capabilities between well-established {BPMN} notations and function orchestration definitions and illustrate how to exploit and combine cloud native services that comes with {FaaS} to create serverless applications.},
	pages = {25--30},
	booktitle = {Proceedings of the Eighth International Workshop on Serverless Computing},
	publisher = {Association for Computing Machinery},
	author = {Meladakis, Kostas and Zeginis, Chrysostomos and Magoutis, Kostas and Plexousakis, Dimitris},
	urldate = {2024-05-17},
	date = {2022-11-22},
	keywords = {{FaaS}, serverless workflow, {OpenWhisk}, {BPMN}2, function orchestration, transactions},
	file = {Full Text PDF:/home/larissa/Zotero/storage/P5QZHSF7/Meladakis et al. - 2022 - Transferring transactional business processes to F.pdf:application/pdf},
}

@inproceedings{kumari_workflow_2022,
	location = {Gunupur, Odisha, India},
	title = {Workflow Sensitive Access Management in Serverless Computing},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66549-056-6},
	url = {https://ieeexplore.ieee.org/document/10051255/},
	doi = {10.1109/iSSSC56467.2022.10051255},
	eventtitle = {2022 {IEEE} 2nd International Symposium on Sustainable Energy, Signal Processing and Cyber Security ({iSSSC})},
	pages = {1--6},
	booktitle = {2022 {IEEE} 2nd International Symposium on Sustainable Energy, Signal Processing and Cyber Security ({iSSSC})},
	publisher = {{IEEE}},
	author = {Kumari, Anisha and Akram Khan, Md. and Sahoo, Bibhudatta},
	urldate = {2024-05-17},
	date = {2022-12-15},
}

@article{mampage_holistic_2022,
	title = {A Holistic View on Resource Management in Serverless Computing Environments: Taxonomy and Future Directions},
	volume = {54},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3510412},
	doi = {10.1145/3510412},
	shorttitle = {A Holistic View on Resource Management in Serverless Computing Environments},
	abstract = {Serverless computing has emerged as an attractive deployment option for cloud applications in recent times. The unique features of this computing model include rapid auto-scaling, strong isolation, fine-grained billing options, and access to a massive service ecosystem, which autonomously handles resource management decisions. This model is increasingly being explored for deployments in geographically distributed edge and fog computing networks as well, due to these characteristics. Effective management of computing resources has always gained a lot of attention among researchers. The need to automate the entire process of resource provisioning, allocation, scheduling, monitoring, and scaling has resulted in the need for specialized focus on resource management under the serverless model. In this article, we identify the major aspects covering the broader concept of resource management in serverless environments and propose a taxonomy of elements that influence these aspects, encompassing characteristics of system design, workload attributes, and stakeholder expectations. We take a holistic view on serverless environments deployed across edge, fog, and cloud computing networks. We also analyse existing works discussing aspects of serverless resource management using this taxonomy. This article further identifies gaps in literature and highlights future research directions for improving capabilities of this computing model.},
	pages = {222:1--222:36},
	number = {11},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Mampage, Anupama and Karunasekera, Shanika and Buyya, Rajkumar},
	urldate = {2024-05-17},
	date = {2022-09-09},
	keywords = {performance prediction, Serverless computing, application modelling, resource management, resource scaling, resource scheduling, workload characterization},
	file = {Full Text PDF:/home/larissa/Zotero/storage/BL44CY47/Mampage et al. - 2022 - A Holistic View on Resource Management in Serverle.pdf:application/pdf},
}

@article{yussupov_standards-based_2022,
	title = {Standards-based modeling and deployment of serverless function orchestrations using {BPMN} and {TOSCA}},
	volume = {52},
	issn = {1097-024X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3073},
	doi = {10.1002/spe.3073},
	abstract = {Function-as-a-Service ({FaaS}) is a cloud service model enabling to implement serverless applications for a variety of use cases. These range from scheduled calls of single functions to complex function orchestrations executed using orchestration services such as {AWS} step functions. However, since the available function orchestration technologies vary in functionalities, supported modeling languages, and {APIs}, modeling such function orchestrations and their deployment require significant technology-specific expertise. Moreover, the resulting models are typically not portable due to provider- and technology-specific details, and major efforts are required when exchanging an orchestrator or provider due to such lock-ins. To tackle this issue, we introduce a vendor- and technology-agnostic method for the modeling and deployment of serverless function orchestrations, which relies on the business process model and notation ({BPMN}) and topology and orchestration specification for cloud applications ({TOSCA}) standards for modeling function orchestrations and their deployment, respectively. We also present a toolchain for modeling serverless function orchestrations in {BPMN}, generating proprietary models supported by different function orchestration technologies from {BPMN} models, specifying their actual deployment in {TOSCA}, and then enacting such deployment. Finally, we illustrate a case study applying our method and toolchain in practice.},
	pages = {1454--1495},
	number = {6},
	journaltitle = {Software: Practice and Experience},
	author = {Yussupov, Vladimir and Soldani, Jacopo and Breitenbücher, Uwe and Leymann, Frank},
	urldate = {2024-05-17},
	date = {2022},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.3073},
	keywords = {serverless, workflows, function-as-a-service, {FaaS}, {BPMN}, function orchestrations, {TOSCA}},
	file = {Full Text PDF:/home/larissa/Zotero/storage/74SYH2AH/Yussupov et al. - 2022 - Standards-based modeling and deployment of serverl.pdf:application/pdf;Snapshot:/home/larissa/Zotero/storage/3J7BI4G9/spe.html:text/html},
}

@inproceedings{kumari_evaluation_2021,
	location = {Cham},
	title = {Evaluation of Integrated Frameworks for Optimizing {QoS} in Serverless Computing},
	isbn = {978-3-030-87007-2},
	doi = {10.1007/978-3-030-87007-2_20},
	abstract = {Serverless computing is an emerging cloud deployment model where developers can concentrate on developing application logic without worrying about the underlying architecture. It is similar to the platform as a service ({PaaS}) but at the functional level. Applications are usually deployed in the form of a set of functions independently and each function may be executed at separate servers thus also named as function as a service ({FaaS}). Serverless at the edge can handle thousands of concurrent functions invocations to process various kinds of events generated from resources like database, system logs, and other storage units, etc. A number of serverless frameworks like Openfaas, Openwhisk, Microsoft Azure, Amazon {AWS} allow dynamic scaling to handle the parallel request of stateless functions from the client-side. A separate container manager may be provisioned to handle distributed load for data processing. In this paper, we have evaluated the performance of serverless frameworks for parallel loads in terms of response time and throughput. In this paper, we have shown that the serverless framework is suitable for handling dynamic applications that can be executed on a number of stateless functions. An extensive comparison of the performance of serverless frameworks in handling concurrent invocations in terms of response time and throughput is also presented. It has been observed that Openwhisk is found to be the better serverless framework in terms of elasticity and scalability.},
	pages = {277--288},
	booktitle = {Computational Science and Its Applications – {ICCSA} 2021},
	publisher = {Springer International Publishing},
	author = {Kumari, Anisha and Sahoo, Bibhudatta and Behera, Ranjan Kumar and Misra, Sanjay and Sharma, Mayank Mohan},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Garau, Chiara and Blečić, Ivan and Taniar, David and Apduhan, Bernady O. and Rocha, Ana Maria A. C. and Tarantino, Eufemia and Torre, Carmelo Maria},
	date = {2021},
	langid = {english},
	keywords = {Serverless computing, Function-as-a-service, Openfaas, {OpenWhisk}, Orchestration},
	file = {Full Text PDF:/home/larissa/Zotero/storage/F7FKDDGC/Kumari et al. - 2021 - Evaluation of Integrated Frameworks for Optimizing.pdf:application/pdf},
}

@article{li_time-cost_2022,
	title = {Time-cost efficient memory configuration for serverless workflow applications},
	volume = {34},
	rights = {© 2022 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7308},
	doi = {10.1002/cpe.7308},
	abstract = {Recently, workflow applications are increasingly migrated to Function-as-a-Service platforms which are easy to manage, highly-scalable, and pay-as-you-go. Meanwhile, users face challenges in migration of serverless applications because of the lack of efficient algorithm for workflow memory configuration to optimize the performance. To this end, this article proposes a heuristic urgency-based algorithm {UWC} and a meta-heuristic hybrid algorithm {BPSO} to tackle the time-cost tradeoff. {UWC} sorts functions and allocates each function an appropriate memory size by greedy strategy. {BPSO} hybridizes particle swarm optimization as well as beetle antennae search algorithm to guide particles to search directionally and utilizes nonlinear inertia weight to avoid local premature convergence. Extensive experiments with classical serverless application demonstrate that {UWC} and {BPSO} are very competitive in comparison with existing algorithms as they can find the optimal workflow memory configuration.},
	pages = {e7308},
	number = {27},
	journaltitle = {Concurrency and Computation: Practice and Experience},
	author = {Li, Zengpeng and Yu, Huiqun and Fan, Guisheng},
	urldate = {2024-05-17},
	date = {2022},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.7308},
	keywords = {performance modeling, serverless computing, constrained optimization, workflow memory configuration},
	file = {Full Text PDF:/home/larissa/Zotero/storage/PYDGTKFG/Li et al. - 2022 - Time-cost efficient memory configuration for serve.pdf:application/pdf;Snapshot:/home/larissa/Zotero/storage/X2STR7JA/cpe.html:text/html},
}

@misc{tetzlaff_bpmn4sml_2022,
	title = {{BPMN}4sML: A {BPMN} Extension for Serverless Machine Learning. Technology Independent and Interoperable Modeling of Machine Learning Workflows and their Serverless Deployment Orchestration},
	url = {http://arxiv.org/abs/2208.02030},
	doi = {10.48550/arXiv.2208.02030},
	shorttitle = {{BPMN}4sML},
	abstract = {Machine learning ({ML}) continues to permeate all layers of academia, industry and society. Despite its successes, mental frameworks to capture and represent machine learning workflows in a consistent and coherent manner are lacking. For instance, the de facto process modeling standard, Business Process Model and Notation ({BPMN}), managed by the Object Management Group, is widely accepted and applied. However, it is short of specific support to represent machine learning workflows. Further, the number of heterogeneous tools for deployment of machine learning solutions can easily overwhelm practitioners. Research is needed to align the process from modeling to deploying {ML} workflows. We analyze requirements for standard based conceptual modeling for machine learning workflows and their serverless deployment. Confronting the shortcomings with respect to consistent and coherent modeling of {ML} workflows in a technology independent and interoperable manner, we extend {BPMN}'s Meta-Object Facility ({MOF}) metamodel and the corresponding notation and introduce {BPMN}4sML ({BPMN} for serverless machine learning). Our extension {BPMN}4sML follows the same outline referenced by the Object Management Group ({OMG}) for {BPMN}. We further address the heterogeneity in deployment by proposing a conceptual mapping to convert {BPMN}4sML models to corresponding deployment models using {TOSCA}. {BPMN}4sML allows technology-independent and interoperable modeling of machine learning workflows of various granularity and complexity across the entire machine learning lifecycle. It aids in arriving at a shared and standardized language to communicate {ML} solutions. Moreover, it takes the first steps toward enabling conversion of {ML} workflow model diagrams to corresponding deployment models for serverless deployment via {TOSCA}.},
	number = {{arXiv}:2208.02030},
	publisher = {{arXiv}},
	author = {Tetzlaff, Laurens Martin},
	urldate = {2024-05-17},
	date = {2022-08-02},
	eprinttype = {arxiv},
	eprint = {2208.02030 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/larissa/Zotero/storage/SIYMBATF/Tetzlaff - 2022 - BPMN4sML A BPMN Extension for Serverless Machine .pdf:application/pdf;arXiv.org Snapshot:/home/larissa/Zotero/storage/YUVFTREB/2208.html:text/html},
}

@misc{donati_event-driven_2024,
	title = {Event-Driven {AI} Workflows in Serverless Computing: Enabling Real-Time Data Processing and Decision-Making},
	url = {https://www.preprints.org/manuscript/202405.0656/v1},
	doi = {10.20944/preprints202405.0656.v1},
	shorttitle = {Event-Driven {AI} Workflows in Serverless Computing},
	abstract = {Real-time data processing and decision-making are increasingly crucial in various applications, driven by the continuous influx of data streams. Event-driven {AI} workflows within serverless computing environments offer a promising approach to handle these real-time demands efficiently. This paper presents a framework for simulating and analyzing the performance characteristics of such workflows. Our proposed approach utilizes simulated data with varying event rates and durations to investigate the impact on key performance metrics like latency, throughput, and resource utilization. This enables a comprehensive evaluation of the inherent trade-offs within event-driven {AI} systems. The key findings reveal a trade-off between latency and throughput. As the event rate increases, average processing latency generally increases while average throughput increases. Resource utilization remains relatively stable across different event rates in the simulated scenarios (e.g., 75.55{\textbackslash}\% at 2 events/second, 74.51{\textbackslash}\% at 10 events/second). This framework provides a valuable tool for understanding the performance characteristics of event-driven {AI} workflows and optimizing resource allocation strategies.},
	number = {2024050656},
	publisher = {Preprints},
	author = {Donati, Ottavia and Macario, Muziano and Karim, Mohd Hassan},
	urldate = {2024-05-17},
	date = {2024-05-10},
	langid = {english},
	keywords = {Scalability, Serverless Computing, Event-Driven {AI}, Performance Analysis, Resource Optimization},
	file = {Full Text PDF:/home/larissa/Zotero/storage/JLR5AM66/Donati et al. - 2024 - Event-Driven AI Workflows in Serverless Computing.pdf:application/pdf},
}

@inproceedings{kousiouris_measuring_2022,
	location = {New York, {NY}, {USA}},
	title = {Measuring Baseline Overheads in Different Orchestration Mechanisms for Large {FaaS} Workflows},
	isbn = {978-1-4503-9159-7},
	url = {https://dl.acm.org/doi/10.1145/3491204.3527467},
	doi = {10.1145/3491204.3527467},
	series = {{ICPE} '22},
	abstract = {Serverless environments have attracted significant attention in recent years as a result of their agility in execution as well as inherent scaling capabilities as a cloud-native execution model. While extensive analysis has been performed in various critical performance aspects of these environments, such as cold start times, the aspect of workflow orchestration delays has been neglected. Given that this paradigm has become more mature in recent years and application complexity has started to rise from a few functions to more complex application structures, the issue of delays in orchestrating these functions may become severe. In this work, one of the main open source {FaaS} platforms, Openwhisk, is utilized in order to measure and investigate its orchestration delays for the main sequence operator of the platform. These are compared to delays included in orchestration of functions through two alternative means, including the execution of orchestrator logic functions in supporting runtimes based on Node-{RED}. The delays inserted by each different orchestration mode are measured and modeled, while boundary points of selection between each mode are presented, based on the number and expected delay of the functions that constitute the workflow. It is indicative that in certain cases, the orchestration overheads might range from 0.29\% to 235\% compared to the beneficial computational time needed for the workflow functions. The results can extend simulation and estimation mechanisms with information on the orchestration overheads.},
	pages = {61--68},
	booktitle = {Companion of the 2022 {ACM}/{SPEC} International Conference on Performance Engineering},
	publisher = {Association for Computing Machinery},
	author = {Kousiouris, George and Giannakos, Chris and Tserpes, Konstantinos and Stamati, Teta},
	urldate = {2024-05-17},
	date = {2022-07-19},
	keywords = {performance, serverless, faas, orchestration, openwhisk, overhead},
	file = {Full Text PDF:/home/larissa/Zotero/storage/64A643BC/Kousiouris et al. - 2022 - Measuring Baseline Overheads in Different Orchestr.pdf:application/pdf},
}

@article{kumari_resource_2022,
	title = {Resource optimization in performance modeling for serverless application},
	volume = {14},
	issn = {2511-2112},
	url = {https://doi.org/10.1007/s41870-022-01073-x},
	doi = {10.1007/s41870-022-01073-x},
	abstract = {Recently, serverless applications and serverless computing have gained popularity due to their pay-as-you-go billing mechanism, high scalability, high availability, and less burden in resource management. Despite that, many practical issues arise for cloud users while migrating their applications to the serverless environment such as estimating the exact cost, performance evaluation, cope up with the trade-off between performance and cost to get the expected quality of service, and restricted budget for serverless applications. In this research, an effort has been made to bridge the gap by proposing an effective performance model for various serverless applications, which can be represented as Directed Acyclic Graph workflows. Further, we have proposed a greedy optimization-based heuristic algorithm to obtain the resource configuration needed to get optimal end to end response time and the cost. We have leveraged the Step Functions and Lambda service of Amazon Web Service platform for evaluating the proposed models. An extensive analysis of performance-cost trade-off has been performed by considering four real-world serverless applications.},
	pages = {2867--2875},
	number = {6},
	journaltitle = {International Journal of Information Technology},
	shortjournal = {Int. j. inf. tecnol.},
	author = {Kumari, Anisha and Patra, Manoj Kumar and Sahoo, Bibhudatta and Behera, Ranjan Kumar},
	urldate = {2024-05-17},
	date = {2022-10-01},
	langid = {english},
	keywords = {Cloud computing, Serverless computing, {FaaS}, Performance modeling, Performance optimization},
	file = {Full Text PDF:/home/larissa/Zotero/storage/TDB76NAK/Kumari et al. - 2022 - Resource optimization in performance modeling for .pdf:application/pdf},
}

@article{shojaee_rad_memory_2024,
	title = {Memory orchestration mechanisms in serverless computing: a taxonomy, review and future directions},
	issn = {1573-7543},
	url = {https://doi.org/10.1007/s10586-023-04251-z},
	doi = {10.1007/s10586-023-04251-z},
	shorttitle = {Memory orchestration mechanisms in serverless computing},
	abstract = {Serverless computing has become very popular in recent years due to its flexibility and cost efficiency. Serverless computing is a cloud computing model that allows developers to write and deploy code without managing the underlying infrastructure. Serverless computing has been welcomed due to its scalability, affordability, and ease of use. However, memory orchestration in serverless computing can be challenging due to the ephemeral nature of serverless functions, which are short-lived and stateless. This article has reviewed memory orchestration mechanisms and then presented a classification of memory orchestration mechanisms in serverless computing, which are classified into three main methods: Machine learning-based approach, Heuristic-based approach, and framework-based approach. The advantages and disadvantages of each mechanism as well as the challenges and performance metrics affecting their effectiveness have been investigated. Each memory orchestration approach, whether heuristic-based, framework-based, or machine learning-based, has its own advantages and disadvantages. Different mechanisms are suitable for different use cases. Therefore, it is important to carefully evaluate the trade-offs between performance, cost, and complexity when choosing a memory orchestration mechanism. Finally, the paper identifies several future research directions for memory orchestration in serverless computing, which include developing memory orchestration mechanisms and dynamic memory management, integrating memory orchestration mechanisms with other resource management mechanisms, security, memory sharing, compression, pricing, and investigating the relationships between memory orchestration with cost and performance in serverless computing.},
	journaltitle = {Cluster Computing},
	shortjournal = {Cluster Comput},
	author = {Shojaee rad, Zahra and Ghobaei-Arani, Mostafa and Ahsan, Reza},
	urldate = {2024-05-17},
	date = {2024-02-08},
	langid = {english},
	keywords = {Performance evaluation, Cloud computing, Serverless computing, Function as a service, Memory configuration, Memory optimization, Memory orchestration, Review},
	file = {Full Text PDF:/home/larissa/Zotero/storage/4AG5BSE4/Shojaee rad et al. - 2024 - Memory orchestration mechanisms in serverless comp.pdf:application/pdf},
}

@article{cicconetti_faas_2022,
	title = {{FaaS} execution models for edge applications},
	volume = {86},
	issn = {1574-1192},
	url = {https://www.sciencedirect.com/science/article/pii/S157411922200102X},
	doi = {10.1016/j.pmcj.2022.101689},
	abstract = {In this paper, we address the problem of supporting stateful workflows following a Function-as-a-Service ({FaaS}) model in edge networks. In particular we focus on the problem of data transfer, which can be a performance bottleneck due to the limited speed of communication links in some edge scenarios and we propose three different schemes: a pure {FaaS} implementation, {StateProp}, i.e., propagation of the application state throughout the entire chain of functions, and {StateLocal}, i.e., a solution where the state is kept local to the workers that run functions and retrieved only as needed. We then extend the proposed schemes to the more general case of applications modeled as Directed Acyclic Graphs ({DAGs}), which cover a broad range of practical applications, e.g., in the Internet of Things ({IoT}) area. Our contribution is validated via a prototype implementation. Experiments in emulated conditions show that applying the data locality principle reduces significantly the volume of network traffic required and improves the end-to-end delay performance, especially with local caching on edge nodes and low link speeds.},
	pages = {101689},
	journaltitle = {Pervasive and Mobile Computing},
	shortjournal = {Pervasive and Mobile Computing},
	author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
	urldate = {2024-05-17},
	date = {2022-10-01},
	keywords = {Distributed computing, Function-as-a-Service, Serverless, Edge computing, In-network intelligence},
	file = {ScienceDirect Snapshot:/home/larissa/Zotero/storage/89W9ZN8Y/S157411922200102X.html:text/html;Submitted Version:/home/larissa/Zotero/storage/IMTDY5XB/Cicconetti et al. - 2022 - FaaS execution models for edge applications.pdf:application/pdf},
}

@article{morabito_secure-by-design_2023,
	title = {Secure-by-design serverless workflows on the Edge–Cloud Continuum through the Osmotic Computing paradigm},
	volume = {22},
	issn = {2542-6605},
	url = {https://www.sciencedirect.com/science/article/pii/S2542660523000604},
	doi = {10.1016/j.iot.2023.100737},
	abstract = {Nowadays, many Cloud companies adopt serverless computation based on the Function as a Service ({FaaS}) paradigm. Such an approach, built on the dynamic provisioning of serverless functions, has significantly altered the design of Cloud applications. Indeed, complex systems may now be viewed as a graph of serverless functions (i.e., workflow) distributed over the Cloud and Edge layers. However, building applications in such a complicated context and introducing security by design is still a non-trivial problem. Meanwhile, Osmotic Computing has advanced as a novel computational paradigm in recent years. This may serve as a suitable foundation for strengthening security in serverless applications throughout the Cloud–Edge Continuum. In this work, we rely on Osmotic Computing principles for modifying the architecture of {OpenWolf}, a novel serverless engine, and improving the execution time of ciphertext data by about 65\%.},
	pages = {100737},
	journaltitle = {Internet of Things},
	shortjournal = {Internet of Things},
	author = {Morabito, Gabriele and Sicari, Christian and Ruggeri, Armando and Celesti, Antonio and Carnevale, Lorenzo},
	urldate = {2024-05-17},
	date = {2023-07-01},
	keywords = {Cloud computing, Serverless computing, Edge computing, Edge–Cloud Continuum, Osmotic Computing},
	file = {Morabito et al. - 2023 - Secure-by-design serverless workflows on the Edge–.pdf:/home/larissa/Zotero/storage/CEYMTF2Z/Morabito et al. - 2023 - Secure-by-design serverless workflows on the Edge–.pdf:application/pdf;ScienceDirect Snapshot:/home/larissa/Zotero/storage/873AIFF5/S2542660523000604.html:text/html},
}

@inproceedings{hautz_characterizing_2023,
	location = {New York, {NY}, {USA}},
	title = {Characterizing {AFCL} Serverless Scientific Workflows in Federated {FaaS}},
	isbn = {9798400704550},
	url = {https://dl.acm.org/doi/10.1145/3631295.3631397},
	doi = {10.1145/3631295.3631397},
	series = {{WoSC} '23},
	abstract = {This paper introduces several, publicly available, serverless scientific workflows Montage, {BWA}, and Monte Carlo developed at a high level of abstraction using the Abstract Function Choreography Language ({AFCL}). Any individual function can run across federated {FaaS} comprising cloud regions of {AWS} and {GCP}. We present the support for composition with {AFCL} and execution with the {xAFCL} serverless workflow management system. For each {AFCL} workflow, we present implementation details, networking, and complexity. The evaluation of the presented serverless workflows shows that workflow functions download ephemeral data and run computation faster on {AWS} than on {GCP}. However, functions on {GCP} upload faster on the collocated storage.},
	pages = {24--29},
	booktitle = {Proceedings of the 9th International Workshop on Serverless Computing},
	publisher = {Association for Computing Machinery},
	author = {Hautz, Mika and Ristov, Sashko and Felderer, Michael},
	urldate = {2024-05-17},
	date = {2023-12-11},
	keywords = {Function-as-a-Service, serverless, scientific workflows, federation},
	file = {Full Text PDF:/home/larissa/Zotero/storage/HX2TI2ZH/Hautz et al. - 2023 - Characterizing AFCL Serverless Scientific Workflow.pdf:application/pdf},
}

@inproceedings{kumari_comparative_2022,
	location = {Cham},
	title = {Comparative Study of Workflow Modeling Services Based on Activity Complexity},
	isbn = {978-3-031-21750-0},
	doi = {10.1007/978-3-031-21750-0_4},
	abstract = {Serverless computing has been widely used in several applications, where tasks are represented through an independent stateless functions. These stateless functions can be orchestrated in the form of workflow and can be deployed and executed in serverless frameworks. A number of serverless frameworks have emerged recently to provide orchestration services for serverless workflow. It is necessary to explore the potential of serverless frameworks which can be helpful for developers in taking business decisions. The performance of a serverless framework depends on several complexities associated with the serverless workflow. In this paper, we have focused on the effect of activity complexity associated with the workflow. The activity complexity of a workflow refers to the number of functions in the serverless application. A comparative analysis of various serverless workflow services based on change in performance parameters based on activity complexity is presented. The performance parameters such as overall function time, overhead time, and total response time are considered as the evaluation parameters. The extensive comparison has been done by considering both sequential and parallel workflow, which have been generated by various workflow services. Some of the findings are also presented for comparative analysis. This could be a potential resource for the application developers who have focused on serverless computing.},
	pages = {39--51},
	booktitle = {Computing, Communication and Learning},
	publisher = {Springer Nature Switzerland},
	author = {Kumari, Anisha and Patra, Manoj Kumar and Sahoo, Bibhudatta},
	editor = {Panda, Sanjaya Kumar and Rout, Rashmi Ranjan and Sadam, Ravi Chandra and Rayanoothala, Bala Venkata Subramaanyam and Li, Kuan-Ching and Buyya, Rajkumar},
	date = {2022},
	langid = {english},
	keywords = {Serverless computing, Workflow, Function-as-a-service, Activity complexity, {AWS} Lambda},
	file = {Full Text PDF:/home/larissa/Zotero/storage/UV5QEANF/Kumari et al. - 2022 - Comparative Study of Workflow Modeling Services Ba.pdf:application/pdf},
}

@inproceedings{werner_application-platform_2021,
	location = {Cham},
	title = {Application-Platform Co-design for Serverless Data Processing},
	isbn = {978-3-030-91431-8},
	doi = {10.1007/978-3-030-91431-8_39},
	abstract = {“Application-platform co-design” refers to the phenomenon of new platforms being created in response to changing application needs, followed by application design and development changing due to the emergence (and the specifics, limitations) of the new platforms, therefore creating, again, new application and platform requirements. This continuous process of application and platform (re-)design describes an engineering and management responsibility to constantly evaluate any given platform for application fit and platform-specific application design, and to consider a new or evolutionary platform development project due to evolving and changing application needs.},
	pages = {627--640},
	booktitle = {Service-Oriented Computing},
	publisher = {Springer International Publishing},
	author = {Werner, Sebastian and Tai, Stefan},
	editor = {Hacid, Hakim and Kao, Odej and Mecella, Massimo and Moha, Naouel and Paik, Hye-young},
	date = {2021},
	langid = {english},
	keywords = {Serverless computing, Serverless data processing, Co-design, Platform design and development, Platform-specific application design and development},
	file = {Full Text PDF:/home/larissa/Zotero/storage/C5MDPJL4/Werner and Tai - 2021 - Application-Platform Co-design for Serverless Data.pdf:application/pdf},
}

@article{morenets_serverless_2020,
	title = {Serverless Event-driven Applications Development Tools and Techniques},
	volume = {3},
	issn = {2617-7323, 2617-3808},
	url = {http://nrpcomp.ukma.edu.ua/article/view/220785},
	doi = {10.18523/2617-3808.2020.3.36-41},
	abstract = {Serverless, a new cloud-based architecture, brings development and deployment flexibility to a new level by significantly decreasing the size of the deployment units. Nevertheless, it still hasn’t been clearly defined for which applications it should be employed and how to use it most effectively, and this is the focus of this research. The study uses Microsoft Azure Functions – one of the popular mature tools – because of its stateful orchestrators – Durable Functions. The tool is used to present and describe four flexible serverless patterns with code examples. The first pattern is {HTTP} nanoservices. The example demonstrates how flexible can be the Function-asa-Service model, which uses relatively small functions as deployment units. The second usage scenario described is a small logic layer between a few other cloud services. Thanks to its event-driver nature, serverless is well-suited for such tasks as making an action in one service after a specific event from another one. New functions easily integrate with the {API} from the first example. The third scenario – distributed computing – relies on the ability of Durable Functions to launch a myriad of functions in parallel and then aggregate their results. and distributed computing. A custom {MapReduce} implementation is presented in this section. The last pattern described in this research significantly simplifies concurrent working with mutable data by implementing the actor model. Durable Entities guarantee that messages are delivered reliably and in order, and also the absence of deadlocks. The results of this work can be used as a practical guide to serverless main concepts and usage scenarios. Main topic of future research was chosen to be the development of a full-fledged serverless application using typical patterns to study the architecture in more depth.},
	pages = {36--41},
	number = {0},
	journaltitle = {{NaUKMA} Research Papers. Computer Science},
	author = {Morenets, Ihor and Shabinskiy, Anton},
	urldate = {2024-05-17},
	date = {2020-12-28},
	langid = {english},
	file = {Morenets and Shabinskiy - 2020 - Serverless Event-driven Applications Development T.pdf:/home/larissa/Zotero/storage/EJ4T8SPL/Morenets and Shabinskiy - 2020 - Serverless Event-driven Applications Development T.pdf:application/pdf},
}

@article{ramos_process_2023,
	title = {Process automation instantiation for intelligence orchestration},
	volume = {2},
	issn = {2813-3110},
	url = {https://www.frontiersin.org/articles/10.3389/friot.2023.1242101},
	doi = {10.3389/friot.2023.1242101},
	abstract = {The devices and use case heterogeneity in {IoT} make it challenging to streamline how to create processes that orchestrate devices that work interactively through actuation and sensing (data generation and data processing) toward fulfilling a goal. The question of what is needed to realize this vision using serverless technologies and leveraging semantic description tools that would eventually help to complement standards to describe such orchestration in a similar way as it is done today for microservices (e.g., using {TOSCA}). The article analyzes what are the different requirements that need to be present in such automation description and tests them towards some practical use cases in a particular application (vertical agriculture). The article also provides a set of analyzed instructions that might be incorporated into a description language or becomes a definition format that serves as an automation orchestration description. They provide the necessary process deployment options and interaction chains between multiple devices and data sources. These instructions are analyzed through one implementation of an execution deployment engine from the intelligence orchestration concept. Also, the practical needs and understanding of the utilization of such instructions are verified and tested with the use case.},
	journaltitle = {Frontiers in the Internet of Things},
	shortjournal = {Front. Internet Things},
	author = {Ramos, Edgar and Arumugam, Senthamiz Selvi},
	urldate = {2024-05-17},
	date = {2023-09-28},
	note = {Publisher: Frontiers},
	keywords = {workflow, orchestration, {AIoT}, Deployment, Exposure, Intelligence, {IoT}, processes},
	file = {Full Text PDF:/home/larissa/Zotero/storage/X632FL34/Ramos and Arumugam - 2023 - Process automation instantiation for intelligence .pdf:application/pdf},
}

@article{li_serverless_2022,
	title = {The Serverless Computing Survey: A Technical Primer for Design Architecture},
	volume = {54},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3508360},
	doi = {10.1145/3508360},
	shorttitle = {The Serverless Computing Survey},
	abstract = {The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potential of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.},
	pages = {220:1--220:34},
	number = {10},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Li, Zijun and Guo, Linsong and Cheng, Jiagan and Chen, Quan and He, Bingsheng and Guo, Minyi},
	urldate = {2024-05-17},
	date = {2022-09-13},
	keywords = {architecture design, Serverless computing, {FaaS}, Lambda paradigm},
	file = {Full Text PDF:/home/larissa/Zotero/storage/U2XF3Y5F/Li et al. - 2022 - The Serverless Computing Survey A Technical Prime.pdf:application/pdf},
}

@article{john_evaluation_2021,
	title = {Evaluation of serverless computing for scalable execution of a joint variant calling workflow},
	volume = {16},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0254363},
	doi = {10.1371/journal.pone.0254363},
	abstract = {Advances in whole-genome sequencing have greatly reduced the cost and time of obtaining raw genetic information, but the computational requirements of analysis remain a challenge. Serverless computing has emerged as an alternative to using dedicated compute resources, but its utility has not been widely evaluated for standardized genomic workflows. In this study, we define and execute a best-practice joint variant calling workflow using the {SWEEP} workflow management system. We present an analysis of performance and scalability, and discuss the utility of the serverless paradigm for executing workflows in the field of genomics research. The {GATK} best-practice short germline joint variant calling pipeline was implemented as a {SWEEP} workflow comprising 18 tasks. The workflow was executed on Illumina paired-end read samples from the European and African super populations of the 1000 Genomes project phase {III}. Cost and runtime increased linearly with increasing sample size, although runtime was driven primarily by a single task for larger problem sizes. Execution took a minimum of around 3 hours for 2 samples, up to nearly 13 hours for 62 samples, with costs ranging from \$2 to \$70.},
	pages = {e0254363},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {John, Aji and Muenzen, Kathleen and Ausmees, Kristiina},
	urldate = {2024-05-17},
	date = {2021-07-09},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Cloud computing, Genomics, Computer hardware, Genome analysis, Language, Metadata, Next-generation sequencing, Single nucleotide polymorphisms},
	file = {Full Text PDF:/home/larissa/Zotero/storage/YJVB4KRC/John et al. - 2021 - Evaluation of serverless computing for scalable ex.pdf:application/pdf},
}

@misc{bharti_exploiting_2023,
	location = {Rochester, {NY}},
	title = {Exploiting Serverless Scalability for Dynamic Task Parallelism},
	url = {https://papers.ssrn.com/abstract=4659540},
	doi = {10.2139/ssrn.4659540},
	abstract = {In recent years, serverless computing has proven to be efficient for running parallel workloads. Several studies exist for the execution of the parallel workload in a serverless environment, but these solutions either use orchestration services or other services where server configuration is required. Serverless orchestration services are meant for implementing different workflows, including parallel ones, but are still in their primitive stages and do not provide inherent support for dynamic parallelism. To fully utilize serverless computing's auto-scalability capability across many application domains, dynamic parallel function composition must be supported. Thus, we concentrate on a comprehensive, serverless, and portable design solution for dynamic task parallelization.Here, we present a framework, {DyTaPa}, that facilitates parallel execution of tasks in a serverless environment. Our framework focuses on designing the applications for dynamic parallelism and takes advantage of the rapid scalability of the serverless environment. The {DyTaPa} supports dynamic scheduling of tasks for parallel execution and is implemented using {AWS} serverless services and cloud object storage services. There are three main components of the framework: (i) the Central Controller is responsible for scalable parallel scheduling; (ii) the Scalable Executor manages the execution of each task; and (iii) the Function Converter serializes the user function code so that it can be kept in object storage and run upon user request. The key feature of {DyTaPa} is that its design is truly scalable, as it can handle the parallel execution of any number of tasks on input data of any size. Our findings show that the tasks are scheduled at a rate of 235 ms/task by the Central Controller to initiate their parallel execution. We also conducted a comparison between the execution of a static parallel workflow in {AWS} Step Functions and a basic serverless driver that handled the parallel composition programmatically. The results showed that the simple serverless solution yielded a 19\% improvement in execution time. These tests show that managing a workflow programmatically outperforms an orchestration service solution. Furthermore, our experiments demonstrate the feasibility of a complete serverless and portable design solution for dynamic task parallelism. A distributed computing application for parallel workloads can be built on top of {DyTaPa}. Therefore, attaining elasticity in a dynamic parallel composition is now simple for cloud application developers.},
	number = {4659540},
	author = {Bharti, Urmil and Goel, Anita and Gupta, S. C.},
	urldate = {2024-05-17},
	date = {2023-12-09},
	langid = {english},
	keywords = {Serverless Computing, Function-as-a-Service, Cloud Computing, {AWS} Step Functions, Parallel Workloads, Task Parallelism},
	file = {Full Text PDF:/home/larissa/Zotero/storage/YEBGE4I6/Bharti et al. - 2023 - Exploiting Serverless Scalability for Dynamic Task.pdf:application/pdf},
}

@article{mahdizadeh_assignment_2024,
	title = {An assignment mechanism for workflow scheduling in Function as a Service edge environment},
	volume = {157},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X24001328},
	doi = {10.1016/j.future.2024.04.003},
	abstract = {Serverless computing has revolutionized cloud-based software development for software developers, addressing many of the associated challenges. With resource management and infrastructure provisioning handled by the provider, developers can focus on deploying services at the application level, which has gained significant popularity. Edge computing, with its proximity to end-users and ability to offer low-cost, high-speed services, has emerged as a promising solution for latency-sensitive applications. By providing Function-as-a-Service ({FaaS}) capabilities at the network edge, it becomes easier to leverage the available edge capacity. However, edge servers have limited capacity to provide {FaaS} services, and user access is typically restricted to their geographic region. This presents a challenge in achieving optimal resource allocation in a multi-user environment, given the dynamic and unpredictable nature of edge computing. To address these challenges, we have proposed a dynamic resource provisioning and allocation approach for Edge {FaaS}. Our approach models the application structure as a directed acyclic graph, representing a workflow of computational functions. Drawing inspiration from the many-to-many assignment mechanism used in the course allocation problem, we have developed a mechanism to assign multi-user workflows to the limited Edge {FaaS} resources. This mechanism considers the priority of users’ ready tasks, determined by upward rank, with the goal of reducing workflow execution time. Additionally, we have proposed two methods for on-demand provisioning: the Highest Bid First Mechanism ({HBFM}) and the Warm Function First Mechanism ({WFFM}). Experimental results have shown that the proposed resource allocation methods effectively allocate limited resources to users, leading to reduced workflow completion time.},
	pages = {543--557},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Mahdizadeh, Samaneh Hajy and Abrishami, Saeid},
	urldate = {2024-05-17},
	date = {2024-08-01},
	keywords = {Workflow scheduling, Course allocation, Edge function-as-a-service, Mechanism design, Resource allocation and provisioning},
	file = {Mahdizadeh and Abrishami - 2024 - An assignment mechanism for workflow scheduling in.pdf:/home/larissa/Zotero/storage/YJILND35/Mahdizadeh and Abrishami - 2024 - An assignment mechanism for workflow scheduling in.pdf:application/pdf;ScienceDirect Snapshot:/home/larissa/Zotero/storage/QFCSZC2W/S0167739X24001328.html:text/html},
}

@article{lee_mitigating_2021,
	title = {Mitigating Cold Start Problem in Serverless Computing with Function Fusion},
	volume = {21},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/24/8416},
	doi = {10.3390/s21248416},
	abstract = {As Artificial Intelligence ({AI}) is becoming ubiquitous in many applications, serverless computing is also emerging as a building block for developing cloud-based {AI} services. Serverless computing has received much interest because of its simplicity, scalability, and resource efficiency. However, due to the trade-off with resource efficiency, serverless computing suffers from the cold start problem, that is, a latency between a request arrival and function execution. The cold start problem significantly influences the overall response time of workflow that consists of functions because the cold start may occur in every function within the workflow. Function fusion can be one of the solutions to mitigate the cold start latency of a workflow. If two functions are fused into a single function, the cold start of the second function is removed; however, if parallel functions are fused, the workflow response time can be increased because the parallel functions run sequentially even if the cold start latency is reduced. This study presents an approach to mitigate the cold start latency of a workflow using function fusion while considering a parallel run. First, we identify three latencies that affect response time, present a workflow response time model considering the latency, and efficiently find a fusion solution that can optimize the response time on the cold start. Our method shows a response time of 28–86\% of the response time of the original workflow in five workflows.},
	pages = {8416},
	number = {24},
	journaltitle = {Sensors},
	author = {Lee, Seungjun and Yoon, Daegun and Yeo, Sangho and Oh, Sangyoon},
	urldate = {2024-05-17},
	date = {2021-01},
	langid = {english},
	note = {Number: 24
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {serverless computing, serverless workflow, function fusion},
	file = {Full Text PDF:/home/larissa/Zotero/storage/HI2VLYBW/Lee et al. - 2021 - Mitigating Cold Start Problem in Serverless Comput.pdf:application/pdf},
}

@incollection{garcia-lopez_trade-offs_2022,
	location = {Cham},
	title = {Trade-Offs and Challenges of Serverless Data Analytics},
	isbn = {978-3-030-78307-5},
	url = {https://doi.org/10.1007/978-3-030-78307-5_3},
	abstract = {Serverless computing has become very popular today since it largely simplifies cloud programming. Developers do no longer need to worry about provisioning or operating servers, and they have to pay only for the compute resources used when their code is run. This new cloud paradigm suits well for many applications, and researchers have already begun investigating the feasibility of serverless computing for data analytics. Unfortunately, today’s serverless computing presents important limitations that make it really difficult to support all sorts of analytics workloads. This chapter first starts by analyzing three fundamental trade-offs of today’s serverless computing model and their relationship with data analytics. It studies how by relaxing disaggregation, isolation, and simple scheduling, it is possible to increase the overall computing performance, but at the expense of essential aspects of the model such as elasticity, security, or sub-second activations, respectively. The consequence of these trade-offs is that analytics applications may well end up embracing hybrid systems composed of serverless and serverful components, which we call {ServerMix} in this chapter. We will review the existing related work to show that most applications can be actually categorized as {ServerMix}.},
	pages = {41--61},
	booktitle = {Technologies and Applications for Big Data Value},
	publisher = {Springer International Publishing},
	author = {García-López, Pedro and Sánchez-Artigas, Marc and Shillaker, Simon and Pietzuch, Peter and Breitgand, David and Vernik, Gil and Sutra, Pierre and Tarrant, Tristan and Juan-Ferrer, Ana and París, Gerard},
	editor = {Curry, Edward and Auer, Sören and Berre, Arne J. and Metzger, Andreas and Perez, Maria S. and Zillner, Sonja},
	urldate = {2024-05-17},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-030-78307-5_3},
	keywords = {Cloud computing, Serverless computing, Data analytics},
	file = {Full Text PDF:/home/larissa/Zotero/storage/4GMK8RKS/García-López et al. - 2022 - Trade-Offs and Challenges of Serverless Data Analy.pdf:application/pdf},
}

@inproceedings{smirnov_apollo_2020,
	location = {Virtual Event Sweden},
	title = {Apollo: Modular and Distributed Runtime System for Serverless Function Compositions on Cloud, Edge, and {IoT} Resources},
	isbn = {978-1-4503-8388-2},
	url = {https://dl.acm.org/doi/10.1145/3452413.3464793},
	doi = {10.1145/3452413.3464793},
	shorttitle = {Apollo},
	abstract = {This paper provides a first presentation of Apollo, a runtime system for serverless function compositions distributed across the cloud-edge-{IoT} continuum. Apollo’s modular design enables a finegrained decomposition of the runtime implementation (scheduling, data transmission, etc.) of the application, so that each of the numerous implementation decisions can be optimized separately, fully exploiting the potential for the optimization of the overall performance and costs. Apollo features (a) a flexible model of the application and the available resources and (b) an implementation process based on a large set of independent agents. This flexible structure enables distributing not only the processing, but the implementation process itself across a large number of resources, each running an independent Apollo instance. The ability to flexibly determine the placement of implementation actions opens up new optimization opportunities, while at the same time providing access to greater computing power for optimizing challenging decisions such as task scheduling and the placement and routing of data.},
	eventtitle = {{HPDC} '21: The 30th International Symposium on High-Performance Parallel and Distributed Computing},
	pages = {5--8},
	booktitle = {Proceedings of the 1st Workshop on High Performance Serverless Computing},
	publisher = {{ACM}},
	author = {Smirnov, Fedor and Pourmohseni, Behnaz and Fahringer, Thomas},
	urldate = {2024-05-17},
	date = {2020-06-25},
	langid = {english},
	file = {Smirnov et al. - 2020 - Apollo Modular and Distributed Runtime System for.pdf:/home/larissa/Zotero/storage/F4ZJXBEW/Smirnov et al. - 2020 - Apollo Modular and Distributed Runtime System for.pdf:application/pdf},
}

@inproceedings{sicari_event-driven_2023,
	location = {Taormina (Messina) Italy},
	title = {Event-Driven {FaaS} Workflows for Enabling {IoT} Data Processing at the Cloud Edge Continuum},
	isbn = {9798400702341},
	url = {https://dl.acm.org/doi/10.1145/3603166.3632125},
	doi = {10.1145/3603166.3632125},
	abstract = {Continuum Computing encompasses the integration of diverse infrastructures, including cloud, edge, and fog, to facilitate seamless migration of applications based on their specific needs, ensuring optimal satisfaction of their requirements. The primary obstacles in this particular context mostly pertain to the incapacity to promptly respond to changes in the environment or the quality of service ({QoS}) constraints of the application, as well as the incapability to maintain an application in a stateless manner, hence impeding its relocation without the risk of data loss. The objective of this research is to tackle the aforementioned issues through the introduction of a framework based on Function-as-a-Service ({FaaS}) and event-driven architecture. This framework enables the decomposition, localization, and relocation of applications inside a Continuum infrastructure, facilitated by a rule engine that is both system and data-aware.},
	eventtitle = {{UCC} '23: {IEEE}/{ACM} 16th International Conference on Utility and Cloud Computing},
	pages = {1--10},
	booktitle = {Proceedings of the {IEEE}/{ACM} 16th International Conference on Utility and Cloud Computing},
	publisher = {{ACM}},
	author = {Sicari, Christian and Balouek, Daniel and Parashar, Manish and Villari, Massimo},
	urldate = {2024-05-17},
	date = {2023-12-04},
	langid = {english},
	file = {Sicari et al. - 2023 - Event-Driven FaaS Workflows for Enabling IoT Data .pdf:/home/larissa/Zotero/storage/SVHQVRHH/Sicari et al. - 2023 - Event-Driven FaaS Workflows for Enabling IoT Data .pdf:application/pdf},
}

@inproceedings{lei_chitu_2023,
	location = {New York, {NY}, {USA}},
	title = {Chitu: Accelerating Serverless Workflows with Asynchronous State Replication Pipelines},
	isbn = {9798400703874},
	url = {https://dl.acm.org/doi/10.1145/3620678.3624794},
	doi = {10.1145/3620678.3624794},
	series = {{SoCC} '23},
	shorttitle = {Chitu},
	abstract = {Serverless workflows are characterized as multi-stage computing, while downstream functions require accessing intermediate states or the output of upstream functions for running. The workflow's performance can be easily affected due to the inefficiency of data access. Studies accelerate data access with various policies, such as direct and indirect methods. However, these methods may fail due to various limitations such as resource availability. In this paper, we propose asynchronous state replication pipelines ({ASRP}) to speed up workflows for general applications, replacing the sequential computing pattern of current workflows. Chitu is built based on the insight with three main points. First, differentiable data types ({DDT}) are provided at the programming model level to support incremental state sharing and computation. Second, {ASRP} works by continuously delivering changes of {DDT} objects in real-time so that downstream functions can consume the objects without waiting for the ending of upstream functions. Third, we make a systematic design to support {DDT} and {ASRP} in Chitu framework, including direct communication and change propagation. We implement Chitu atop {OpenFaaS}, compare it with popular serverless workflow frameworks, and evaluate it with three commonly seen cases. The results show that Chitu accelerates data transmission in general serverless workflows up to 1.7×, and speeds up end-to-end applications by up to 57\%.},
	pages = {597--610},
	booktitle = {Proceedings of the 2023 {ACM} Symposium on Cloud Computing},
	publisher = {Association for Computing Machinery},
	author = {Lei, Zhengyu and Shi, Xiao and Lv, Cunchi and Yu, Xiaobing and Zhao, Xiaofang},
	urldate = {2024-05-17},
	date = {2023-10-31},
	keywords = {Serverless workflow, Asynchronous state replication, Differentiable data type, Pipeline},
	file = {Full Text PDF:/home/larissa/Zotero/storage/XCT3X8T5/Lei et al. - 2023 - Chitu Accelerating Serverless Workflows with Asyn.pdf:application/pdf},
}

@inproceedings{ristov_large-scale_2023,
	location = {New York, {NY}, {USA}},
	title = {Large-scale Graph Processing and Simulation with Serverless Workflows in Federated {FaaS}},
	isbn = {9798400700729},
	url = {https://dl.acm.org/doi/10.1145/3578245.3585333},
	doi = {10.1145/3578245.3585333},
	series = {{ICPE} '23 Companion},
	abstract = {Serverless computing offers an affordable and easy way to code lightweight functions that can be invoked based on some events to perform simple tasks. For more complicated processing, multiple serverless functions can be orchestrated as a directed acyclic graph to form a serverless workflow, so-called function choreography ({FC}). Although most famous cloud providers offer {FC} management systems such as {AWS} Step Functions, and there are also several open-source {FC} management systems (e.g., Apache {OpenWhisk}), their primary focus is on describing the control flow and data flow between serverless functions in the {FC}. Moreover, the existing {FC} management systems rarely consider the processed data, which is commonly represented in a graph format. In this paper, we review the capabilities of the existing {FC} management systems in supporting graph processing applications. We also raise two key research questions related to large-scale graph processing using serverless computing in federated Function-as-a-Service ({FaaS}). As part of the Graph-Massivizer project, funded by the Horizon Europe research and innovation program, we will research and develop (prototype) solutions that will address these challenges.},
	pages = {227--231},
	booktitle = {Companion of the 2023 {ACM}/{SPEC} International Conference on Performance Engineering},
	publisher = {Association for Computing Machinery},
	author = {Ristov, Sashko and Farahani, Reza and Prodan, Radu},
	urldate = {2024-05-17},
	date = {2023-04-15},
	keywords = {serverless computing, workflows, computing continuum, graph processing, massive graph},
	file = {Full Text PDF:/home/larissa/Zotero/storage/HA8CPE34/Ristov et al. - 2023 - Large-scale Graph Processing and Simulation with S.pdf:application/pdf},
}

@inproceedings{gusev_cardiohpc_2022,
	title = {{CardioHPC}: Serverless Approaches for Real-Time Heart Monitoring of Thousands of Patients},
	url = {https://ieeexplore.ieee.org/abstract/document/10023939},
	doi = {10.1109/WORKS56498.2022.00015},
	shorttitle = {{CardioHPC}},
	abstract = {We analyze a heart monitoring center for patients wearing electrocardiogram sensors outside hospitals. This prevents serious heart damages and increases life expectancy and health-care efficiency. In this paper, we address a problem to provide a scalable infrastructure for the real-time processing scenario for at least 10,000 patients simultaneously, and efficient fast processing architecture for the postponed scenario when patients upload data after realized measurements. {CardioHPC} is a project to realize a simulation of these two scenarios using digital signal processing algorithms and artificial intelligence-based detection and classification software for automated reporting and alerting. We elaborate the challenges we met in experimenting with different serverless implementations: 1) container-based on Google Cloud Run, and 2) Function-as-a-Service ({FaaS}) on {AWS} Lambda. Experimental results present the effect of overhead in the request and transfer time, and speedup achieved by analyzing the response time and throughput on both container-based and {FaaS} implementations as serverless workflows.},
	eventtitle = {2022 {IEEE}/{ACM} Workshop on Workflows in Support of Large-Scale Science ({WORKS})},
	pages = {76--83},
	booktitle = {2022 {IEEE}/{ACM} Workshop on Workflows in Support of Large-Scale Science ({WORKS})},
	author = {Gusev, Marjan and Ristov, Sashko and Amza, Andrei and Hohenegger, Armin and Prodan, Radu and Mileski, Dimitar and Gushev, Pano and Temelkov, Goran},
	urldate = {2024-05-17},
	date = {2022-11},
	keywords = {Software, Software algorithms, Throughput, workflow, Heart, serverless, Real-time systems, Batch processing, monitoring, real-time heart, Sensors, Signal processing algorithms, stream processing},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/VQC63BWQ/10023939.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/83GNYCFR/Gusev et al. - 2022 - CardioHPC Serverless Approaches for Real-Time Hea.pdf:application/pdf},
}

@inproceedings{jarachanthan_acts_2023,
	title = {{ACTS}: Autonomous Cost-Efficient Task Orchestration for Serverless Analytics},
	url = {https://ieeexplore.ieee.org/abstract/document/10188782},
	doi = {10.1109/IWQoS57198.2023.10188782},
	shorttitle = {{ACTS}},
	abstract = {Serverless computing has become increasingly popular for cloud applications, due to its compelling properties of high-level abstractions, lightweight runtime, high elasticity and pay-per-use billing. In this revolutionary computing paradigm shift, challenges arise when adapting data analytics applications to the serverless environment, due to the lack of support for efficient state sharing, which attract ever-growing research attention. In this paper, we aim to exploit the advantages of task-level orchestration and fine-grained resource provisioning for data analytics on serverless platforms, with the hope of fulfilling the promise of serverless deployment to the maximum extent. To this end, we present {ACTS}, an autonomous cost-efficient task orchestration framework for serverless analytics. {ACTS} judiciously schedules and coordinates function tasks to mitigate cold-start latency and state sharing overhead. In addition, {ACTS} explores the optimization space of fine-grained workload distribution and function resource configuration for cost efficiency. We have deployed and implemented {ACTS} on {AWS} Lambda, evaluated with various data analytics workloads. Results from extensive experiments demonstrate that {ACTS} achieves up to 98\% monetary cost reduction while maintaining superior job completion time performance, in comparison with the state-of-the-art baselines.},
	eventtitle = {2023 {IEEE}/{ACM} 31st International Symposium on Quality of Service ({IWQoS})},
	pages = {1--10},
	booktitle = {2023 {IEEE}/{ACM} 31st International Symposium on Quality of Service ({IWQoS})},
	author = {Jarachanthan, Jananie and Chen, Li and Xu, Fei},
	urldate = {2024-05-17},
	date = {2023-06},
	note = {{ISSN}: 2766-8568},
	keywords = {Costs, Runtime, Quality of service, serverless computing, Schedules, Serverless computing, Elasticity, cloud resource provisioning, cost-efficiency, Data analysis, data analytics},
	file = {IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/FWVBRJ8P/Jarachanthan et al. - 2023 - ACTS Autonomous Cost-Efficient Task Orchestration.pdf:application/pdf},
}

@inproceedings{khochare_xfaas_2023,
	title = {{XFaaS}: Cross-platform Orchestration of {FaaS} Workflows on Hybrid Clouds},
	url = {https://ieeexplore.ieee.org/abstract/document/10171551},
	doi = {10.1109/CCGrid57682.2023.00053},
	shorttitle = {{XFaaS}},
	abstract = {Functions as a Service ({FaaS}) have gained popularity for programming public clouds due to their simple abstraction, ease of deployment, effortless scaling and granular billing. Cloud providers also offer basic capabilities to compose these functions into workflows. {FaaS} and {FaaS} workflow models, however, are proprietary to each cloud provider. This prevents their portability across cloud providers, and requires effort to design workflows that run on different cloud providers or data centers. Such requirements are increasingly important to meet regulatory requirements, leverage cost arbitrage and avoid vendor lock-in. Further, the {FaaS} execution models are also different, and the overheads of {FaaS} workflows due to message indirection and cold-starts need custom optimizations for different platforms. In this paper, we propose {XFaaS}, a cross-platform deployment and orchestration engine for {FaaS} workflows to operate on multiple clouds. {XFaaS} allows “zero touch” deployment of functions and workflows across {AWS} and Azure clouds by automatically generating the necessary code wrappers, cloud queues, and coordinating with the native {FaaS} engine of the cloud providers. It also uses intelligent function fusion and placement logic to reduce the workflow execution latency in a hybrid cloud while mitigating costs, using performance and billing models specific to the providers based in detailed benchmarks. Our empirical results indicate that fusion offers up to ≈75 \% benefits in latency and ≈57\% reduction in cost, while placement strategies reduce the latency by ≈ 24\%, compared to baselines in the best cases.},
	eventtitle = {2023 {IEEE}/{ACM} 23rd International Symposium on Cluster, Cloud and Internet Computing ({CCGrid})},
	pages = {498--512},
	booktitle = {2023 {IEEE}/{ACM} 23rd International Symposium on Cluster, Cloud and Internet Computing ({CCGrid})},
	author = {Khochare, Aakash and Khare, Tuhin and Kulkarni, Varad and Simmhan, Yogesh},
	urldate = {2024-05-17},
	date = {2023-05},
	keywords = {Costs, Packaging, Programming, Cloud computing, Codes, {FaaS}, Serverless, Data centers, Multi-Cloud, {NoSQL} databases, Workflows},
	file = {IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/9KSQFS8B/Khochare et al. - 2023 - XFaaS Cross-platform Orchestration of FaaS Workfl.pdf:application/pdf},
}

@article{feng_heterogeneity-aware_2024,
	title = {Heterogeneity-aware Proactive Elastic Resource Allocation for Serverless Applications},
	issn = {1939-1374},
	url = {https://ieeexplore.ieee.org/abstract/document/10382635},
	doi = {10.1109/TSC.2024.3350711},
	abstract = {Serverless computing is a popular cloud computing model that offers on-demand resource allocation and pay-as-you-go application execution. However, there are still challenges in allocating resources for workflow applications: inaccurate and inefficient resource estimation, high-latency inter-function communication, and long server readiness time. Therefore, we propose the heterogeneity-aware Proactive {serverLess} {wOrkflow} Elastic Allocation method ({PLOEA}) to address these issues and optimize infrastructure costs for cloud service providers ({CSPs}) while meeting the diverse needs of developers. Specifically, we propose a resource configuration estimation method for heterogeneous workflow applications that builds an ensemble multi-task expert classifier to analyze individual and common resource usage patterns, ensuring estimation accuracy and efficiency. Further, we propose a group allocation strategy for multiple applications that optimizes the spatiotemporal distribution of instances by considering the allocation urgency, communication affinity between functions, and the multi-core architecture of servers. Furthermore, we present a proactive server elastic scaling method that senses workload features, including workload level, trend, and magnitude changes, and combines them with {CSP}'s attention differences to guide the server scaling size. Finally, experiments based on public datasets prove that {PLOEA} provides better service quality and cost efficiency than existing methods.},
	pages = {1--14},
	journaltitle = {{IEEE} Transactions on Services Computing},
	author = {Feng, Binbin and Ding, Zhijun and Zhou, Xiaobo and Jiang, Changjun},
	urldate = {2024-05-17},
	date = {2024},
	note = {Conference Name: {IEEE} Transactions on Services Computing},
	keywords = {Costs, Predictive models, Servers, Runtime, Quality of service, workflow, Estimation, Serverless, instance allocation, Media, {NUMA}, resource estimation, server scaling, workload prediction},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/UNB772S6/10382635.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/GYE28BMV/Feng et al. - 2024 - Heterogeneity-aware Proactive Elastic Resource All.pdf:application/pdf},
}

@inproceedings{kusnierz_serverless_2022,
	title = {A Serverless Engine for High Energy Physics Distributed Analysis},
	url = {https://ieeexplore.ieee.org/abstract/document/9826036},
	doi = {10.1109/CCGrid54584.2022.00067},
	abstract = {The Large Hadron Collider ({LHC}) at {CERN} has generated in the last decade an unprecedented volume of data for the High-Energy Physics ({HEP}) field. Scientific collaborations interested in analysing such data very often require computing power beyond a single machine. This issue has been tackled traditionally by running analyses in distributed environments using stateful, managed batch computing systems. While this approach has been effective so far, current estimates for future computing needs of the field present large scaling challenges. Such a managed approach may not be the only viable way to tackle them and an interesting alternative could be provided by serverless architectures, to enable an even larger scaling potential. This work describes a novel approach to running real {HEP} scientific applications through a distributed serverless computing engine. The engine is built upon {ROOT}, a well-established {HEP} data analysis software, and distributes its computations to a large pool of concurrent executions on Amazon Web Services Lambda Serverless Platform. Thanks to the developed tool, physicists are able to access datasets stored at {CERN} (also those that are under restricted access policies) and process it on remote infrastructures outside of their typical environment. The analysis of the serverless functions is monitored at runtime to gather performance metrics, both for data- and computation-intensive workloads.},
	eventtitle = {2022 22nd {IEEE} International Symposium on Cluster, Cloud and Internet Computing ({CCGrid})},
	pages = {575--584},
	booktitle = {2022 22nd {IEEE} International Symposium on Cluster, Cloud and Internet Computing ({CCGrid})},
	author = {Kuśnierz, Jacek and Padulano, Vincenzo E. and Malawski, Maciej and Burkiewicz, Kamil and Saavedra, Enric Tejedor and Alonso-Jordá, Pedro and Pitt, Michael and Avati, Valentina},
	urldate = {2024-05-17},
	date = {2022-05},
	keywords = {Computer architecture, Runtime, Web services, Codes, Serverless computing, Serverless, {AWS}, C++ languages, Distributed Computing, {CERN}, {HEP}, Lambda, Large Hadron Collider, {MapReduce}, {ROOT}},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/W5MQVQZW/9826036.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/BYH2DNRQ/Kuśnierz et al. - 2022 - A Serverless Engine for High Energy Physics Distri.pdf:application/pdf},
}

@inproceedings{bharti_scalable_2022,
	title = {A Scalable Design Approach for State Propagation in Serverless Workflow},
	url = {https://ieeexplore.ieee.org/abstract/document/9972158},
	doi = {10.1109/GCAT55367.2022.9972158},
	abstract = {Serverless development is challenging as applications are composed of stateless and short-lived functions. Many workflows require time-bound functions to transfer their state to other function before termination. The serverless Function-as-a-Service offerings lack state management support; therefore, it must be handled at application-level. In this paper, we propose a scalable design approach that simplifies development of workflows that require sharing of ephemeral intermediate data. Our design uses object serialization/deserialization with cloud object storage to share state across functions. It provides a mechanism for fine-grained support for state propagation and synchronization in a serverless workflow. This solution is cost-effective and efficient as it does not depend on any external database or cache for state management. The design has been validated by implementing ‘Word Count’- a classic {MapReduce} use case. Our results show that the proposed scalable design can process input of any size and can handle state propagation in complex serverless workflow.},
	eventtitle = {2022 {IEEE} 3rd Global Conference for Advancement in Technology ({GCAT})},
	pages = {1--7},
	booktitle = {2022 {IEEE} 3rd Global Conference for Advancement in Technology ({GCAT})},
	author = {Bharti, Urmil and Goel, Anita and Gupta, S. C.},
	urldate = {2024-05-17},
	date = {2022-10},
	keywords = {Costs, Databases, Computational modeling, Libraries, Containers, Serverless computing, {FaaS}, Serverless, Computer languages, Function-as-a-service, Serverless Workflows, Serverless Composition, Serverless Stateful Application, State Management},
	file = {IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/WI8A2R6S/Bharti et al. - 2022 - A Scalable Design Approach for State Propagation i.pdf:application/pdf},
}

@inproceedings{ristov_faast_2022,
	title = {{FaaSt}: Optimize makespan of serverless workflows in federated commercial {FaaS}},
	url = {https://ieeexplore.ieee.org/abstract/document/9912692},
	doi = {10.1109/CLUSTER51413.2022.00032},
	shorttitle = {{FaaSt}},
	abstract = {Nowadays, scientists migrate workflow applications on serverless Function-as-a-Service ({FaaS}) platforms in a form of so called function choreographies ({FCs}) to benefit from {FaaS} high elasticity and instantly spawning numerous functions. How-ever, the heterogeneous nature of federated {FaaS} overburdens decisions for the most appropriate configuration setup. Unfor-tunately, related work mainly support either (i) scheduling of serverful workflow applications that run on virtual machines or (ii) container-based algorithms to schedule individual functions on specific container (executor). Either approach is hard to implement for {FCs} in federated {FaaS}; the former due to specifics of the {FaaS} deployment model, while the latter because they are primarily focused on bag of functions and reducing startup latency down to microseconds. Such optimization is negligible for scientific {FCs} whose functions may run hundreds of seconds due to enormous compute and I/O operations to distributed cloud storage. Instead, scientific {FCs} would benefit from schedulers that select the appropriate {FaaS} provider, cloud region, and memory settings. To bridge this gap in scheduling scientific {FCs}, this paper introduces {FaaSt}, a novel list-based {FC} scheduler that optimizes makespan of an {FC} that runs functions in federated {FaaS}. The evaluation with three other schedulers showed that {FaaSt} overcomes limitations of a single {FaaS} region and generates speedup of up to 2.82× when running {FCs} across four cloud regions compared to a single region. Moreover, {FaaSt} achieves speedup of up to 1.74 × compared to the other state-of-the-art {FC} schedulers across the same four regions.},
	eventtitle = {2022 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	pages = {183--194},
	booktitle = {2022 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	author = {Ristov, Sashko and Gritsch, Philipp},
	urldate = {2024-05-17},
	date = {2022-09},
	note = {{ISSN}: 2168-9253},
	keywords = {Computational modeling, Cloud computing, serverless, Schedules, Containers, {FaaS}, Elasticity, scientific workflows, Cluster computing, function choreography, Processor scheduling, scheduling},
	file = {IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/LLDGFS79/Ristov and Gritsch - 2022 - FaaSt Optimize makespan of serverless workflows i.pdf:application/pdf},
}

@article{trakadas_hybrid_2019,
	title = {Hybrid Clouds for Data-Intensive, 5G-Enabled {IoT} Applications: An Overview, Key Issues and Relevant Architecture},
	volume = {19},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/16/3591},
	doi = {10.3390/s19163591},
	shorttitle = {Hybrid Clouds for Data-Intensive, 5G-Enabled {IoT} Applications},
	abstract = {Hybrid cloud multi-access edge computing ({MEC}) deployments have been proposed as efficient means to support Internet of Things ({IoT}) applications, relying on a plethora of nodes and data. In this paper, an overview on the area of hybrid clouds considering relevant research areas is given, providing technologies and mechanisms for the formation of such {MEC} deployments, as well as emphasizing several key issues that should be tackled by novel approaches, especially under the 5G paradigm. Furthermore, a decentralized hybrid cloud {MEC} architecture, resulting in a Platform-as-a-Service ({PaaS}) is proposed and its main building blocks and layers are thoroughly described. Aiming to offer a broad perspective on the business potential of such a platform, the stakeholder ecosystem is also analyzed. Finally, two use cases in the context of smart cities and mobile health are presented, aimed at showing how the proposed {PaaS} enables the development of respective {IoT} applications.},
	pages = {3591},
	number = {16},
	journaltitle = {Sensors},
	author = {Trakadas, Panagiotis and Nomikos, Nikolaos and Michailidis, Emmanouel T. and Zahariadis, Theodore and Facca, Federico M. and Breitgand, David and Rizou, Stamatia and Masip, Xavi and Gkonis, Panagiotis},
	urldate = {2024-05-17},
	date = {2019-01},
	langid = {english},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cloud native solutions, cloud-to-fog infrastructure, data pipelines, hybrid clouds, {IoT} applications, {PaaS}},
	file = {Full Text PDF:/home/larissa/Zotero/storage/RRTQWPC7/Trakadas et al. - 2019 - Hybrid Clouds for Data-Intensive, 5G-Enabled IoT A.pdf:application/pdf},
}

@inproceedings{khochare_toward_2022,
	title = {Toward Scientific Workflows in a Serverless World},
	url = {https://ieeexplore.ieee.org/abstract/document/9973585},
	doi = {10.1109/eScience55777.2022.00057},
	abstract = {Serverless computing and {FaaS} have gained popularity due to their ease of design, deployment, scaling and billing on clouds. However, when used to compose and orchestrate scientific workflows, they pose limitations due to cold starts, message indirection, vendor lock-in and lack of provenance support. Here, we propose a design for a Ser verless Scientific Workflow Orchestrator that overcomes these challenges using techniques like function fusion, pilot invocations and data fabrics.},
	eventtitle = {2022 {IEEE} 18th International Conference on e-Science (e-Science)},
	pages = {399--400},
	booktitle = {2022 {IEEE} 18th International Conference on e-Science (e-Science)},
	author = {Khochare, Aakash and Simmhan, Yogesh and Mehta, Sameep and Agarwal, Arvind},
	urldate = {2024-05-17},
	date = {2022-10},
	keywords = {Cloud computing, Serverless computing, {FaaS}, Serverless, Workflow, Fabrics},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/HAQP7DFJ/9973585.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/BJQWD8TJ/Khochare et al. - 2022 - Toward Scientific Workflows in a Serverless World.pdf:application/pdf},
}

@article{carpio_benchfaas_2023,
	title = {{BenchFaaS}: Benchmarking Serverless Functions in an Edge Computing Network Testbed},
	volume = {37},
	issn = {1558-156X},
	url = {https://ieeexplore.ieee.org/abstract/document/9877930},
	doi = {10.1109/MNET.125.2200294},
	shorttitle = {{BenchFaaS}},
	abstract = {The serverless computing model has evolved as one of the key solutions in the cloud for fast autoscaling and capacity planning. In edge computing environments, however, the serverless model is challenged by the system heterogeneity and performance variability. In this paper, we introduce {BenchFaaS}, an open-source edge computing network testbed which automates the deployment and benchmarking of serverless functions. Our edge computing network considers a cluster of virtual machines and Raspberry Pis, and is designed to benchmark serverless functions under different hardware and network conditions. We measure and evaluate: (i) overhead incurred by testbed, (ii) performance of compute intensive tasks, (iii) impact of application payload size, (iv) scalability, and (v) performance of chained serverless functions. We share the lessons learnt in engineering and implementing the testbed. We present the measurement results and analyze the impact of networked infrastructure on serverless performance. The measurements indicate that a properly dimensioned edge computing network can effectively serve as a serverless infrastructure.},
	pages = {81--88},
	number = {5},
	journaltitle = {{IEEE} Network},
	author = {Carpio, Francisco and Michalke, Marc and Jukan, Admela},
	urldate = {2024-05-17},
	date = {2023-09},
	note = {Conference Name: {IEEE} Network},
	keywords = {Scalability, Performance evaluation, Servers, Benchmark testing, Computational modeling, Random access memory, Serverless computing, Open source software, Edge computing, Overlay networks, Payloads, Size measurement, Test facilities, Virtual machining, Wide area networks},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/Q2DEYWTT/9877930.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/PR5QQSB2/Carpio et al. - 2023 - BenchFaaS Benchmarking Serverless Functions in an.pdf:application/pdf},
}

@inproceedings{garofalo_workflow_2024,
	location = {New York, {NY}, {USA}},
	title = {Workflow Engines in the Compute Continuum: a Comparative Analysis},
	isbn = {9798400702341},
	url = {https://dl.acm.org/doi/10.1145/3603166.3632148},
	doi = {10.1145/3603166.3632148},
	series = {{UCC} '23},
	shorttitle = {Workflow Engines in the Compute Continuum},
	abstract = {The intersection of Cloud and Edge computing has ushered in a new era of distributed computing, known as Continuum Computing. This computational paradigm shift leverages the computational strength of Cloud infrastructure and the pervasiveness of Edge devices to handle massive workloads, reshaping the landscape of distributed computing. One of the most challenging applications of Continuum Computing is the orchestration of scientific workflows, known as Continuum workflows. This problem is tackled by Workflow Management Systems ({WMSs}). However, the variety of {WMSs} in the literature is based on different techniques due to the rapid evolution of these systems. This study aims to compare two {WMSs}, that are Pegasus {WMS} and {OpenWolf}, by analyzing their key orchestration strategies and evaluating the performance of the two systems on workflows designed for Artificial Intelligence ({AI}) tasks. This comparative analysis is the first step toward identifying the most suitable frameworks for deploying {AI}-driven pipelines across the Continuum.},
	pages = {1--10},
	booktitle = {Proceedings of the {IEEE}/{ACM} 16th International Conference on Utility and Cloud Computing},
	publisher = {Association for Computing Machinery},
	author = {Garofalo, Marco and Morabito, Gabriele and Fazio, Maria and Celesti, Antonio and Villari, Massimo},
	urldate = {2024-05-17},
	date = {2024-04-04},
	keywords = {artificial intelligence, cloud-edge, compute continuum, continuum workflow, {OpenWolf}, pegasus, {WMS}},
	file = {Full Text PDF:/home/larissa/Zotero/storage/AI5KTBVG/Garofalo et al. - 2024 - Workflow Engines in the Compute Continuum a Compa.pdf:application/pdf},
}

@inproceedings{sowinski_autonomous_2023,
	title = {Autonomous Choreography of {WebAssembly} Workloads in the Federated Cloud-Edge-{IoT} Continuum},
	url = {https://ieeexplore.ieee.org/abstract/document/10490045},
	doi = {10.1109/CloudNet59005.2023.10490045},
	abstract = {The concept of the federated Cloud-Edge-{IoT} continuum promises to alleviate many woes of current systems, improving resource use, energy efficiency, quality of service, and more. However, this continuum is still far from being realized in practice, with no comprehensive solutions for developing, deploying, and managing continuum-native applications. Breakthrough innovations and novel system architectures are needed to cope with the ever-increasing heterogeneity and the multi-stakeholder nature of computing resources. This work proposes a novel architecture for choreographing workloads in the continuum, attempting to address these challenges. The architecture tackles this issue comprehensively, spanning from the workloads themselves, through networking and data exchange, up to the orchestration and choreography mechanisms. The concept emphasizes the use of varied {AI} techniques, enabling autonomous and intelligent management of resources and workloads. Open standards are also a key part of the proposition, making it possible to fully engage third parties in multi-stakeholder scenarios. Although the presented architecture is promising, much work is required to realize it in practice. To this end, the key directions for future research are outlined.},
	eventtitle = {2023 {IEEE} 12th International Conference on Cloud Networking ({CloudNet})},
	pages = {454--459},
	booktitle = {2023 {IEEE} 12th International Conference on Cloud Networking ({CloudNet})},
	author = {Sowiński, Piotr and Lacalle, Ignacio and Vaño, Rafael and Palau, Carlos E.},
	urldate = {2024-05-17},
	date = {2023-11},
	note = {{ISSN}: 2771-5663},
	keywords = {Computer architecture, Quality of service, Technological innovation, orchestration, Standards, computing continuum, Artificial intelligence, Energy efficiency, scheduler, Systems architecture, {WebAssembly}},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/77PUYW7Q/10490045.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/FWU5SYDS/Sowiński et al. - 2023 - Autonomous Choreography of WebAssembly Workloads i.pdf:application/pdf},
}

@inproceedings{ristov_godeploy_2022,
	title = {{GoDeploy}: Portable Deployment of Serverless Functions in Federated {FaaS}},
	url = {https://ieeexplore.ieee.org/abstract/document/9973070},
	doi = {10.1109/CloudSummit54781.2022.00012},
	shorttitle = {{GoDeploy}},
	abstract = {Federated Function-as-a-Service ({FaaS}) offers higher scalability, better resilience and cost-performance trade-off than running serverless applications in a single cloud region. However, existing Infrastructure-as-Code ({IaC}) tools are mainly focused on the {FaaS} provider, rather than on applications, which increases developer effort to code multiple times the same data in order to deploy a serverless function on various cloud regions in federated {FaaS}. To bridge this gap, this paper introduces {GoDeploy}, a framework that simplifies coding the deployment of serverless functions in Federated {FaaS}. Using the design principle “code once, deploy everywhere”, {GoDeploy} offers developers a domain-specific language, which introduces a three-levels hierarchy serverless function → {FaaS} providers →cloud regions of {FaaS} provider, rather than existing either the two-levels hierarchy {FaaS} provider → serverless functions or flat horizontal structure. Moreover, {GoDeploy} hides the complexity and requirements of each {FaaS} provider to store deployment packages (zip) of serverless functions on their storages. With this approach, {GoDeploy} reduces deployment script length measured in lines of code ({LoC}) compared to the recent {FaaSifier} M2FaaS by up to 33.33\% for deployment on three cloud regions of {AWS}. When deploying a single function on three cloud regions of each of three {FaaS} providers {AWS}, {IBM}, Google, {LoC} are reduced by up to 72.34\% compared to the state-of-the-art {IaC} tool Terraform. The improvement is higher when a serverless function needs to be deployed on multiple cloud regions because {GoDeploy}'s three-level hierarchy requires a single {LoC} per cloud region, compared to multiple {LoC} in Terraform's and M2FaaS {DSLs}.},
	eventtitle = {2022 {IEEE} Cloud Summit},
	pages = {38--43},
	booktitle = {2022 {IEEE} Cloud Summit},
	author = {Ristov, Sashko and Brandacher, Simon and Felderer, Michael and Breu, Ruth},
	urldate = {2024-05-17},
	date = {2022-10},
	keywords = {Scalability, Complexity theory, Bridges, Function-as-a-Service, Codes, Automation, Internet, Encoding, Domain Specific Language, Infrastructure-as-a-Code, Length measurement, portability},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/UXBLL5QW/9973070.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/FSBQ56RI/Ristov et al. - 2022 - GoDeploy Portable Deployment of Serverless Functi.pdf:application/pdf},
}

@inproceedings{jana_dagit_2023,
	title = {{DAGit}: A Platform For Enabling Serverless Applications},
	url = {https://ieeexplore.ieee.org/abstract/document/10487054},
	doi = {10.1109/HiPC58850.2023.00054},
	shorttitle = {{DAGit}},
	abstract = {Serverless computing is rapidly gaining popularity for provisioning composable, auto-scalable and cost-effective applications. An important mechanism for deploying serverless applications is specification of function workflows (via {DAGs}). The end-to-end life cycle of this process being {DAG} specification, {DAG} orchestration, execution of {DAG} components and persistent storage of application outputs. To the best of our knowledge, an open-source platform that offers functionality along all these components does not exist. Towards this, our primary contribution is {DAGit}, an open-source solution for serverless applications-as-a-service. The main features of {DAGit} are interfaces and specifications to register serverless functions, applications (via {DAGs}) and triggers to instantiate the serverless applications. {DAGit} provides a rich set of {DAG} primitives to enable a varied set of applications and also implements a scalable orchestrator for application execution. As part of this work, we present the architecture and design details of {DAGit}, and demonstrate its feature set via showcasing the specification and execution of a varied set of serverless applications. Further, we also present a performance and resource costs characterization of executing applications on the {DAGit} platform.},
	eventtitle = {2023 {IEEE} 30th International Conference on High Performance Computing, Data, and Analytics ({HiPC})},
	pages = {367--376},
	booktitle = {2023 {IEEE} 30th International Conference on High Performance Computing, Data, and Analytics ({HiPC})},
	author = {Jana, Anubhav and Kulkarni, Purushottam and Bellur, Umesh},
	urldate = {2024-05-17},
	date = {2023-12},
	note = {{ISSN}: 2640-0316},
	keywords = {Computer architecture, Costs, High performance computing, Pipelines, Graphics processing units, Serverless computing, {FaaS}, serverless workflows, agile composition, Registers},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/9V2XUY27/10487054.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/G3ZU62ER/Jana et al. - 2023 - DAGit A Platform For Enabling Serverless Applicat.pdf:application/pdf},
}

@article{dehury_def-drel_2024,
	title = {Def-{DReL}: Towards a sustainable serverless functions deployment strategy for fog-cloud environments using deep reinforcement learning},
	volume = {152},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494623011973},
	doi = {10.1016/j.asoc.2023.111179},
	shorttitle = {Def-{DReL}},
	abstract = {Modern cloud applications are composed of tens of thousands of environment-agnostic serverless functions that can be deployed in either a fog or cloud environment. The key to sustaining fog computing is to offload the maximum amounts of computation to the cloud, and accommodate as many users as possible without compromising quality of service ({QoS}). However, recent research mainly focuses on assigning maximum resources to serverless applications from the fog node and not taking full advantage of the cloud environment, leading to a lack of sustainability in fog computing. As a way to fill this research gap, we explored what percentage of a user’s request should be handled by fog and cloud. As a result, we proposed Def-{DReL}, a Systematic Deployment of Serverless Functions in Fog and Cloud environments using Deep Reinforcement Learning, by taking into account several real-life parameters, including distance from a nearby fog node and latency, priority of the user, priority of serverless applications, and resource usage. Def-{DReL}’s performance is further compared with that of recent related algorithms. Simulation and comparison results clearly demonstrate a lesser number of serverless functions from each user (with approximately 10\% improvement) being deployed in the fog node, resulting in accommodating limited fog resources to more number of users. The other simulation results show its superiority over other algorithms as well as its applicability to real-life scenarios.},
	pages = {111179},
	journaltitle = {Applied Soft Computing},
	shortjournal = {Applied Soft Computing},
	author = {Dehury, Chinmaya Kumar and Poojara, Shivananda and Srirama, Satish Narayana},
	urldate = {2024-05-17},
	date = {2024-02-01},
	keywords = {Cloud computing, Serverless computing, Deep reinforcement learning, Fog computing, Function offloading, Serverless function deployment},
	file = {Dehury et al. - 2024 - Def-DReL Towards a sustainable serverless functio.pdf:/home/larissa/Zotero/storage/Q598DELQ/Dehury et al. - 2024 - Def-DReL Towards a sustainable serverless functio.pdf:application/pdf;ScienceDirect Snapshot:/home/larissa/Zotero/storage/UD9C3KEQ/S1568494623011973.html:text/html},
}

@inproceedings{scheuner_crossfit_2022,
	title = {{CrossFit}: Fine-grained Benchmarking of Serverless Application Performance across Cloud Providers},
	url = {https://ieeexplore.ieee.org/abstract/document/10061777},
	doi = {10.1109/UCC56403.2022.00016},
	shorttitle = {{CrossFit}},
	abstract = {Serverless computing emerged as a promising cloud computing paradigm for deploying cloud-native applications but raises new performance challenges. Existing performance evaluation studies focus on micro-benchmarking to measure an individual aspect of serverless functions, such as {CPU} speed, but lack an in-depth analysis of differences in application performance across cloud providers. This paper presents {CrossFit}, an approach for detailed and fair cross-provider performance benchmarking of serverless applications based on a providerindependent tracing model. Our case study demonstrates how detailed distributed tracing enables drill-down analysis to explain performance differences between two leading cloud providers, {AWS} and Azure. The results for an asynchronous application show that trigger time contributes most delay to the end-to-end latency and explains the main performance difference between cloud providers. Our results further reveal how increasing and bursty workloads affect performance stability, median latency, and tail latency.},
	eventtitle = {2022 {IEEE}/{ACM} 15th International Conference on Utility and Cloud Computing ({UCC})},
	pages = {51--60},
	booktitle = {2022 {IEEE}/{ACM} 15th International Conference on Utility and Cloud Computing ({UCC})},
	author = {Scheuner, Joel and Deng, Rui and Steghöfer, Jan-Philipp and Leitner, Philipp},
	urldate = {2024-05-17},
	date = {2022-12},
	keywords = {performance, Performance evaluation, benchmarking, Runtime, Instruments, Cloud computing, serverless, Serverless computing, {FaaS}, distributed tracing, observability, Standardization, Tail},
	file = {IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/Y69JLL7E/Scheuner et al. - 2022 - CrossFit Fine-grained Benchmarking of Serverless .pdf:application/pdf},
}

@misc{nguyen_quantum_2024,
	title = {Quantum Cloud Computing: A Review, Open Problems, and Future Directions},
	url = {http://arxiv.org/abs/2404.11420},
	doi = {10.48550/arXiv.2404.11420},
	shorttitle = {Quantum Cloud Computing},
	abstract = {Quantum cloud computing is an emerging paradigm of computing that empowers quantum applications and their deployment on quantum computing resources without the need for a specialized environment to host and operate physical quantum computers. This paper reviews recent advances, identifies open problems, and proposes future directions in quantum cloud computing. It discusses the state-of-the-art quantum cloud advances, including the various cloud-based models, platforms, and recently developed technologies and software use cases. Furthermore, it discusses different aspects of the quantum cloud, including resource management, quantum serverless, security, and privacy problems. Finally, the paper examines open problems and proposes the future directions of quantum cloud computing, including potential opportunities and ongoing research in this emerging field.},
	number = {{arXiv}:2404.11420},
	publisher = {{arXiv}},
	author = {Nguyen, Hoa T. and Krishnan, Prabhakar and Krishnaswamy, Dilip and Usman, Muhammad and Buyya, Rajkumar},
	urldate = {2024-05-17},
	date = {2024-04-17},
	eprinttype = {arxiv},
	eprint = {2404.11420 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Emerging Technologies},
	file = {arXiv Fulltext PDF:/home/larissa/Zotero/storage/S5QDBBLR/Nguyen et al. - 2024 - Quantum Cloud Computing A Review, Open Problems, .pdf:application/pdf;arXiv.org Snapshot:/home/larissa/Zotero/storage/3BRWAA43/2404.html:text/html},
}

@article{zhang_enabling_2022,
	title = {Enabling Cost-Effective, {SLO}-Aware Machine Learning Inference Serving on Public Cloud},
	volume = {10},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {2168-7161, 2372-0018},
	url = {https://ieeexplore.ieee.org/document/9132666/},
	doi = {10.1109/TCC.2020.3006751},
	pages = {1765--1779},
	number = {3},
	journaltitle = {{IEEE} Transactions on Cloud Computing},
	shortjournal = {{IEEE} Trans. Cloud Comput.},
	author = {Zhang, Chengliang and Yu, Minchen and Wang, Wei and Yan, Feng},
	urldate = {2024-05-17},
	date = {2022-07-01},
}

@article{sethi_shipping_2023,
	title = {Shipping code towards data in an inter-region serverless environment to leverage latency},
	volume = {79},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-023-05104-7},
	doi = {10.1007/s11227-023-05104-7},
	abstract = {Serverless computing emerges as a new standard to build cloud applications, where developers write compact functions that respond to events in the cloud infrastructure. Several cloud service industries started adopting serverless for deploying their applications. But one key limitation in serverless computing is that it disregards the significance of data. In the age of big data, when applications run around a huge volume, to transfer data from the data side to the computation side to co-allocate the data and code, leads to high latency. All existing serverless architectures are based on the data shipping architecture. In this paper, we present an inter-region code shipping architecture for serverless, that enables the code to flow from computation side to the data side where the size of the code is negligible compared to the data size. We tested our proposed architecture over a real-time cloud platform Amazon Web Services with the integration of the Fission serverless tool. The evaluation of the proposed code shipping architecture shows for a data file size of 64 {MB}, the latency in the proposed code shipping architecture is 8.36 ms and in existing data shipped architecture is found to be 16.8 ms. Hence, the proposed architecture achieves a speedup of 2x on the round latency for high data sizes in a serverless environment. We define round latency to be the duration to read and write back the data in the storage.},
	pages = {11585--11610},
	number = {10},
	journaltitle = {The Journal of Supercomputing},
	shortjournal = {J Supercomput},
	author = {Sethi, Biswajeet and Addya, Sourav Kanti and Bhutada, Jay and Ghosh, Soumya K.},
	urldate = {2024-05-17},
	date = {2023-07-01},
	langid = {english},
	keywords = {Cloud computing, Serverless computing, Cloud storage, Function, Latency},
	file = {Full Text PDF:/home/larissa/Zotero/storage/SGSQM7Z2/Sethi et al. - 2023 - Shipping code towards data in an inter-region serv.pdf:application/pdf},
}

@misc{wang_orpheus_2021,
	title = {{ORPHEUS}: Living Labs for End-to-End Data Infrastructures for Digital Agriculture},
	url = {http://arxiv.org/abs/2111.09422},
	doi = {10.48550/arXiv.2111.09422},
	shorttitle = {{ORPHEUS}},
	abstract = {{IoT} networks are being used to collect, analyze, and utilize sensor data. There are still some key requirements to leverage {IoT} networks in digital agriculture, e.g., design and deployment of energy saving and ruggedized sensor nodes ({SN}), reliable and long-range wireless network connectivity, end-to-end data collection pipelines for batch and streaming data. Thus, we introduce our living lab {ORPHEUS} and its design and implementation trajectory to showcase our orchestrated testbed of {IoT} sensors, data connectivity, database orchestration, and visualization dashboard. We deploy light-weight energy saving {SNs} in the field to collect data, using {LoRa} (Long Range wireless) to transmit data from the {SNs} to the Gateway node, upload all the data to the database server, and finally visualize the data. For future exploration, we also built a testbed of embedded devices using four different variants of {NVIDIA} Jetson development modules (Nano, {TX}2, Xavier {NX}, {AGX} Xavier) to benchmark the potential upgrade choices for {SNs} in {ORPHEUS}. Based on our deployment in multiple farms in a 3-county region around Purdue University, and on the Purdue University campus, we present analyses from our living lab deployment and additional components of the next-generation {IoT} farm.},
	number = {{arXiv}:2111.09422},
	publisher = {{arXiv}},
	author = {Wang, Pengcheng and Yi, Edgardo Barsallo and Ratkus, Tomas and Chaterji, Somali},
	urldate = {2024-05-17},
	date = {2021-10-04},
	eprinttype = {arxiv},
	eprint = {2111.09422 [cs]},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {arXiv Fulltext PDF:/home/larissa/Zotero/storage/Z2GIJ8CN/Wang et al. - 2021 - ORPHEUS Living Labs for End-to-End Data Infrastru.pdf:application/pdf;arXiv.org Snapshot:/home/larissa/Zotero/storage/2RRQI3I9/2111.html:text/html},
}

@article{cinar_rise_2023,
	title = {The Rise of Serverless Architectures: Security Challenges and Best Practices},
	volume = {16},
	issn = {2581-8260},
	url = {https://doi.org/10.9734/ajrcos/2023/v16i4382},
	doi = {10.9734/ajrcos/2023/v16i4382},
	shorttitle = {The Rise of Serverless Architectures},
	abstract = {The field of serverless computing has had significant growth and recognition in the past decade. This emerging area has garnered attention because to its notable impact on cost reduction, latency reduction, scalability improvement, and elimination of server-side management, among other benefits. Nevertheless, there is still a dearth of comprehensive study that would facilitate developers and academics in gaining a more profound comprehension of the importance of serverless computing in many scenarios. Therefore, it is imperative to provide scholarly study data that has been published within this particular field. This study conducted a comprehensive analysis of 275 scholarly articles retrieved from reputable literature sources, with the aim of extracting valuable insights pertaining to serverless computing. Subsequently, the acquired data underwent analysis in order to address many study inquiries pertaining to the contemporary advancements in serverless computing, encompassing its fundamental principles, available platforms, and patterns of utilization, among other relevant aspects. In addition, we analyze the current obstacles confronting serverless computing and explore potential avenues for future research to facilitate its deployment and utilization.},
	pages = {194--210},
	number = {4},
	journaltitle = {Asian Journal of Research in Computer Science},
	author = {Cinar, Burak},
	urldate = {2024-05-17},
	date = {2023-10-25},
	langid = {english},
	note = {Num Pages: 17
Number: 4},
	file = {Full Text PDF:/home/larissa/Zotero/storage/9MXDKRIH/Cinar - 2023 - The Rise of Serverless Architectures Security Cha.pdf:application/pdf;Snapshot:/home/larissa/Zotero/storage/ME4VFW5X/1676.html:text/html},
}

@inproceedings{alfares_container_2024,
	location = {New York, {NY}, {USA}},
	title = {Container Sizing for Microservices with Dynamic Workload by Online Optimization},
	isbn = {9798400704598},
	url = {https://dl.acm.org/doi/10.1145/3631311.3632399},
	doi = {10.1145/3631311.3632399},
	series = {{WoC} '23},
	abstract = {Over the past ten years, many different approaches have been proposed for different aspects of the problem of resources management for long running, dynamic and diverse workloads such as processing query streams or distributed deep learning. Particularly for applications consisting of containerized microservices, researchers have attempted to address problems of dynamic selection of, for example: types and quantities of virtualized services (e.g., {IaaS}/{VMs}), horizontal and vertical scaling of different microservices, assigning microservices to {VMs}, task scheduling, or some combination thereof. In this context, we argue that online optimization frameworks like simulated annealing are highly suitable for exploration of the trade-offs between performance ({SLO}) and cost, particularly when the complex workloads and cloud-service offerings vary over time. Based on a macroscopic objective that combines both performance and cost terms, annealing facilitates light-weight and coherent policies of exploration and exploitation. In this paper, we first give some background on simulated annealing and then experimentally demonstrate its usefulness for container sizing using microservice benchmarks. We conclude with a discussion of how the basic annealing platform can be applied to other resource-management problems, hybridized with other methods, and accommodate user-specified rules of thumb.},
	pages = {1--6},
	booktitle = {Proceedings of the 9th International Workshop on Container Technologies and Container Clouds},
	publisher = {Association for Computing Machinery},
	author = {Alfares, Nader and Kesidis, George},
	urldate = {2024-05-17},
	date = {2024-01-08},
	file = {Full Text PDF:/home/larissa/Zotero/storage/JE2TLQ5S/Alfares and Kesidis - 2024 - Container Sizing for Microservices with Dynamic Wo.pdf:application/pdf},
}

@article{xu_adaptive_2022,
	title = {An adaptive function placement in serverless computing},
	volume = {25},
	issn = {1573-7543},
	url = {https://doi.org/10.1007/s10586-021-03506-x},
	doi = {10.1007/s10586-021-03506-x},
	abstract = {Serverless computing is a new fine-grained computing and deploying paradigm which combined terminal devices, edge nodes and cloud data center into a complete computing system. Stateless function is the critical factor to implement serverless computing. For optimizing the function execution, function placement is becoming the urgent problem to be solved in the serverless computing. To specify what is function placement in serverless computing, an adaptive function placement framework is presented. For prejudging the trend of function placement, positive or negative properties of event are classed. Based on the positive or negative impact on the function placement decision and function placement trend, mathematical model of function placement is presented for the first time. For specifying the adaptive function placement in serverless computing, An adaptive function placement model is formulated based on the Markov Decision Process ({MDP}). In the model, function states space, placement rate, placement probability, decision time, action set, cost and criterion are presented for the first time to specify the computation of the function’s {MDP}.The presented algorithm decides function execution in terminal devices,local edge nodes or the function execution in the remote available cloud servers in a real time and adaptive way. The presented algorithm also permits dynamically allocating functions to minimize the Dec(t) while keeping performance satisfaction. Evaluation and experiment show the presented adaptive function placement algorithm can get the satisfied requirements of different indexes.},
	pages = {3161--3174},
	number = {5},
	journaltitle = {Cluster Computing},
	shortjournal = {Cluster Comput},
	author = {Xu, Donghong and Sun, Zhongbin},
	urldate = {2024-05-17},
	date = {2022-10-01},
	langid = {english},
	keywords = {Serverless computing, Agent, Function placement, Stateless function},
	file = {Full Text PDF:/home/larissa/Zotero/storage/KV4CAS5H/Xu and Sun - 2022 - An adaptive function placement in serverless compu.pdf:application/pdf},
}

@inproceedings{mahapatra_-storage_2024,
	location = {New York, {NY}, {USA}},
	title = {In-Storage Domain-Specific Acceleration for Serverless Computing},
	volume = {2},
	isbn = {9798400703850},
	url = {https://dl.acm.org/doi/10.1145/3620665.3640413},
	doi = {10.1145/3620665.3640413},
	series = {{ASPLOS} '24},
	abstract = {While (I) serverless computing is emerging as a popular form of cloud execution, datacenters are going through major changes: ({II}) storage dissaggregation in the system infrastructure level and ({III}) integration of domain-specific accelerators in the hardware level. Each of these three trends individually provide significant benefits; however, when combined the benefits diminish. On the convergence of these trends, the paper makes the observation that for serverless functions, the overhead of accessing dissaggregated storage overshadows the gains from accelerators. Therefore, to benefit from all these trends in conjunction, we propose In-Storage Domain-Specific Acceleration for Serverless Computing (dubbed {DSCS}-Serverless1). The idea contributes a server-less model that utilizes a programmable accelerator embedded within computational storage to unlock the potential of acceleration in disaggregated datacenters. Our results with eight applications show that integrating a comparatively small accelerator within the storage ({DSCS}-Serverless) that fits within the storage's power constraints (25 Watts), significantly outperforms a traditional disaggregated system that utilizes {NVIDIA} {RTX} 2080 Ti {GPU} (250 Watts). Further, the work highlights that disaggregation, serverless model, and the limited power budget for computation in storage device require a different design than the conventional practices of integrating microprocessors and {FPGAs}. This insight is in contrast with current practices of designing computational storage devices that are yet to address the challenges associated with the shifts in datacenters. In comparison with two such conventional designs that use {ARM} cores or a Xilinx {FPGA}, {DSCS}-Serverless provides 3.7× and 1.7× end-to-end application speedup, 4.3× and 1.9× energy reduction, and 3.2× and 2.3× better cost efficiency, respectively.},
	pages = {530--548},
	booktitle = {Proceedings of the 29th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
	publisher = {Association for Computing Machinery},
	author = {Mahapatra, Rohan and Ghodrati, Soroush and Ahn, Byung Hoon and Kinzer, Sean and Wang, Shu-Ting and Xu, Hanyang and Karthikeyan, Lavanya and Sharma, Hardik and Yazdanbakhsh, Amir and Alian, Mohammad and Esmaeilzadeh, Hadi},
	urldate = {2024-05-17},
	date = {2024-04-27},
	keywords = {serverless computing, accelerator, computational storage drive ({CSD}), deep neural network ({DNN}), disaggregated datacenter, domain specific architecture ({DSA}), in-storage acceleration, large language model ({LLM}), neural processing unit ({NPU}), serverless function, storage systems},
	file = {Full Text PDF:/home/larissa/Zotero/storage/E76RX4HN/Mahapatra et al. - 2024 - In-Storage Domain-Specific Acceleration for Server.pdf:application/pdf},
}

@inproceedings{barrak_spirt_2023,
	title = {{SPIRT}: A Fault-Tolerant and Reliable Peer-to-Peer Serverless {ML} Training Architecture},
	url = {https://ieeexplore.ieee.org/abstract/document/10366723},
	doi = {10.1109/QRS60937.2023.00069},
	shorttitle = {{SPIRT}},
	abstract = {The advent of serverless computing has ushered in notable advancements in distributed machine learning, particularly within parameter server-based architectures. Yet, the integration of serverless features within peer-to-peer (P2P) distributed networks remains largely uncharted. In this paper, we introduce {SPIRT}, a fault-tolerant, reliable, scalable and secure serverless P2P {ML} training architecture. designed to bridge this existing gap. Capitalizing on the inherent robustness and reliability innate to P2P systems, we emphasized Intra-peer scalability for concurrent gradient to mitigate communication overhead from increased peer interactions. {SPIRT}, employs {RedisAI} for in-database operations, achieves an 82\% reduction in model update times. This architecture showcases resilience against peer failures and adeptly manages the integration of new peers. Furthermore, {SPIRT} ensures secure communication between peers, enhancing the reliability of distributed machine learning tasks. Even in the face of Byzantine attacks, the system’s robust aggregation algorithms maintain high levels of accuracy. These findings illuminate the promising potential of serverless architectures in P2P distributed machine learning, offering a significant stride towards the development of more efficient, scalable, and resilient applications.},
	eventtitle = {2023 {IEEE} 23rd International Conference on Software Quality, Reliability, and Security ({QRS})},
	pages = {650--661},
	booktitle = {2023 {IEEE} 23rd International Conference on Software Quality, Reliability, and Security ({QRS})},
	author = {Barrak, Amine and Jaziri, Mayssa and Trabelsi, Ranim and Jaafar, Fehmi and Petrillo, Fabio},
	urldate = {2024-05-17},
	date = {2023-10},
	note = {{ISSN}: 2693-9177},
	keywords = {Computer architecture, Scalability, Robustness, Serverless Computing, Fault tolerance, Machine learning, Training, Distributed Machine Learning, Fault Tolerance, Fault tolerant systems, Peer-to-Peer (P2P), Robust Aggregation},
	file = {IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/GEE7GARN/Barrak et al. - 2023 - SPIRT A Fault-Tolerant and Reliable Peer-to-Peer .pdf:application/pdf},
}

@inproceedings{kottur_implementing_2022,
	location = {New York, {NY}, {USA}},
	title = {Implementing {ChaCha} based crypto primitives on programmable {SmartNICs}},
	isbn = {978-1-4503-9329-4},
	url = {https://dl.acm.org/doi/10.1145/3528082.3544833},
	doi = {10.1145/3528082.3544833},
	series = {{FFSPIN} '22},
	abstract = {Control and management plane applications such as serverless function orchestration and 4G/5G control plane functions are offloaded to {smartNICs} to reduce communication and processing latency. Such applications involve multiple inter-host interactions that were traditionally secured using {SSL}/{TLS} {gRPC}-based communication channels. Offloading the applications to {smartNIC} implies that we must also offload the security algorithms. Otherwise, we need to send the application messages to the host {VM}/container for crypto operations, negating offload benefits. We propose crypto externs for Netronome Agilio {smartNICs} that implement authentication and confidentiality (encryption/decryption) using the {ChaCha} stream cipher algorithm. {AES} and {ChaCha} are two popular cipher suites, but we chose {ChaCha} since none of the {smartNICs} have {ChaCha}-based crypto accelerators. However, {smartNICs} have restricted instruction set, and limited memory, making it difficult to implement security algorithms. This paper identifies and addresses several challenges to implement {ChaCha} crypto primitives successfully. Our evaluations show that our crypto extern implementation satisfies the scalability requirement of popular applications such as serverless management functions and host in-band network telemetry.},
	pages = {15--23},
	booktitle = {Proceedings of the {ACM} {SIGCOMM} Workshop on Formal Foundations and Security of Programmable Network Infrastructures},
	publisher = {Association for Computing Machinery},
	author = {Kottur, Shaguftha Zuveria and Kadiyala, Krishna and Tammana, Praveen and Shah, Rinku},
	urldate = {2024-05-17},
	date = {2022-08-22},
	keywords = {{ChaCha} algorithm, in-network crypto primitives, programmable data planes, {SmartNICs}},
	file = {Full Text PDF:/home/larissa/Zotero/storage/E8YU5I9S/Kottur et al. - 2022 - Implementing ChaCha based crypto primitives on pro.pdf:application/pdf},
}

@inproceedings{stojkovic_specfaas_2023,
	title = {{SpecFaaS}: Accelerating Serverless Applications with Speculative Function Execution},
	url = {https://ieeexplore.ieee.org/abstract/document/10071120},
	doi = {10.1109/HPCA56546.2023.10071120},
	shorttitle = {{SpecFaaS}},
	abstract = {Serverless computing has emerged as a popular cloud computing paradigm. Serverless environments are convenient to users and efficient for cloud providers. However, they can induce substantial application execution overheads, especially in applications with many functions.In this paper, we propose to accelerate serverless applications with a novel approach based on software-supported speculative execution of functions. Our proposal is termed Speculative Function-as-a-Service ({SpecFaaS}). It is inspired by out-of-order execution in modern processors, and is grounded in a characterization analysis of {FaaS} applications. In {SpecFaaS}, functions in an application are executed early, speculatively, before their control and data dependences are resolved. Control dependences are predicted like in pipeline branch prediction, and data dependences are speculatively satisfied with memoization. With this support, the execution of downstream functions is overlapped with that of upstream functions, substantially reducing the end-to-end execution time of applications. We prototype {SpecFaaS} on Apache {OpenWhisk}, an open-source serverless computing platform. For a set of applications in a warmed-up environment, {SpecFaaS} attains an average speedup of 4.6×. Further, on average, the application throughput increases by 3.9× and the tail latency decreases by 58.7\%.},
	eventtitle = {2023 {IEEE} International Symposium on High-Performance Computer Architecture ({HPCA})},
	pages = {814--827},
	booktitle = {2023 {IEEE} International Symposium on High-Performance Computer Architecture ({HPCA})},
	author = {Stojkovic, Jovan and Xu, Tianyin and Franke, Hubertus and Torrellas, Josep},
	urldate = {2024-05-17},
	date = {2023-02},
	note = {{ISSN}: 2378-203X},
	keywords = {Computer architecture, Prototypes, Pipelines, Program processors, Cloud computing, Function-as-a-Service, Serverless computing, Tail, Out of order},
	file = {IEEE Xplore Abstract Record:/home/larissa/Zotero/storage/L9RXF6R9/10071120.html:text/html;IEEE Xplore Full Text PDF:/home/larissa/Zotero/storage/ZJ6L5UKL/Stojkovic et al. - 2023 - SpecFaaS Accelerating Serverless Applications wit.pdf:application/pdf},
}

@article{eizaguirre_seer_2024,
	title = {A Seer knows best: Auto-tuned object storage shuffling for serverless analytics},
	volume = {183},
	issn = {0743-7315},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731523001338},
	doi = {10.1016/j.jpdc.2023.104763},
	shorttitle = {A Seer knows best},
	abstract = {Serverless platforms offer high resource elasticity and pay-as-you-go billing, making them a compelling choice for data analytics. To craft a “pure” serverless solution, the common practice is to transfer intermediate data between serverless functions via serverless object storage ({IBM} {COS}; {AWS} S3). However, prior works have led to inconclusive results about the performance of object storage systems, since they have left large margin for optimization. To verify that object storage has been underrated, we devise a novel shuffle manager for serverless data analytics called Seer. Specifically, Seer dynamically chooses between two shuffle algorithms to maximize performance. The algorithm choice is made online based on some predictive models, and very importantly, without end users having to specify intermediate shuffle data sizes at the time of the job submission. We integrate Seer with {PyWren}-{IBM} [31], a well-known serverless analytics framework, and evaluate it against both serverful (e.g., Spark) and serverless systems (e.g., Google {BigQuery}, Caerus [46] and {SONIC} [22]). Our results certify that our new shuffle manager can deliver performance improvements over them.},
	pages = {104763},
	journaltitle = {Journal of Parallel and Distributed Computing},
	shortjournal = {Journal of Parallel and Distributed Computing},
	author = {Eizaguirre, Germán T. and Sánchez-Artigas, Marc},
	urldate = {2024-05-17},
	date = {2024-01-01},
	keywords = {Serverless computing, I/O optimization, Object storage, Shuffle},
	file = {Eizaguirre and Sánchez-Artigas - 2024 - A Seer knows best Auto-tuned object storage shuff.pdf:/home/larissa/Zotero/storage/LKGKHUMC/Eizaguirre and Sánchez-Artigas - 2024 - A Seer knows best Auto-tuned object storage shuff.pdf:application/pdf;ScienceDirect Snapshot:/home/larissa/Zotero/storage/LZ2KUXJK/S0743731523001338.html:text/html},
}

@inproceedings{banaszak_use_2022,
	location = {Singapore},
	title = {The Use of Serverless Processing in Web Application Development},
	isbn = {978-981-19958-2-8},
	doi = {10.1007/978-981-19-9582-8_21},
	abstract = {This paper analyses the applicability of serverless processing in the web application field. We discuss whether the serverless approach fulfils the web applications’ requirements, and provide the hands-on implementation of a serverless-based web application to illustrate the new possibilities offered by the serverless environment and provide a deeper understanding and practical experience of the researched domain.},
	pages = {230--242},
	booktitle = {New Trends in Computer Technologies and Applications},
	publisher = {Springer Nature},
	author = {Banaszak, Robert and Kobusinska, Anna},
	editor = {Hsieh, Sun-Yuan and Hung, Ling-Ju and Klasing, Ralf and Lee, Chia-Wei and Peng, Sheng-Lung},
	date = {2022},
	langid = {english},
	keywords = {{FaaS}, Serverless processing, Web applications},
	file = {Full Text PDF:/home/larissa/Zotero/storage/BSP6QB26/Banaszak and Kobusinska - 2022 - The Use of Serverless Processing in Web Applicatio.pdf:application/pdf},
}

@inproceedings{larcher_scale_2023,
	location = {Denver {CO} {USA}},
	title = {Scale Composite {BaaS} Services With {AFCL} Workflows},
	isbn = {9798400707858},
	url = {https://dl.acm.org/doi/10.1145/3624062.3624282},
	doi = {10.1145/3624062.3624282},
	abstract = {Due to various restrictions in serverless computing, developers face significant challenges to pipeline multiple Backend-as-a-Service ({BaaS}) services, which is restricted by the maximum size of the serverless function’s deployment package, or by throughput and concurrency restrictions for functions and {BaaS} services.},
	eventtitle = {{SC}-W 2023: Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
	pages = {2033--2041},
	booktitle = {Proceedings of the {SC} '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
	publisher = {{ACM}},
	author = {Larcher, Thomas and Ristov, Sashko},
	urldate = {2024-05-17},
	date = {2023-11-12},
	langid = {english},
	file = {Larcher and Ristov - 2023 - Scale Composite BaaS Services With AFCL Workflows.pdf:/home/larissa/Zotero/storage/V93CCWVH/Larcher and Ristov - 2023 - Scale Composite BaaS Services With AFCL Workflows.pdf:application/pdf},
}

@misc{kristiansson_colonyos_2024,
	title = {{ColonyOS} -- A Meta-Operating System for Distributed Computing Across Heterogeneous Platform},
	url = {http://arxiv.org/abs/2403.16486},
	abstract = {This paper presents {ColonyOS}1, an open-source meta-operating system designed to improve integration and utilization of diverse computing platforms, including {IoT}, edge, cloud, and {HPC}. Operating as an overlay, {ColonyOS} can interface with a wide range of computing environments, fostering creation of so-called compute continuums. This makes it possible to develop {AI} workflows and applications that can operate across platforms.},
	number = {{arXiv}:2403.16486},
	publisher = {{arXiv}},
	author = {Kristiansson, Johan},
	urldate = {2024-05-17},
	date = {2024-03-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2403.16486 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Kristiansson - 2024 - ColonyOS -- A Meta-Operating System for Distribute.pdf:/home/larissa/Zotero/storage/XF3I4NYU/Kristiansson - 2024 - ColonyOS -- A Meta-Operating System for Distribute.pdf:application/pdf},
}

@inproceedings{gunasekaran_minimizing_2020,
	location = {Delft Netherlands},
	title = {Minimizing Cost and Maximizing Performance for Cloud Platforms},
	isbn = {978-1-4503-8200-7},
	url = {https://dl.acm.org/doi/10.1145/3429351.3431747},
	doi = {10.1145/3429351.3431747},
	abstract = {We are witnessing the rapid growth of cloud computing with the proliferation of tenants adopting cloud for elasticity, availability, and flexibility for a plethora of applications. To efficiently cater for different tenant requirements, cloud providers have steadily evolved to offer a myriad of resource and service types which inherently complicates the cloud adoption process. On the other hand, the perpetuating growth of cloud tenants in turn impel providers to expand datacenters to cope with the tenant demand. The objective of this proposal is to maximize the performance and minimize the cost for both tenants and cloud providers, by providing efficient means of managing resource allocations for their applications. Towards this, the proposal comprises of three intertwined tasks. First, we start from a tenant perspective, with the first two tasks aimed at investigating the primary reasons for performance-cost inefficiency. Second, from a provider perspective, the third task investigates the primary reasons for performance-energy inefficiency in datacenters. All the three tasks can collectively improve the performance and cost efficiency of emerging applications in next generation cloud platforms.},
	eventtitle = {Middleware '20: 21st International Middleware Conference},
	pages = {29--34},
	booktitle = {Proceedings of the 21st International Middleware Conference Doctoral Symposium},
	publisher = {{ACM}},
	author = {Gunasekaran, Jashwant Raj},
	urldate = {2024-05-17},
	date = {2020-12-07},
	langid = {english},
	file = {Gunasekaran - 2020 - Minimizing Cost and Maximizing Performance for Clo.pdf:/home/larissa/Zotero/storage/JZU8GLYD/Gunasekaran - 2020 - Minimizing Cost and Maximizing Performance for Clo.pdf:application/pdf},
}

@article{de_silva_impact_2024,
	title = {The Impact of Software Testing on Serverless Applications},
	volume = {12},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10489959/},
	doi = {10.1109/ACCESS.2024.3384459},
	abstract = {With the increasing prevalence of serverless applications, a conspicuous gap in research has emerged regarding the comprehensive insights into the ramifications of serverless technology on software testing. This study aims to bridge this gap by examining the intricate interplay between software testing and serverless applications. Its overarching objectives include identifying the multifaceted challenges and constraints encountered in testing serverless applications and formulating a cogent testing strategy. This strategy delineates the optimal balance between unit, integration, and end-to-end tests in the context of serverless applications. To achieve these objectives, the research utilizes a multifaceted approach, including interviews with seasoned industry experts who have over a decade of experience. These experts provide invaluable insights into the complex dynamics of software testing in the serverless landscape. The research findings emphasize the increased complexity in testing serverless applications and advocate for software teams to adopt a shift-left approach. This involves gaining a deep understanding of the overarching services and their constraints before developing a testing strategy. Furthermore, it is crucial to acknowledge that there is no ’one-size-fits-all’ optimal testing ratio. Instead, it is contingent upon a constellation of influential factors.},
	pages = {51086--51099},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {De Silva, Dilshan and Hewawasam, Lakindu},
	urldate = {2024-05-17},
	date = {2024},
	langid = {english},
	file = {De Silva and Hewawasam - 2024 - The Impact of Software Testing on Serverless Appli.pdf:/home/larissa/Zotero/storage/XNWMVZDZ/De Silva and Hewawasam - 2024 - The Impact of Software Testing on Serverless Appli.pdf:application/pdf},
}

@inproceedings{liang_edgeorcher_2023,
	location = {Hong Kong, Hong Kong},
	title = {{EdgeOrcher}: Predictive Function Orchestration for Serverless-Based Edge Native Applications},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350339864},
	url = {https://ieeexplore.ieee.org/document/10272539/},
	doi = {10.1109/ICDCS57875.2023.00094},
	shorttitle = {{EdgeOrcher}},
	abstract = {Serverless computing is becoming prevalent to develop resource-demanding and delay-sensitive edge native applications across the edge and cloud. The unique pricing mechanism of serverless computing brings new opportunities to reduce the cost of edge native applications, by orchestrating function fusion and placement across the edge and cloud. However, function fusion potentially increases the latency of the serverless workﬂow. To navigate this performance-cost tradeoff, we present an online predictive function orchestration framework which leverages predictions to dynamically optimize the function fusion and placement. Preliminary evaluation results verify the efﬁcacy of the proposed framework.},
	eventtitle = {2023 {IEEE} 43rd International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {1--2},
	booktitle = {2023 {IEEE} 43rd International Conference on Distributed Computing Systems ({ICDCS})},
	publisher = {{IEEE}},
	author = {Liang, Yunkai and Zhou, Zhi and Chen, Xu},
	urldate = {2024-05-17},
	date = {2023-07},
	langid = {english},
	file = {Liang et al. - 2023 - EdgeOrcher Predictive Function Orchestration for .pdf:/home/larissa/Zotero/storage/PLRCJJW3/Liang et al. - 2023 - EdgeOrcher Predictive Function Orchestration for .pdf:application/pdf},
}

@inproceedings{shakeel_implementing_2023,
	location = {New Delhi, India},
	title = {Implementing a Serverless Workflow using {AWS} Step Function},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66549-382-6},
	url = {https://ieeexplore.ieee.org/document/10150562/},
	doi = {10.1109/REEDCON57544.2023.10150562},
	abstract = {In recent years, cloud computing has become the preferred method for delivering effective internet services. Within this field, serverless computing has emerged as a standout approach, providing flexibility and agility for building complex applications. Major internet organizations such as Google, Facebook, Amazon, {IBM}, and Microsoft utilize cloud-based programming to construct large applications that can be managed and upgraded independently. However, infrastructure expenses can be a significant barrier. {AWS} Lambda and other serverless computing services provide a solution to this problem by offering scalability without requiring additional servers. This paper explores the operational mechanics of {AWS} Lambda and other {AWS} services, showcasing how they can be used to create an efficient and cost-effective serverless workflow model.},
	eventtitle = {2023 International Conference on Recent Advances in Electrical, Electronics \& Digital Healthcare Technologies ({REEDCON})},
	pages = {68--73},
	booktitle = {2023 International Conference on Recent Advances in Electrical, Electronics \& Digital Healthcare Technologies ({REEDCON})},
	publisher = {{IEEE}},
	author = {Shakeel, Iman and Mehfuz, Shabana and Ahmad, Shahnawaz},
	urldate = {2024-05-17},
	date = {2023-05-01},
	langid = {english},
	file = {Shakeel et al. - 2023 - Implementing a Serverless Workflow using AWS Step .pdf:/home/larissa/Zotero/storage/SSAIKBNL/Shakeel et al. - 2023 - Implementing a Serverless Workflow using AWS Step .pdf:application/pdf},
}

@inproceedings{maio_tarot_2022,
	location = {Vancouver, {WA}, {USA}},
	title = {{TAROT}: Spatio-Temporal Function Placement for Serverless Smart City Applications},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-087-3},
	url = {https://ieeexplore.ieee.org/document/10061776/},
	doi = {10.1109/UCC56403.2022.00013},
	shorttitle = {{TAROT}},
	abstract = {Emerging smart city applications (i.e., trafﬁc management, smart tourism) have to (i) process data coming from different {IoT} devices and (ii) deliver results of data processing to various user devices (e.g., smart vehicles or smartphone) while considering applications’ latency constraints. Serverless edge computing has proven to be very effective for latency-aware processing of {IoT} data, since it allows application developers to deﬁne data processing logic in terms of functions which react to data events. However, data processing functions should be dynamically placed and migrated while considering {IoT} data sources’ location and user devices’ mobility to minimize end-to-end latency. Unfortunately, current serverless computing solutions do not support mobility-aware placement of functions. In this paper, we propose dynamic function placement based on user devices’ mobility to address latency requirements of smart city applications. We consider serverless smart city applications, since this computational model allows to model application as a function execution in response to speciﬁc events, which makes it suitable for event-driven applications typical of smart city and {IoT}. First, we identify the parameters affecting end-to-end latency of serverless smart cities’ applications. Then, based on our ﬁndings, we design {TAROT}, a latency-aware function placement method based on data-driven mobility predictions. Results show improvements up to 46\% for average end-to-end latency in comparison to state-of-the-art solutions.},
	eventtitle = {2022 {IEEE}/{ACM} 15th International Conference on Utility and Cloud Computing ({UCC})},
	pages = {21--30},
	booktitle = {2022 {IEEE}/{ACM} 15th International Conference on Utility and Cloud Computing ({UCC})},
	publisher = {{IEEE}},
	author = {Maio, Vincenzo De and Bermbach, David and Brandic, Ivona},
	urldate = {2024-05-17},
	date = {2022-12},
	langid = {english},
	file = {Maio et al. - 2022 - TAROT Spatio-Temporal Function Placement for Serv.pdf:/home/larissa/Zotero/storage/CZWQ9CIM/Maio et al. - 2022 - TAROT Spatio-Temporal Function Placement for Serv.pdf:application/pdf},
}

@inproceedings{niu_gtaa_2019,
	location = {Milan, Italy},
	title = {{GTAA}: A Geo-Aware Task Allocation Approach in Cloud Workflow},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72812-717-0},
	url = {https://ieeexplore.ieee.org/document/8818391/},
	doi = {10.1109/ICWS.2019.00082},
	shorttitle = {{GTAA}},
	abstract = {The cloud computing simpliﬁes application development into the orchestration of virtual-services workﬂow. However, network latency between geographically distributed hosts would slow down the workﬂow’s makespan time. This paper proposes a geo-aware task allocation approach ({GTAA}). {GTAA} partitions the workﬂow for geo-distributed data centers({DCs}) and reduces sub-workﬂows across {DCs}. {GTAA} aims to optimize overall workﬂow makespan time and improves the efﬁciency of workﬂow.},
	eventtitle = {2019 {IEEE} International Conference on Web Services ({ICWS})},
	pages = {449--451},
	booktitle = {2019 {IEEE} International Conference on Web Services ({ICWS})},
	publisher = {{IEEE}},
	author = {Niu, Meng and Cheng, Bo and Chen, Junling},
	urldate = {2024-05-17},
	date = {2019-07},
	langid = {english},
	file = {Niu et al. - 2019 - GTAA A Geo-Aware Task Allocation Approach in Clou.pdf:/home/larissa/Zotero/storage/Q8VFJWV3/Niu et al. - 2019 - GTAA A Geo-Aware Task Allocation Approach in Clou.pdf:application/pdf},
}

@inproceedings{bai_generic_2023,
	location = {Chicago, {IL}, {USA}},
	title = {A Generic Efficient Scientific Workflow Engine for the Optimizations of Run-Time Execution},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350340754},
	url = {https://ieeexplore.ieee.org/document/10234366/},
	doi = {10.1109/SSE60056.2023.00023},
	abstract = {Workﬂow has proven to be a highly effective computing model for a variety of scientiﬁc applications, offering ﬂexible data types and unstructured parallelism that surpasses simple parallel execution models such as {MapReduce}. However, current workﬂow management systems in cloud computing environments experience unnecessary delays in task execution due to the separation of task execution and data transfer processes, which causes a child task to wait until all its predecessor tasks complete, rather than waiting only for necessary input data becoming ready. The goal of this paper is to eliminate the unnecessary delay of child tasks in a workﬂow, which is achieved through a new workﬂow engine architecture that separates workﬂow planner from workﬂow executor in the general framework of the {DATAVIEW} scientiﬁc workﬂow management system. This new engine architecture can be generalized and applied to other workﬂow systems. Our design integrates a new task release mechanism based on a data dependency model with the workﬂow executor of {DATAVIEW}. This approach enables prompt task launching once input data becomes available, instead of waiting for all predecessor tasks to ﬁnish. The architecture employs distributed algorithms for implementing the workﬂow executor and the task executors, performing various optimization on data movement, task movement, and communication among different subsystems. The experiments show that our new architecture based on the new task release model can signiﬁcantly reduce overall execution time of a workﬂow in {DATAVIEW}.},
	eventtitle = {2023 {IEEE} International Conference on Software Services Engineering ({SSE})},
	pages = {98--103},
	booktitle = {2023 {IEEE} International Conference on Software Services Engineering ({SSE})},
	publisher = {{IEEE}},
	author = {Bai, Changxin and Liu, Junwen and Tahabilder, Anik and Imran, M M and Lu, Shiyong and Che, Dunren},
	urldate = {2024-05-17},
	date = {2023-07},
	langid = {english},
	file = {Bai et al. - 2023 - A Generic Efficient Scientific Workflow Engine for.pdf:/home/larissa/Zotero/storage/9XDWBZQC/Bai et al. - 2023 - A Generic Efficient Scientific Workflow Engine for.pdf:application/pdf},
}

@inproceedings{moczurad_visual-textual_2018,
	location = {Zurich},
	title = {Visual-Textual Framework for Serverless Computation: A Luna Language Approach},
	isbn = {978-1-72810-359-4},
	url = {https://ieeexplore.ieee.org/document/8605775/},
	doi = {10.1109/UCC-Companion.2018.00052},
	shorttitle = {Visual-Textual Framework for Serverless Computation},
	abstract = {As serverless technologies are emerging as a breakthrough in the cloud computing industry, the lack of proper tooling is becoming apparent. The model of computation that the serverless is imposing is as ﬂexible as it is hard to manage and grasp. We present a novel approach towards serverless computing that tightly integrates it with the visual-textual, functional programming language: Luna. This way we are hoping to achieve the clarity and cognitive ease of visual solutions while retaining the ﬂexibility and expressive power of textual programming languages. We created a proof of concept of the Luna Serverless Framework in which we extend the Luna standard library and we leverage the language features to create an intuitive {API} for serverless function calls using {AWS} Lambda and to call external functions implemented in {JavaScript}.},
	eventtitle = {2018 {IEEE}/{ACM} International Conference on Utility and Cloud Computing Companion ({UCC} Companion)},
	pages = {169--174},
	booktitle = {2018 {IEEE}/{ACM} International Conference on Utility and Cloud Computing Companion ({UCC} Companion)},
	publisher = {{IEEE}},
	author = {Moczurad, Piotr and Malawski, Maciej},
	urldate = {2024-05-17},
	date = {2018-12},
	langid = {english},
	file = {Moczurad and Malawski - 2018 - Visual-Textual Framework for Serverless Computatio.pdf:/home/larissa/Zotero/storage/7XKKS5WP/Moczurad and Malawski - 2018 - Visual-Textual Framework for Serverless Computatio.pdf:application/pdf},
}

@article{chowdhury_evaluating_2024,
	title = {Evaluating the Security Implications of Serverless Computing Environments: A Focus on Vulnerabilities and Countermeasures},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://zenodo.org/doi/10.5281/zenodo.10488030},
	doi = {10.5281/ZENODO.10488030},
	shorttitle = {Evaluating the Security Implications of Serverless Computing Environments},
	abstract = {This research investigates the security landscape of serverless computing environments within cloud infrastructures. With a primary focus on identifying and analyzing vulnerabilities specific to serverless applications, the study aims to contribute valuable insights to the evolving field of cybersecurity. The research explores hacking techniques tailored to exploit these vulnerabilities, emphasizing real-world scenarios. Additionally, the paper proposes and evaluates countermeasures, best practices, and security controls to enhance the resilience of serverless applications against cyber threats. By addressing the unique security challenges posed by serverless architectures, this research seeks to advance the understanding of effective measures to secure cloud-based, serverless computing environments.},
	author = {Chowdhury, Naem Azam},
	urldate = {2024-05-17},
	date = {2024-01-10},
	langid = {english},
	note = {Publisher: [object Object]},
	file = {Chowdhury - 2024 - Evaluating the Security Implications of Serverless.pdf:/home/larissa/Zotero/storage/ISENJ9ZL/Chowdhury - 2024 - Evaluating the Security Implications of Serverless.pdf:application/pdf},
}
